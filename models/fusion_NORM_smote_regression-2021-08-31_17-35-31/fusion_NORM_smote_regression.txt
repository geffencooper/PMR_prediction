============================ Raw Args ============================
Namespace(batch_size=32, classification='n', dropout='n', droput_prob=0.0, gpu_i=0, hidden_init_rand='n', hidden_size=64, imbalanced_sampler='n', input_size=23, l2_reg='n', load_trained='n', log_dest='../models/fusion_NORM_smote_regression-2021-08-31_17-35-31', loss_freq=10, lr=0.002, model_name='PMRfusionNN', normalize='y', num_classes=-1, num_epochs=5, num_layers=1, optim='RMS', regression='y', root_dir='/data/perception-working/Geffen/avec_data/', session_name='fusion_NORM_smote_regression', train_data_dir='SMOTE/', train_labels_csv='labels2.csv', trained_path='none', val_data_dir='none', val_freq=0, val_labels_csv='val_metadata.csv', weight_decay_amnt=0.0, weighted_loss='n')



================================ Start Training ================================

Session Name: fusion_NORM_smote_regression

Model Name: PMRfusionNN

Device: 0  ---->  GeForce GTX 1080 Ti

Hyperparameters:
Batch Size: 32
Learning Rate: 0.002
Hidden Size: 64
Number of Layer: 1
Number of Epochs: 5
Normalization:y

Train Epoch: 0 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 1.373149
Train Epoch: 0 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 1.431488
Train Epoch: 0 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 1.077671
Train Epoch: 0 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 0.665573
Train Epoch: 0 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 0.648327
Train Epoch: 0 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 0.820729
Train Epoch: 0 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 0.667707
Train Epoch: 0 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 0.632044
Train Epoch: 0 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 1.093051
Train Epoch: 0 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 0.528594
Train Epoch: 0 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.558370
Train Epoch: 0 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.919719
Train Epoch: 0 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.869789
Train Epoch: 0 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.619033
Train Epoch: 0 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.516181
Train Epoch: 0 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.468440
Train Epoch: 0 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 0.589100
Train Epoch: 0 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 0.273026
Train Epoch: 0 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 0.342461
Train Epoch: 0 Iteration: 200 [6400/34075 (19%)]	 Batch 200 Loss: 0.600745
Train Epoch: 0 Iteration: 210 [6720/34075 (20%)]	 Batch 210 Loss: 0.422218
Train Epoch: 0 Iteration: 220 [7040/34075 (21%)]	 Batch 220 Loss: 0.535248
Train Epoch: 0 Iteration: 230 [7360/34075 (22%)]	 Batch 230 Loss: 0.614486
Train Epoch: 0 Iteration: 240 [7680/34075 (23%)]	 Batch 240 Loss: 0.768610
Train Epoch: 0 Iteration: 250 [8000/34075 (23%)]	 Batch 250 Loss: 0.449529
Train Epoch: 0 Iteration: 260 [8320/34075 (24%)]	 Batch 260 Loss: 0.584665
Train Epoch: 0 Iteration: 270 [8640/34075 (25%)]	 Batch 270 Loss: 0.396127
Train Epoch: 0 Iteration: 280 [8960/34075 (26%)]	 Batch 280 Loss: 0.568181
Train Epoch: 0 Iteration: 290 [9280/34075 (27%)]	 Batch 290 Loss: 0.820829
Train Epoch: 0 Iteration: 300 [9600/34075 (28%)]	 Batch 300 Loss: 0.892130
Train Epoch: 0 Iteration: 310 [9920/34075 (29%)]	 Batch 310 Loss: 0.550154
Train Epoch: 0 Iteration: 320 [10240/34075 (30%)]	 Batch 320 Loss: 0.186224
Train Epoch: 0 Iteration: 330 [10560/34075 (31%)]	 Batch 330 Loss: 0.645261
Train Epoch: 0 Iteration: 340 [10880/34075 (32%)]	 Batch 340 Loss: 0.538391
Train Epoch: 0 Iteration: 350 [11200/34075 (33%)]	 Batch 350 Loss: 0.455612
Train Epoch: 0 Iteration: 360 [11520/34075 (34%)]	 Batch 360 Loss: 0.492681
Train Epoch: 0 Iteration: 370 [11840/34075 (35%)]	 Batch 370 Loss: 0.419978
Train Epoch: 0 Iteration: 380 [12160/34075 (36%)]	 Batch 380 Loss: 0.336284
Train Epoch: 0 Iteration: 390 [12480/34075 (37%)]	 Batch 390 Loss: 0.387186
Train Epoch: 0 Iteration: 400 [12800/34075 (38%)]	 Batch 400 Loss: 0.669574
Train Epoch: 0 Iteration: 410 [13120/34075 (38%)]	 Batch 410 Loss: 0.364847
Train Epoch: 0 Iteration: 420 [13440/34075 (39%)]	 Batch 420 Loss: 0.643471
Train Epoch: 0 Iteration: 430 [13760/34075 (40%)]	 Batch 430 Loss: 0.384242
Train Epoch: 0 Iteration: 440 [14080/34075 (41%)]	 Batch 440 Loss: 0.337869
Train Epoch: 0 Iteration: 450 [14400/34075 (42%)]	 Batch 450 Loss: 0.611315
Train Epoch: 0 Iteration: 460 [14720/34075 (43%)]	 Batch 460 Loss: 0.207802
Train Epoch: 0 Iteration: 470 [15040/34075 (44%)]	 Batch 470 Loss: 0.758012
Train Epoch: 0 Iteration: 480 [15360/34075 (45%)]	 Batch 480 Loss: 0.786482
Train Epoch: 0 Iteration: 490 [15680/34075 (46%)]	 Batch 490 Loss: 0.866068
Train Epoch: 0 Iteration: 500 [16000/34075 (47%)]	 Batch 500 Loss: 0.568896
Train Epoch: 0 Iteration: 510 [16320/34075 (48%)]	 Batch 510 Loss: 0.522960
Train Epoch: 0 Iteration: 520 [16640/34075 (49%)]	 Batch 520 Loss: 0.393926
Train Epoch: 0 Iteration: 530 [16960/34075 (50%)]	 Batch 530 Loss: 0.344455
Train Epoch: 0 Iteration: 540 [17280/34075 (51%)]	 Batch 540 Loss: 0.376489
Train Epoch: 0 Iteration: 550 [17600/34075 (52%)]	 Batch 550 Loss: 0.311082
Train Epoch: 0 Iteration: 560 [17920/34075 (53%)]	 Batch 560 Loss: 0.443242
Train Epoch: 0 Iteration: 570 [18240/34075 (54%)]	 Batch 570 Loss: 0.590787
Train Epoch: 0 Iteration: 580 [18560/34075 (54%)]	 Batch 580 Loss: 0.376593
Train Epoch: 0 Iteration: 590 [18880/34075 (55%)]	 Batch 590 Loss: 0.500202
Train Epoch: 0 Iteration: 600 [19200/34075 (56%)]	 Batch 600 Loss: 0.513155
Train Epoch: 0 Iteration: 610 [19520/34075 (57%)]	 Batch 610 Loss: 0.404303
Train Epoch: 0 Iteration: 620 [19840/34075 (58%)]	 Batch 620 Loss: 0.364119
Train Epoch: 0 Iteration: 630 [20160/34075 (59%)]	 Batch 630 Loss: 0.269422
Train Epoch: 0 Iteration: 640 [20480/34075 (60%)]	 Batch 640 Loss: 0.293971
Train Epoch: 0 Iteration: 650 [20800/34075 (61%)]	 Batch 650 Loss: 0.387955
Train Epoch: 0 Iteration: 660 [21120/34075 (62%)]	 Batch 660 Loss: 0.246112
Train Epoch: 0 Iteration: 670 [21440/34075 (63%)]	 Batch 670 Loss: 0.413990
Train Epoch: 0 Iteration: 680 [21760/34075 (64%)]	 Batch 680 Loss: 0.393871
Train Epoch: 0 Iteration: 690 [22080/34075 (65%)]	 Batch 690 Loss: 0.466861
Train Epoch: 0 Iteration: 700 [22400/34075 (66%)]	 Batch 700 Loss: 0.246712
Train Epoch: 0 Iteration: 710 [22720/34075 (67%)]	 Batch 710 Loss: 0.430645
Train Epoch: 0 Iteration: 720 [23040/34075 (68%)]	 Batch 720 Loss: 0.279369
Train Epoch: 0 Iteration: 730 [23360/34075 (69%)]	 Batch 730 Loss: 0.279911
Train Epoch: 0 Iteration: 740 [23680/34075 (69%)]	 Batch 740 Loss: 0.372187
Train Epoch: 0 Iteration: 750 [24000/34075 (70%)]	 Batch 750 Loss: 0.211938
Train Epoch: 0 Iteration: 760 [24320/34075 (71%)]	 Batch 760 Loss: 0.313344
Train Epoch: 0 Iteration: 770 [24640/34075 (72%)]	 Batch 770 Loss: 0.424487
Train Epoch: 0 Iteration: 780 [24960/34075 (73%)]	 Batch 780 Loss: 0.236247
Train Epoch: 0 Iteration: 790 [25280/34075 (74%)]	 Batch 790 Loss: 0.333291
Train Epoch: 0 Iteration: 800 [25600/34075 (75%)]	 Batch 800 Loss: 0.370726
Train Epoch: 0 Iteration: 810 [25920/34075 (76%)]	 Batch 810 Loss: 0.474100
Train Epoch: 0 Iteration: 820 [26240/34075 (77%)]	 Batch 820 Loss: 0.256871
Train Epoch: 0 Iteration: 830 [26560/34075 (78%)]	 Batch 830 Loss: 0.442637
Train Epoch: 0 Iteration: 840 [26880/34075 (79%)]	 Batch 840 Loss: 0.482977
Train Epoch: 0 Iteration: 850 [27200/34075 (80%)]	 Batch 850 Loss: 0.249041
Train Epoch: 0 Iteration: 860 [27520/34075 (81%)]	 Batch 860 Loss: 0.318673
Train Epoch: 0 Iteration: 870 [27840/34075 (82%)]	 Batch 870 Loss: 0.368700
Train Epoch: 0 Iteration: 880 [28160/34075 (83%)]	 Batch 880 Loss: 0.393279
Train Epoch: 0 Iteration: 890 [28480/34075 (84%)]	 Batch 890 Loss: 0.397419
Train Epoch: 0 Iteration: 900 [28800/34075 (85%)]	 Batch 900 Loss: 0.344189
Train Epoch: 0 Iteration: 910 [29120/34075 (85%)]	 Batch 910 Loss: 0.257204
Train Epoch: 0 Iteration: 920 [29440/34075 (86%)]	 Batch 920 Loss: 0.226164
Train Epoch: 0 Iteration: 930 [29760/34075 (87%)]	 Batch 930 Loss: 0.565066
Train Epoch: 0 Iteration: 940 [30080/34075 (88%)]	 Batch 940 Loss: 0.365809
Train Epoch: 0 Iteration: 950 [30400/34075 (89%)]	 Batch 950 Loss: 0.238782
Train Epoch: 0 Iteration: 960 [30720/34075 (90%)]	 Batch 960 Loss: 0.462990
Train Epoch: 0 Iteration: 970 [31040/34075 (91%)]	 Batch 970 Loss: 0.400650
Train Epoch: 0 Iteration: 980 [31360/34075 (92%)]	 Batch 980 Loss: 0.331183
Train Epoch: 0 Iteration: 990 [31680/34075 (93%)]	 Batch 990 Loss: 0.336961
Train Epoch: 0 Iteration: 1000 [32000/34075 (94%)]	 Batch 1000 Loss: 0.189966
Train Epoch: 0 Iteration: 1010 [32320/34075 (95%)]	 Batch 1010 Loss: 0.217671
Train Epoch: 0 Iteration: 1020 [32640/34075 (96%)]	 Batch 1020 Loss: 0.283018
Train Epoch: 0 Iteration: 1030 [32960/34075 (97%)]	 Batch 1030 Loss: 0.337499
Train Epoch: 0 Iteration: 1040 [33280/34075 (98%)]	 Batch 1040 Loss: 0.463655
Train Epoch: 0 Iteration: 1050 [33600/34075 (99%)]	 Batch 1050 Loss: 0.227149
Train Epoch: 0 Iteration: 1060 [33920/34075 (100%)]	 Batch 1060 Loss: 0.260422


----------------- Epoch 0 -----------------

validation computation time: 10.0  minutes

Validation Loss: 0.8842
Training Loss:0.4626
Lowest Validation Loss: 0.884201
Time Elapsed: 0h 30m 41s

--------------------------------------------------------


Train Epoch: 1 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 0.221122
Train Epoch: 1 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 0.227565
Train Epoch: 1 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 0.159778
Train Epoch: 1 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 0.181885
Train Epoch: 1 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 0.268540
Train Epoch: 1 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 0.127032
Train Epoch: 1 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 0.151742
Train Epoch: 1 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 0.295405
Train Epoch: 1 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 0.196286
Train Epoch: 1 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 0.205434
Train Epoch: 1 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.125960
Train Epoch: 1 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.187645
Train Epoch: 1 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.253574
Train Epoch: 1 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.142115
Train Epoch: 1 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.128476
Train Epoch: 1 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.255184
Train Epoch: 1 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 0.173796
Train Epoch: 1 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 0.189907
Train Epoch: 1 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 0.203316
Train Epoch: 1 Iteration: 200 [6400/34075 (19%)]	 Batch 200 Loss: 0.205738
Train Epoch: 1 Iteration: 210 [6720/34075 (20%)]	 Batch 210 Loss: 0.143839
Train Epoch: 1 Iteration: 220 [7040/34075 (21%)]	 Batch 220 Loss: 0.357982
Train Epoch: 1 Iteration: 230 [7360/34075 (22%)]	 Batch 230 Loss: 0.507290
Train Epoch: 1 Iteration: 240 [7680/34075 (23%)]	 Batch 240 Loss: 0.142573
Train Epoch: 1 Iteration: 250 [8000/34075 (23%)]	 Batch 250 Loss: 0.247135
Train Epoch: 1 Iteration: 260 [8320/34075 (24%)]	 Batch 260 Loss: 0.135468
Train Epoch: 1 Iteration: 270 [8640/34075 (25%)]	 Batch 270 Loss: 0.203962
Train Epoch: 1 Iteration: 280 [8960/34075 (26%)]	 Batch 280 Loss: 0.247925
Train Epoch: 1 Iteration: 290 [9280/34075 (27%)]	 Batch 290 Loss: 0.286367
Train Epoch: 1 Iteration: 300 [9600/34075 (28%)]	 Batch 300 Loss: 0.225344
Train Epoch: 1 Iteration: 310 [9920/34075 (29%)]	 Batch 310 Loss: 0.254698
Train Epoch: 1 Iteration: 320 [10240/34075 (30%)]	 Batch 320 Loss: 0.133819
Train Epoch: 1 Iteration: 330 [10560/34075 (31%)]	 Batch 330 Loss: 0.339308
Train Epoch: 1 Iteration: 340 [10880/34075 (32%)]	 Batch 340 Loss: 0.156009
Train Epoch: 1 Iteration: 350 [11200/34075 (33%)]	 Batch 350 Loss: 0.193618
Train Epoch: 1 Iteration: 360 [11520/34075 (34%)]	 Batch 360 Loss: 0.301887
Train Epoch: 1 Iteration: 370 [11840/34075 (35%)]	 Batch 370 Loss: 0.224340
Train Epoch: 1 Iteration: 380 [12160/34075 (36%)]	 Batch 380 Loss: 0.224036
Train Epoch: 1 Iteration: 390 [12480/34075 (37%)]	 Batch 390 Loss: 0.104602
Train Epoch: 1 Iteration: 400 [12800/34075 (38%)]	 Batch 400 Loss: 0.179610
Train Epoch: 1 Iteration: 410 [13120/34075 (38%)]	 Batch 410 Loss: 0.288735
Train Epoch: 1 Iteration: 420 [13440/34075 (39%)]	 Batch 420 Loss: 0.086681
Train Epoch: 1 Iteration: 430 [13760/34075 (40%)]	 Batch 430 Loss: 0.300086
Train Epoch: 1 Iteration: 440 [14080/34075 (41%)]	 Batch 440 Loss: 0.295799
Train Epoch: 1 Iteration: 450 [14400/34075 (42%)]	 Batch 450 Loss: 0.417405
Train Epoch: 1 Iteration: 460 [14720/34075 (43%)]	 Batch 460 Loss: 0.276008
Train Epoch: 1 Iteration: 470 [15040/34075 (44%)]	 Batch 470 Loss: 0.234500
Train Epoch: 1 Iteration: 480 [15360/34075 (45%)]	 Batch 480 Loss: 0.400849
Train Epoch: 1 Iteration: 490 [15680/34075 (46%)]	 Batch 490 Loss: 0.171678
Train Epoch: 1 Iteration: 500 [16000/34075 (47%)]	 Batch 500 Loss: 0.142862
Train Epoch: 1 Iteration: 510 [16320/34075 (48%)]	 Batch 510 Loss: 0.149544
Train Epoch: 1 Iteration: 520 [16640/34075 (49%)]	 Batch 520 Loss: 0.156892
Train Epoch: 1 Iteration: 530 [16960/34075 (50%)]	 Batch 530 Loss: 0.137709
Train Epoch: 1 Iteration: 540 [17280/34075 (51%)]	 Batch 540 Loss: 0.190436
Train Epoch: 1 Iteration: 550 [17600/34075 (52%)]	 Batch 550 Loss: 0.221660
Train Epoch: 1 Iteration: 560 [17920/34075 (53%)]	 Batch 560 Loss: 0.149883
Train Epoch: 1 Iteration: 570 [18240/34075 (54%)]	 Batch 570 Loss: 0.334072
Train Epoch: 1 Iteration: 580 [18560/34075 (54%)]	 Batch 580 Loss: 0.268799
Train Epoch: 1 Iteration: 590 [18880/34075 (55%)]	 Batch 590 Loss: 0.298565
Train Epoch: 1 Iteration: 600 [19200/34075 (56%)]	 Batch 600 Loss: 0.220730
Train Epoch: 1 Iteration: 610 [19520/34075 (57%)]	 Batch 610 Loss: 0.563225
Train Epoch: 1 Iteration: 620 [19840/34075 (58%)]	 Batch 620 Loss: 0.145000
Train Epoch: 1 Iteration: 630 [20160/34075 (59%)]	 Batch 630 Loss: 0.122843
Train Epoch: 1 Iteration: 640 [20480/34075 (60%)]	 Batch 640 Loss: 0.302512
Train Epoch: 1 Iteration: 650 [20800/34075 (61%)]	 Batch 650 Loss: 0.137925
Train Epoch: 1 Iteration: 660 [21120/34075 (62%)]	 Batch 660 Loss: 0.390232
Train Epoch: 1 Iteration: 670 [21440/34075 (63%)]	 Batch 670 Loss: 0.233301
Train Epoch: 1 Iteration: 680 [21760/34075 (64%)]	 Batch 680 Loss: 0.242161
Train Epoch: 1 Iteration: 690 [22080/34075 (65%)]	 Batch 690 Loss: 0.154880
Train Epoch: 1 Iteration: 700 [22400/34075 (66%)]	 Batch 700 Loss: 0.403339
Train Epoch: 1 Iteration: 710 [22720/34075 (67%)]	 Batch 710 Loss: 0.168185
Train Epoch: 1 Iteration: 720 [23040/34075 (68%)]	 Batch 720 Loss: 0.137108
Train Epoch: 1 Iteration: 730 [23360/34075 (69%)]	 Batch 730 Loss: 0.362118
Train Epoch: 1 Iteration: 740 [23680/34075 (69%)]	 Batch 740 Loss: 0.174437
Train Epoch: 1 Iteration: 750 [24000/34075 (70%)]	 Batch 750 Loss: 0.272621
Train Epoch: 1 Iteration: 760 [24320/34075 (71%)]	 Batch 760 Loss: 0.159252
Train Epoch: 1 Iteration: 770 [24640/34075 (72%)]	 Batch 770 Loss: 0.143660
Train Epoch: 1 Iteration: 780 [24960/34075 (73%)]	 Batch 780 Loss: 0.280807
Train Epoch: 1 Iteration: 790 [25280/34075 (74%)]	 Batch 790 Loss: 0.205266
Train Epoch: 1 Iteration: 800 [25600/34075 (75%)]	 Batch 800 Loss: 0.136222
Train Epoch: 1 Iteration: 810 [25920/34075 (76%)]	 Batch 810 Loss: 0.228950
Train Epoch: 1 Iteration: 820 [26240/34075 (77%)]	 Batch 820 Loss: 0.158775
Train Epoch: 1 Iteration: 830 [26560/34075 (78%)]	 Batch 830 Loss: 0.151960
Train Epoch: 1 Iteration: 840 [26880/34075 (79%)]	 Batch 840 Loss: 0.307680
Train Epoch: 1 Iteration: 850 [27200/34075 (80%)]	 Batch 850 Loss: 0.187663
Train Epoch: 1 Iteration: 860 [27520/34075 (81%)]	 Batch 860 Loss: 0.156087
Train Epoch: 1 Iteration: 870 [27840/34075 (82%)]	 Batch 870 Loss: 0.183338
Train Epoch: 1 Iteration: 880 [28160/34075 (83%)]	 Batch 880 Loss: 0.350337
Train Epoch: 1 Iteration: 890 [28480/34075 (84%)]	 Batch 890 Loss: 0.246097
Train Epoch: 1 Iteration: 900 [28800/34075 (85%)]	 Batch 900 Loss: 0.336204
Train Epoch: 1 Iteration: 910 [29120/34075 (85%)]	 Batch 910 Loss: 0.494524
Train Epoch: 1 Iteration: 920 [29440/34075 (86%)]	 Batch 920 Loss: 0.109044
Train Epoch: 1 Iteration: 930 [29760/34075 (87%)]	 Batch 930 Loss: 0.385330
Train Epoch: 1 Iteration: 940 [30080/34075 (88%)]	 Batch 940 Loss: 0.317684
Train Epoch: 1 Iteration: 950 [30400/34075 (89%)]	 Batch 950 Loss: 0.132733
Train Epoch: 1 Iteration: 960 [30720/34075 (90%)]	 Batch 960 Loss: 0.139085
Train Epoch: 1 Iteration: 970 [31040/34075 (91%)]	 Batch 970 Loss: 0.167856
Train Epoch: 1 Iteration: 980 [31360/34075 (92%)]	 Batch 980 Loss: 0.172789
Train Epoch: 1 Iteration: 990 [31680/34075 (93%)]	 Batch 990 Loss: 0.127495
Train Epoch: 1 Iteration: 1000 [32000/34075 (94%)]	 Batch 1000 Loss: 0.157465
Train Epoch: 1 Iteration: 1010 [32320/34075 (95%)]	 Batch 1010 Loss: 0.183787
Train Epoch: 1 Iteration: 1020 [32640/34075 (96%)]	 Batch 1020 Loss: 0.562786
Train Epoch: 1 Iteration: 1030 [32960/34075 (97%)]	 Batch 1030 Loss: 0.400437
Train Epoch: 1 Iteration: 1040 [33280/34075 (98%)]	 Batch 1040 Loss: 0.325410
Train Epoch: 1 Iteration: 1050 [33600/34075 (99%)]	 Batch 1050 Loss: 0.188385
Train Epoch: 1 Iteration: 1060 [33920/34075 (100%)]	 Batch 1060 Loss: 0.145574


----------------- Epoch 1 -----------------

validation computation time: 8.0  minutes

Validation Loss: 0.8065
Training Loss:0.2275
Lowest Validation Loss: 0.806498
Time Elapsed: 1h 3m 46s

--------------------------------------------------------


Train Epoch: 2 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 0.103068
Train Epoch: 2 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 0.183633
Train Epoch: 2 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 0.188955
Train Epoch: 2 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 0.147984
Train Epoch: 2 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 0.105946
Train Epoch: 2 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 0.401283
Train Epoch: 2 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 0.138407
Train Epoch: 2 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 0.119437
Train Epoch: 2 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 0.224041
Train Epoch: 2 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 0.173918
Train Epoch: 2 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.199619
Train Epoch: 2 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.220666
Train Epoch: 2 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.150040
Train Epoch: 2 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.201000
Train Epoch: 2 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.138425
Train Epoch: 2 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.152013
Train Epoch: 2 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 0.320659
Train Epoch: 2 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 0.715471
Train Epoch: 2 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 0.091939
Train Epoch: 2 Iteration: 200 [6400/34075 (19%)]	 Batch 200 Loss: 0.110253
Train Epoch: 2 Iteration: 210 [6720/34075 (20%)]	 Batch 210 Loss: 0.136930
Train Epoch: 2 Iteration: 220 [7040/34075 (21%)]	 Batch 220 Loss: 0.130751
Train Epoch: 2 Iteration: 230 [7360/34075 (22%)]	 Batch 230 Loss: 0.089614
Train Epoch: 2 Iteration: 240 [7680/34075 (23%)]	 Batch 240 Loss: 0.322739
Train Epoch: 2 Iteration: 250 [8000/34075 (23%)]	 Batch 250 Loss: 0.124577
Train Epoch: 2 Iteration: 260 [8320/34075 (24%)]	 Batch 260 Loss: 0.178115
Train Epoch: 2 Iteration: 270 [8640/34075 (25%)]	 Batch 270 Loss: 0.196094
Train Epoch: 2 Iteration: 280 [8960/34075 (26%)]	 Batch 280 Loss: 0.345092
Train Epoch: 2 Iteration: 290 [9280/34075 (27%)]	 Batch 290 Loss: 0.183701
Train Epoch: 2 Iteration: 300 [9600/34075 (28%)]	 Batch 300 Loss: 0.300330
Train Epoch: 2 Iteration: 310 [9920/34075 (29%)]	 Batch 310 Loss: 0.122290
Train Epoch: 2 Iteration: 320 [10240/34075 (30%)]	 Batch 320 Loss: 0.210272
Train Epoch: 2 Iteration: 330 [10560/34075 (31%)]	 Batch 330 Loss: 0.133018
Train Epoch: 2 Iteration: 340 [10880/34075 (32%)]	 Batch 340 Loss: 0.210045
Train Epoch: 2 Iteration: 350 [11200/34075 (33%)]	 Batch 350 Loss: 0.099773
Train Epoch: 2 Iteration: 360 [11520/34075 (34%)]	 Batch 360 Loss: 0.135176
Train Epoch: 2 Iteration: 370 [11840/34075 (35%)]	 Batch 370 Loss: 0.112443
Train Epoch: 2 Iteration: 380 [12160/34075 (36%)]	 Batch 380 Loss: 0.134133
Train Epoch: 2 Iteration: 390 [12480/34075 (37%)]	 Batch 390 Loss: 0.193320
Train Epoch: 2 Iteration: 400 [12800/34075 (38%)]	 Batch 400 Loss: 0.156030
Train Epoch: 2 Iteration: 410 [13120/34075 (38%)]	 Batch 410 Loss: 0.192035
Train Epoch: 2 Iteration: 420 [13440/34075 (39%)]	 Batch 420 Loss: 0.162541
Train Epoch: 2 Iteration: 430 [13760/34075 (40%)]	 Batch 430 Loss: 0.198316
Train Epoch: 2 Iteration: 440 [14080/34075 (41%)]	 Batch 440 Loss: 0.349461
Train Epoch: 2 Iteration: 450 [14400/34075 (42%)]	 Batch 450 Loss: 0.184806
Train Epoch: 2 Iteration: 460 [14720/34075 (43%)]	 Batch 460 Loss: 0.114571
Train Epoch: 2 Iteration: 470 [15040/34075 (44%)]	 Batch 470 Loss: 0.200344
Train Epoch: 2 Iteration: 480 [15360/34075 (45%)]	 Batch 480 Loss: 0.103060
Train Epoch: 2 Iteration: 490 [15680/34075 (46%)]	 Batch 490 Loss: 0.233489
Train Epoch: 2 Iteration: 500 [16000/34075 (47%)]	 Batch 500 Loss: 0.230300
Train Epoch: 2 Iteration: 510 [16320/34075 (48%)]	 Batch 510 Loss: 0.099481
Train Epoch: 2 Iteration: 520 [16640/34075 (49%)]	 Batch 520 Loss: 0.176382
Train Epoch: 2 Iteration: 530 [16960/34075 (50%)]	 Batch 530 Loss: 0.174814
Train Epoch: 2 Iteration: 540 [17280/34075 (51%)]	 Batch 540 Loss: 0.200417
Train Epoch: 2 Iteration: 550 [17600/34075 (52%)]	 Batch 550 Loss: 0.211027
Train Epoch: 2 Iteration: 560 [17920/34075 (53%)]	 Batch 560 Loss: 0.140803
Train Epoch: 2 Iteration: 570 [18240/34075 (54%)]	 Batch 570 Loss: 0.155743
Train Epoch: 2 Iteration: 580 [18560/34075 (54%)]	 Batch 580 Loss: 0.204295
Train Epoch: 2 Iteration: 590 [18880/34075 (55%)]	 Batch 590 Loss: 0.234016
Train Epoch: 2 Iteration: 600 [19200/34075 (56%)]	 Batch 600 Loss: 0.121566
Train Epoch: 2 Iteration: 610 [19520/34075 (57%)]	 Batch 610 Loss: 0.097282
Train Epoch: 2 Iteration: 620 [19840/34075 (58%)]	 Batch 620 Loss: 0.109799
Train Epoch: 2 Iteration: 630 [20160/34075 (59%)]	 Batch 630 Loss: 0.067234
Train Epoch: 2 Iteration: 640 [20480/34075 (60%)]	 Batch 640 Loss: 0.100853
Train Epoch: 2 Iteration: 650 [20800/34075 (61%)]	 Batch 650 Loss: 0.133722
Train Epoch: 2 Iteration: 660 [21120/34075 (62%)]	 Batch 660 Loss: 0.109860
Train Epoch: 2 Iteration: 670 [21440/34075 (63%)]	 Batch 670 Loss: 0.124920
Train Epoch: 2 Iteration: 680 [21760/34075 (64%)]	 Batch 680 Loss: 0.275813
Train Epoch: 2 Iteration: 690 [22080/34075 (65%)]	 Batch 690 Loss: 0.182714
Train Epoch: 2 Iteration: 700 [22400/34075 (66%)]	 Batch 700 Loss: 0.148024
Train Epoch: 2 Iteration: 710 [22720/34075 (67%)]	 Batch 710 Loss: 0.143137
Train Epoch: 2 Iteration: 720 [23040/34075 (68%)]	 Batch 720 Loss: 0.173688
Train Epoch: 2 Iteration: 730 [23360/34075 (69%)]	 Batch 730 Loss: 0.179827
Train Epoch: 2 Iteration: 740 [23680/34075 (69%)]	 Batch 740 Loss: 0.088135
Train Epoch: 2 Iteration: 750 [24000/34075 (70%)]	 Batch 750 Loss: 0.207696
Train Epoch: 2 Iteration: 760 [24320/34075 (71%)]	 Batch 760 Loss: 0.160750
Train Epoch: 2 Iteration: 770 [24640/34075 (72%)]	 Batch 770 Loss: 0.076023
Train Epoch: 2 Iteration: 780 [24960/34075 (73%)]	 Batch 780 Loss: 0.110789
Train Epoch: 2 Iteration: 790 [25280/34075 (74%)]	 Batch 790 Loss: 0.136116
Train Epoch: 2 Iteration: 800 [25600/34075 (75%)]	 Batch 800 Loss: 0.149629
Train Epoch: 2 Iteration: 810 [25920/34075 (76%)]	 Batch 810 Loss: 0.097014
Train Epoch: 2 Iteration: 820 [26240/34075 (77%)]	 Batch 820 Loss: 0.131350
Train Epoch: 2 Iteration: 830 [26560/34075 (78%)]	 Batch 830 Loss: 0.273737
Train Epoch: 2 Iteration: 840 [26880/34075 (79%)]	 Batch 840 Loss: 0.180366
Train Epoch: 2 Iteration: 850 [27200/34075 (80%)]	 Batch 850 Loss: 0.122684
Train Epoch: 2 Iteration: 860 [27520/34075 (81%)]	 Batch 860 Loss: 0.113828
Train Epoch: 2 Iteration: 870 [27840/34075 (82%)]	 Batch 870 Loss: 0.095816
Train Epoch: 2 Iteration: 880 [28160/34075 (83%)]	 Batch 880 Loss: 0.185074
Train Epoch: 2 Iteration: 890 [28480/34075 (84%)]	 Batch 890 Loss: 0.101398
Train Epoch: 2 Iteration: 900 [28800/34075 (85%)]	 Batch 900 Loss: 0.107160
Train Epoch: 2 Iteration: 910 [29120/34075 (85%)]	 Batch 910 Loss: 0.234471
Train Epoch: 2 Iteration: 920 [29440/34075 (86%)]	 Batch 920 Loss: 0.109153
Train Epoch: 2 Iteration: 930 [29760/34075 (87%)]	 Batch 930 Loss: 0.105330
Train Epoch: 2 Iteration: 940 [30080/34075 (88%)]	 Batch 940 Loss: 0.107342
Train Epoch: 2 Iteration: 950 [30400/34075 (89%)]	 Batch 950 Loss: 0.092260
Train Epoch: 2 Iteration: 960 [30720/34075 (90%)]	 Batch 960 Loss: 0.083035
Train Epoch: 2 Iteration: 970 [31040/34075 (91%)]	 Batch 970 Loss: 0.083933
Train Epoch: 2 Iteration: 980 [31360/34075 (92%)]	 Batch 980 Loss: 0.282141
Train Epoch: 2 Iteration: 990 [31680/34075 (93%)]	 Batch 990 Loss: 0.079457
Train Epoch: 2 Iteration: 1000 [32000/34075 (94%)]	 Batch 1000 Loss: 0.697887
Train Epoch: 2 Iteration: 1010 [32320/34075 (95%)]	 Batch 1010 Loss: 0.098668
Train Epoch: 2 Iteration: 1020 [32640/34075 (96%)]	 Batch 1020 Loss: 0.126778
Train Epoch: 2 Iteration: 1030 [32960/34075 (97%)]	 Batch 1030 Loss: 0.077721
Train Epoch: 2 Iteration: 1040 [33280/34075 (98%)]	 Batch 1040 Loss: 0.107110
Train Epoch: 2 Iteration: 1050 [33600/34075 (99%)]	 Batch 1050 Loss: 0.135025
Train Epoch: 2 Iteration: 1060 [33920/34075 (100%)]	 Batch 1060 Loss: 0.143738


----------------- Epoch 2 -----------------

validation computation time: 8.0  minutes

Validation Loss: 0.7953
Training Loss:0.1652
Lowest Validation Loss: 0.795274
Time Elapsed: 1h 40m 39s

--------------------------------------------------------


Train Epoch: 3 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 0.074976
Train Epoch: 3 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 0.117817
Train Epoch: 3 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 0.055629
Train Epoch: 3 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 0.113192
Train Epoch: 3 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 0.096218
Train Epoch: 3 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 0.096282
Train Epoch: 3 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 0.100150
Train Epoch: 3 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 0.119879
Train Epoch: 3 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 0.119903
Train Epoch: 3 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 0.129576
Train Epoch: 3 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.115902
Train Epoch: 3 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.076845
Train Epoch: 3 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.080105
Train Epoch: 3 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.074294
Train Epoch: 3 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.102681
Train Epoch: 3 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.109941
Train Epoch: 3 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 0.152392
Train Epoch: 3 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 0.131856
Train Epoch: 3 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 0.132641
Train Epoch: 3 Iteration: 200 [6400/34075 (19%)]	 Batch 200 Loss: 0.216690
Train Epoch: 3 Iteration: 210 [6720/34075 (20%)]	 Batch 210 Loss: 0.114679
Train Epoch: 3 Iteration: 220 [7040/34075 (21%)]	 Batch 220 Loss: 0.062742
Train Epoch: 3 Iteration: 230 [7360/34075 (22%)]	 Batch 230 Loss: 0.095010
Train Epoch: 3 Iteration: 240 [7680/34075 (23%)]	 Batch 240 Loss: 0.121707
Train Epoch: 3 Iteration: 250 [8000/34075 (23%)]	 Batch 250 Loss: 0.101799
Train Epoch: 3 Iteration: 260 [8320/34075 (24%)]	 Batch 260 Loss: 0.059417
Train Epoch: 3 Iteration: 270 [8640/34075 (25%)]	 Batch 270 Loss: 0.190028
Train Epoch: 3 Iteration: 280 [8960/34075 (26%)]	 Batch 280 Loss: 0.107672
Train Epoch: 3 Iteration: 290 [9280/34075 (27%)]	 Batch 290 Loss: 0.048048
Train Epoch: 3 Iteration: 300 [9600/34075 (28%)]	 Batch 300 Loss: 0.071775
Train Epoch: 3 Iteration: 310 [9920/34075 (29%)]	 Batch 310 Loss: 0.085161
Train Epoch: 3 Iteration: 320 [10240/34075 (30%)]	 Batch 320 Loss: 0.086968
Train Epoch: 3 Iteration: 330 [10560/34075 (31%)]	 Batch 330 Loss: 0.210798
Train Epoch: 3 Iteration: 340 [10880/34075 (32%)]	 Batch 340 Loss: 0.159850
Train Epoch: 3 Iteration: 350 [11200/34075 (33%)]	 Batch 350 Loss: 0.093913
Train Epoch: 3 Iteration: 360 [11520/34075 (34%)]	 Batch 360 Loss: 0.138600
Train Epoch: 3 Iteration: 370 [11840/34075 (35%)]	 Batch 370 Loss: 0.110118
Train Epoch: 3 Iteration: 380 [12160/34075 (36%)]	 Batch 380 Loss: 0.111214
Train Epoch: 3 Iteration: 390 [12480/34075 (37%)]	 Batch 390 Loss: 0.107589
Train Epoch: 3 Iteration: 400 [12800/34075 (38%)]	 Batch 400 Loss: 0.184421
Train Epoch: 3 Iteration: 410 [13120/34075 (38%)]	 Batch 410 Loss: 0.108294
Train Epoch: 3 Iteration: 420 [13440/34075 (39%)]	 Batch 420 Loss: 0.117544
Train Epoch: 3 Iteration: 430 [13760/34075 (40%)]	 Batch 430 Loss: 0.054647
Train Epoch: 3 Iteration: 440 [14080/34075 (41%)]	 Batch 440 Loss: 0.111830
Train Epoch: 3 Iteration: 450 [14400/34075 (42%)]	 Batch 450 Loss: 0.094647
Train Epoch: 3 Iteration: 460 [14720/34075 (43%)]	 Batch 460 Loss: 0.099021
Train Epoch: 3 Iteration: 470 [15040/34075 (44%)]	 Batch 470 Loss: 0.207025
Train Epoch: 3 Iteration: 480 [15360/34075 (45%)]	 Batch 480 Loss: 0.109047
Train Epoch: 3 Iteration: 490 [15680/34075 (46%)]	 Batch 490 Loss: 0.137193
Train Epoch: 3 Iteration: 500 [16000/34075 (47%)]	 Batch 500 Loss: 0.084865
Train Epoch: 3 Iteration: 510 [16320/34075 (48%)]	 Batch 510 Loss: 0.142377
Train Epoch: 3 Iteration: 520 [16640/34075 (49%)]	 Batch 520 Loss: 0.094645
Train Epoch: 3 Iteration: 530 [16960/34075 (50%)]	 Batch 530 Loss: 0.062287
Train Epoch: 3 Iteration: 540 [17280/34075 (51%)]	 Batch 540 Loss: 0.181205
Train Epoch: 3 Iteration: 550 [17600/34075 (52%)]	 Batch 550 Loss: 0.258015
Train Epoch: 3 Iteration: 560 [17920/34075 (53%)]	 Batch 560 Loss: 0.153018
Train Epoch: 3 Iteration: 570 [18240/34075 (54%)]	 Batch 570 Loss: 0.095434
Train Epoch: 3 Iteration: 580 [18560/34075 (54%)]	 Batch 580 Loss: 0.142055
Train Epoch: 3 Iteration: 590 [18880/34075 (55%)]	 Batch 590 Loss: 0.119719
Train Epoch: 3 Iteration: 600 [19200/34075 (56%)]	 Batch 600 Loss: 0.089124
Train Epoch: 3 Iteration: 610 [19520/34075 (57%)]	 Batch 610 Loss: 0.128297
Train Epoch: 3 Iteration: 620 [19840/34075 (58%)]	 Batch 620 Loss: 0.231233
Train Epoch: 3 Iteration: 630 [20160/34075 (59%)]	 Batch 630 Loss: 0.064808
Train Epoch: 3 Iteration: 640 [20480/34075 (60%)]	 Batch 640 Loss: 0.120592
Train Epoch: 3 Iteration: 650 [20800/34075 (61%)]	 Batch 650 Loss: 0.235585
Train Epoch: 3 Iteration: 660 [21120/34075 (62%)]	 Batch 660 Loss: 0.063687
Train Epoch: 3 Iteration: 670 [21440/34075 (63%)]	 Batch 670 Loss: 0.173482
Train Epoch: 3 Iteration: 680 [21760/34075 (64%)]	 Batch 680 Loss: 0.120871
Train Epoch: 3 Iteration: 690 [22080/34075 (65%)]	 Batch 690 Loss: 0.049799
Train Epoch: 3 Iteration: 700 [22400/34075 (66%)]	 Batch 700 Loss: 0.074442
Train Epoch: 3 Iteration: 710 [22720/34075 (67%)]	 Batch 710 Loss: 0.074545
Train Epoch: 3 Iteration: 720 [23040/34075 (68%)]	 Batch 720 Loss: 0.106868
Train Epoch: 3 Iteration: 730 [23360/34075 (69%)]	 Batch 730 Loss: 0.107789
Train Epoch: 3 Iteration: 740 [23680/34075 (69%)]	 Batch 740 Loss: 0.096753
Train Epoch: 3 Iteration: 750 [24000/34075 (70%)]	 Batch 750 Loss: 0.100736
Train Epoch: 3 Iteration: 760 [24320/34075 (71%)]	 Batch 760 Loss: 0.084696
Train Epoch: 3 Iteration: 770 [24640/34075 (72%)]	 Batch 770 Loss: 0.122309
Train Epoch: 3 Iteration: 780 [24960/34075 (73%)]	 Batch 780 Loss: 0.095816
Train Epoch: 3 Iteration: 790 [25280/34075 (74%)]	 Batch 790 Loss: 0.080630
Train Epoch: 3 Iteration: 800 [25600/34075 (75%)]	 Batch 800 Loss: 0.123970
Train Epoch: 3 Iteration: 810 [25920/34075 (76%)]	 Batch 810 Loss: 0.107660
Train Epoch: 3 Iteration: 820 [26240/34075 (77%)]	 Batch 820 Loss: 0.090378
Train Epoch: 3 Iteration: 830 [26560/34075 (78%)]	 Batch 830 Loss: 0.066942
Train Epoch: 3 Iteration: 840 [26880/34075 (79%)]	 Batch 840 Loss: 0.107552
Train Epoch: 3 Iteration: 850 [27200/34075 (80%)]	 Batch 850 Loss: 0.121836
Train Epoch: 3 Iteration: 860 [27520/34075 (81%)]	 Batch 860 Loss: 0.070037
Train Epoch: 3 Iteration: 870 [27840/34075 (82%)]	 Batch 870 Loss: 0.088842
Train Epoch: 3 Iteration: 880 [28160/34075 (83%)]	 Batch 880 Loss: 0.053122
Train Epoch: 3 Iteration: 890 [28480/34075 (84%)]	 Batch 890 Loss: 0.074712
Train Epoch: 3 Iteration: 900 [28800/34075 (85%)]	 Batch 900 Loss: 0.075262
Train Epoch: 3 Iteration: 910 [29120/34075 (85%)]	 Batch 910 Loss: 0.098691
Train Epoch: 3 Iteration: 920 [29440/34075 (86%)]	 Batch 920 Loss: 0.140600
Train Epoch: 3 Iteration: 930 [29760/34075 (87%)]	 Batch 930 Loss: 0.094153
Train Epoch: 3 Iteration: 940 [30080/34075 (88%)]	 Batch 940 Loss: 0.122776
Train Epoch: 3 Iteration: 950 [30400/34075 (89%)]	 Batch 950 Loss: 0.123193
Train Epoch: 3 Iteration: 960 [30720/34075 (90%)]	 Batch 960 Loss: 0.100973
Train Epoch: 3 Iteration: 970 [31040/34075 (91%)]	 Batch 970 Loss: 0.144379
Train Epoch: 3 Iteration: 980 [31360/34075 (92%)]	 Batch 980 Loss: 0.208084
Train Epoch: 3 Iteration: 990 [31680/34075 (93%)]	 Batch 990 Loss: 0.136799
Train Epoch: 3 Iteration: 1000 [32000/34075 (94%)]	 Batch 1000 Loss: 0.091396
Train Epoch: 3 Iteration: 1010 [32320/34075 (95%)]	 Batch 1010 Loss: 0.115727
Train Epoch: 3 Iteration: 1020 [32640/34075 (96%)]	 Batch 1020 Loss: 0.150818
Train Epoch: 3 Iteration: 1030 [32960/34075 (97%)]	 Batch 1030 Loss: 0.113541
Train Epoch: 3 Iteration: 1040 [33280/34075 (98%)]	 Batch 1040 Loss: 0.053399
Train Epoch: 3 Iteration: 1050 [33600/34075 (99%)]	 Batch 1050 Loss: 0.123786
Train Epoch: 3 Iteration: 1060 [33920/34075 (100%)]	 Batch 1060 Loss: 0.089148


----------------- Epoch 3 -----------------

validation computation time: 9.0  minutes

Validation Loss: 0.8257
Training Loss:0.1261
Lowest Validation Loss: 0.795274
Time Elapsed: 2h 18m 3s

--------------------------------------------------------


Train Epoch: 4 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 0.078671
Train Epoch: 4 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 0.061649
Train Epoch: 4 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 0.073580
Train Epoch: 4 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 0.091104
Train Epoch: 4 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 0.049463
Train Epoch: 4 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 0.053536
Train Epoch: 4 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 0.058611
Train Epoch: 4 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 0.126221
Train Epoch: 4 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 0.176029
Train Epoch: 4 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 0.103849
Train Epoch: 4 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.062241
Train Epoch: 4 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.070591
Train Epoch: 4 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.071125
Train Epoch: 4 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.080028
Train Epoch: 4 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.149438
Train Epoch: 4 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.066677
Train Epoch: 4 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 0.083592
Train Epoch: 4 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 0.099167
Train Epoch: 4 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 0.173148
Train Epoch: 4 Iteration: 200 [6400/34075 (19%)]	 Batch 200 Loss: 0.087718
Train Epoch: 4 Iteration: 210 [6720/34075 (20%)]	 Batch 210 Loss: 0.086883
Train Epoch: 4 Iteration: 220 [7040/34075 (21%)]	 Batch 220 Loss: 0.077392
Train Epoch: 4 Iteration: 230 [7360/34075 (22%)]	 Batch 230 Loss: 0.098345
Train Epoch: 4 Iteration: 240 [7680/34075 (23%)]	 Batch 240 Loss: 0.064298
Train Epoch: 4 Iteration: 250 [8000/34075 (23%)]	 Batch 250 Loss: 0.101973
Train Epoch: 4 Iteration: 260 [8320/34075 (24%)]	 Batch 260 Loss: 0.051425
Train Epoch: 4 Iteration: 270 [8640/34075 (25%)]	 Batch 270 Loss: 0.082724
Train Epoch: 4 Iteration: 280 [8960/34075 (26%)]	 Batch 280 Loss: 0.067678
Train Epoch: 4 Iteration: 290 [9280/34075 (27%)]	 Batch 290 Loss: 0.092887
Train Epoch: 4 Iteration: 300 [9600/34075 (28%)]	 Batch 300 Loss: 0.071553
Train Epoch: 4 Iteration: 310 [9920/34075 (29%)]	 Batch 310 Loss: 0.073232
Train Epoch: 4 Iteration: 320 [10240/34075 (30%)]	 Batch 320 Loss: 0.079194
Train Epoch: 4 Iteration: 330 [10560/34075 (31%)]	 Batch 330 Loss: 0.064833
Train Epoch: 4 Iteration: 340 [10880/34075 (32%)]	 Batch 340 Loss: 0.106155
Train Epoch: 4 Iteration: 350 [11200/34075 (33%)]	 Batch 350 Loss: 0.080126
Train Epoch: 4 Iteration: 360 [11520/34075 (34%)]	 Batch 360 Loss: 0.075866
Train Epoch: 4 Iteration: 370 [11840/34075 (35%)]	 Batch 370 Loss: 0.098204
Train Epoch: 4 Iteration: 380 [12160/34075 (36%)]	 Batch 380 Loss: 0.076018
Train Epoch: 4 Iteration: 390 [12480/34075 (37%)]	 Batch 390 Loss: 0.086792
Train Epoch: 4 Iteration: 400 [12800/34075 (38%)]	 Batch 400 Loss: 0.075790
Train Epoch: 4 Iteration: 410 [13120/34075 (38%)]	 Batch 410 Loss: 0.086093
Train Epoch: 4 Iteration: 420 [13440/34075 (39%)]	 Batch 420 Loss: 0.307984
Train Epoch: 4 Iteration: 430 [13760/34075 (40%)]	 Batch 430 Loss: 0.104801
Train Epoch: 4 Iteration: 440 [14080/34075 (41%)]	 Batch 440 Loss: 0.272553
Train Epoch: 4 Iteration: 450 [14400/34075 (42%)]	 Batch 450 Loss: 0.083081
Train Epoch: 4 Iteration: 460 [14720/34075 (43%)]	 Batch 460 Loss: 0.248147
Train Epoch: 4 Iteration: 470 [15040/34075 (44%)]	 Batch 470 Loss: 0.154541
Train Epoch: 4 Iteration: 480 [15360/34075 (45%)]	 Batch 480 Loss: 0.126562
Train Epoch: 4 Iteration: 490 [15680/34075 (46%)]	 Batch 490 Loss: 0.128571
Train Epoch: 4 Iteration: 500 [16000/34075 (47%)]	 Batch 500 Loss: 0.166822
Train Epoch: 4 Iteration: 510 [16320/34075 (48%)]	 Batch 510 Loss: 0.137279
Train Epoch: 4 Iteration: 520 [16640/34075 (49%)]	 Batch 520 Loss: 0.098222
Train Epoch: 4 Iteration: 530 [16960/34075 (50%)]	 Batch 530 Loss: 0.190230
Train Epoch: 4 Iteration: 540 [17280/34075 (51%)]	 Batch 540 Loss: 0.060523
Train Epoch: 4 Iteration: 550 [17600/34075 (52%)]	 Batch 550 Loss: 0.071524
Train Epoch: 4 Iteration: 560 [17920/34075 (53%)]	 Batch 560 Loss: 0.117969
Train Epoch: 4 Iteration: 570 [18240/34075 (54%)]	 Batch 570 Loss: 0.150860
Train Epoch: 4 Iteration: 580 [18560/34075 (54%)]	 Batch 580 Loss: 0.108992
Train Epoch: 4 Iteration: 590 [18880/34075 (55%)]	 Batch 590 Loss: 0.057794
Train Epoch: 4 Iteration: 600 [19200/34075 (56%)]	 Batch 600 Loss: 0.043572
Train Epoch: 4 Iteration: 610 [19520/34075 (57%)]	 Batch 610 Loss: 0.121951
Train Epoch: 4 Iteration: 620 [19840/34075 (58%)]	 Batch 620 Loss: 0.112006
Train Epoch: 4 Iteration: 630 [20160/34075 (59%)]	 Batch 630 Loss: 0.074451
Train Epoch: 4 Iteration: 640 [20480/34075 (60%)]	 Batch 640 Loss: 0.108159
Train Epoch: 4 Iteration: 650 [20800/34075 (61%)]	 Batch 650 Loss: 0.122376
Train Epoch: 4 Iteration: 660 [21120/34075 (62%)]	 Batch 660 Loss: 0.167910
Train Epoch: 4 Iteration: 670 [21440/34075 (63%)]	 Batch 670 Loss: 0.110744
Train Epoch: 4 Iteration: 680 [21760/34075 (64%)]	 Batch 680 Loss: 0.208510
Train Epoch: 4 Iteration: 690 [22080/34075 (65%)]	 Batch 690 Loss: 0.081468
Train Epoch: 4 Iteration: 700 [22400/34075 (66%)]	 Batch 700 Loss: 0.113212
Train Epoch: 4 Iteration: 710 [22720/34075 (67%)]	 Batch 710 Loss: 0.050611
Train Epoch: 4 Iteration: 720 [23040/34075 (68%)]	 Batch 720 Loss: 0.079587
Train Epoch: 4 Iteration: 730 [23360/34075 (69%)]	 Batch 730 Loss: 0.088651
Train Epoch: 4 Iteration: 740 [23680/34075 (69%)]	 Batch 740 Loss: 0.048461
Train Epoch: 4 Iteration: 750 [24000/34075 (70%)]	 Batch 750 Loss: 0.330502
Train Epoch: 4 Iteration: 760 [24320/34075 (71%)]	 Batch 760 Loss: 0.066072
Train Epoch: 4 Iteration: 770 [24640/34075 (72%)]	 Batch 770 Loss: 0.076645
Train Epoch: 4 Iteration: 780 [24960/34075 (73%)]	 Batch 780 Loss: 0.239329
Train Epoch: 4 Iteration: 790 [25280/34075 (74%)]	 Batch 790 Loss: 0.067688
Train Epoch: 4 Iteration: 800 [25600/34075 (75%)]	 Batch 800 Loss: 0.075566
Train Epoch: 4 Iteration: 810 [25920/34075 (76%)]	 Batch 810 Loss: 0.092164
Train Epoch: 4 Iteration: 820 [26240/34075 (77%)]	 Batch 820 Loss: 0.087945
Train Epoch: 4 Iteration: 830 [26560/34075 (78%)]	 Batch 830 Loss: 0.089433
Train Epoch: 4 Iteration: 840 [26880/34075 (79%)]	 Batch 840 Loss: 0.104031
Train Epoch: 4 Iteration: 850 [27200/34075 (80%)]	 Batch 850 Loss: 0.047688
Train Epoch: 4 Iteration: 860 [27520/34075 (81%)]	 Batch 860 Loss: 0.070208
Train Epoch: 4 Iteration: 870 [27840/34075 (82%)]	 Batch 870 Loss: 0.099903
Train Epoch: 4 Iteration: 880 [28160/34075 (83%)]	 Batch 880 Loss: 0.107139
Train Epoch: 4 Iteration: 890 [28480/34075 (84%)]	 Batch 890 Loss: 0.065850
Train Epoch: 4 Iteration: 900 [28800/34075 (85%)]	 Batch 900 Loss: 0.073285
Train Epoch: 4 Iteration: 910 [29120/34075 (85%)]	 Batch 910 Loss: 0.265447
Train Epoch: 4 Iteration: 920 [29440/34075 (86%)]	 Batch 920 Loss: 0.113875
Train Epoch: 4 Iteration: 930 [29760/34075 (87%)]	 Batch 930 Loss: 0.102015
Train Epoch: 4 Iteration: 940 [30080/34075 (88%)]	 Batch 940 Loss: 0.071583
Train Epoch: 4 Iteration: 950 [30400/34075 (89%)]	 Batch 950 Loss: 0.072242
Train Epoch: 4 Iteration: 960 [30720/34075 (90%)]	 Batch 960 Loss: 0.235813
Train Epoch: 4 Iteration: 970 [31040/34075 (91%)]	 Batch 970 Loss: 0.047080
Train Epoch: 4 Iteration: 980 [31360/34075 (92%)]	 Batch 980 Loss: 0.066302
Train Epoch: 4 Iteration: 990 [31680/34075 (93%)]	 Batch 990 Loss: 0.120032
Train Epoch: 4 Iteration: 1000 [32000/34075 (94%)]	 Batch 1000 Loss: 0.142026
Train Epoch: 4 Iteration: 1010 [32320/34075 (95%)]	 Batch 1010 Loss: 0.375611
Train Epoch: 4 Iteration: 1020 [32640/34075 (96%)]	 Batch 1020 Loss: 0.100123
Train Epoch: 4 Iteration: 1030 [32960/34075 (97%)]	 Batch 1030 Loss: 0.062010
Train Epoch: 4 Iteration: 1040 [33280/34075 (98%)]	 Batch 1040 Loss: 0.165561
Train Epoch: 4 Iteration: 1050 [33600/34075 (99%)]	 Batch 1050 Loss: 0.070213
Train Epoch: 4 Iteration: 1060 [33920/34075 (100%)]	 Batch 1060 Loss: 0.068333


----------------- Epoch 4 -----------------

validation computation time: 9.0  minutes

Validation Loss: 0.8231
Training Loss:0.1078
Lowest Validation Loss: 0.795274
Time Elapsed: 2h 59m 5s

--------------------------------------------------------


================================ Finished Training ================================
validation computation time: 9.0  minutes

Validation Loss: 0.8243
Lowest Validation Loss: 0.795274
Time Elapsed: 3h 8m 35s
Epochs: [0, 1, 2, 3, 4]
Val_Losses: [0.8842013068497181, 0.8064976556019651, 0.7952740533898274, 0.8257059033753144, 0.8231145961003171]
Train_Losses: [0.4626001354615043, 0.2274608315437808, 0.16518285139148733, 0.1261022791329463, 0.10780767898278353]
