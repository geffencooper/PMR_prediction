============================ Raw Args ============================
Namespace(batch_size=32, classification='n', dropout='y', droput_prob=0.5, gpu_i=0, hidden_init_rand='n', hidden_size=64, imbalanced_sampler='n', input_size=23, l2_reg='y', load_trained='n', log_dest='../models/fusion_NORM_smote_regression-2021-09-01_10-02-26', loss_freq=10, lr=0.002, model_name='PMRfusionNN', normalize='y', num_classes=-1, num_epochs=5, num_layers=1, optim='RMS', regression='y', root_dir='/data/perception-working/Geffen/avec_data/', session_name='fusion_NORM_smote_regression', train_data_dir='SMOTE/', train_labels_csv='labels2.csv', trained_path='none', val_data_dir='none', val_freq=0, val_labels_csv='val_metadata.csv', weight_decay_amnt=0.001, weighted_loss='n')



================================ Start Training ================================

Session Name: fusion_NORM_smote_regression

Model Name: PMRfusionNN

Device: 0  ---->  GeForce GTX 1080 Ti

Hyperparameters:
Batch Size: 32
Learning Rate: 0.002
Hidden Size: 64
Number of Layer: 1
Number of Epochs: 5
Normalization:y

Train Epoch: 0 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 1.049059
Train Epoch: 0 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 1.119772
Train Epoch: 0 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 1.049222
Train Epoch: 0 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 0.968863
Train Epoch: 0 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 0.698900
Train Epoch: 0 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 1.436369
Train Epoch: 0 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 1.104617
Train Epoch: 0 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 0.879624
Train Epoch: 0 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 0.813332
Train Epoch: 0 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 0.926020
Train Epoch: 0 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.720900
Train Epoch: 0 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.801800
Train Epoch: 0 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.754206
Train Epoch: 0 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.396036
Train Epoch: 0 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.640523
Train Epoch: 0 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.790786
Train Epoch: 0 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 1.005880
Train Epoch: 0 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 1.163339
Train Epoch: 0 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 1.036312
Train Epoch: 0 Iteration: 200 [6400/34075 (19%)]	 Batch 200 Loss: 0.693342
Train Epoch: 0 Iteration: 210 [6720/34075 (20%)]	 Batch 210 Loss: 0.533610
Train Epoch: 0 Iteration: 220 [7040/34075 (21%)]	 Batch 220 Loss: 0.444894
Train Epoch: 0 Iteration: 230 [7360/34075 (22%)]	 Batch 230 Loss: 0.446330
Train Epoch: 0 Iteration: 240 [7680/34075 (23%)]	 Batch 240 Loss: 0.729405
Train Epoch: 0 Iteration: 250 [8000/34075 (23%)]	 Batch 250 Loss: 1.169766
Train Epoch: 0 Iteration: 260 [8320/34075 (24%)]	 Batch 260 Loss: 0.628911
Train Epoch: 0 Iteration: 270 [8640/34075 (25%)]	 Batch 270 Loss: 0.273096
Train Epoch: 0 Iteration: 280 [8960/34075 (26%)]	 Batch 280 Loss: 0.518411
Train Epoch: 0 Iteration: 290 [9280/34075 (27%)]	 Batch 290 Loss: 0.573627
Train Epoch: 0 Iteration: 300 [9600/34075 (28%)]	 Batch 300 Loss: 0.514677
Train Epoch: 0 Iteration: 310 [9920/34075 (29%)]	 Batch 310 Loss: 0.784947
Train Epoch: 0 Iteration: 320 [10240/34075 (30%)]	 Batch 320 Loss: 0.439176
Train Epoch: 0 Iteration: 330 [10560/34075 (31%)]	 Batch 330 Loss: 0.551624
Train Epoch: 0 Iteration: 340 [10880/34075 (32%)]	 Batch 340 Loss: 0.479990
Train Epoch: 0 Iteration: 350 [11200/34075 (33%)]	 Batch 350 Loss: 0.526198
Train Epoch: 0 Iteration: 360 [11520/34075 (34%)]	 Batch 360 Loss: 0.533261
Train Epoch: 0 Iteration: 370 [11840/34075 (35%)]	 Batch 370 Loss: 0.433215
Train Epoch: 0 Iteration: 380 [12160/34075 (36%)]	 Batch 380 Loss: 0.352204
Train Epoch: 0 Iteration: 390 [12480/34075 (37%)]	 Batch 390 Loss: 0.504348
Train Epoch: 0 Iteration: 400 [12800/34075 (38%)]	 Batch 400 Loss: 0.514631
Train Epoch: 0 Iteration: 410 [13120/34075 (38%)]	 Batch 410 Loss: 0.737747
Train Epoch: 0 Iteration: 420 [13440/34075 (39%)]	 Batch 420 Loss: 0.577125
Train Epoch: 0 Iteration: 430 [13760/34075 (40%)]	 Batch 430 Loss: 0.452107
Train Epoch: 0 Iteration: 440 [14080/34075 (41%)]	 Batch 440 Loss: 0.420949
Train Epoch: 0 Iteration: 450 [14400/34075 (42%)]	 Batch 450 Loss: 0.434429
Train Epoch: 0 Iteration: 460 [14720/34075 (43%)]	 Batch 460 Loss: 1.031415
Train Epoch: 0 Iteration: 470 [15040/34075 (44%)]	 Batch 470 Loss: 0.589340
Train Epoch: 0 Iteration: 480 [15360/34075 (45%)]	 Batch 480 Loss: 0.573700
Train Epoch: 0 Iteration: 490 [15680/34075 (46%)]	 Batch 490 Loss: 1.028480
Train Epoch: 0 Iteration: 500 [16000/34075 (47%)]	 Batch 500 Loss: 0.830104
Train Epoch: 0 Iteration: 510 [16320/34075 (48%)]	 Batch 510 Loss: 0.461302
Train Epoch: 0 Iteration: 520 [16640/34075 (49%)]	 Batch 520 Loss: 0.663229
Train Epoch: 0 Iteration: 530 [16960/34075 (50%)]	 Batch 530 Loss: 0.352059
Train Epoch: 0 Iteration: 540 [17280/34075 (51%)]	 Batch 540 Loss: 0.690831
Train Epoch: 0 Iteration: 550 [17600/34075 (52%)]	 Batch 550 Loss: 0.449656
Train Epoch: 0 Iteration: 560 [17920/34075 (53%)]	 Batch 560 Loss: 0.463129
Train Epoch: 0 Iteration: 570 [18240/34075 (54%)]	 Batch 570 Loss: 0.285735
Train Epoch: 0 Iteration: 580 [18560/34075 (54%)]	 Batch 580 Loss: 0.579299
Train Epoch: 0 Iteration: 590 [18880/34075 (55%)]	 Batch 590 Loss: 0.403004
Train Epoch: 0 Iteration: 600 [19200/34075 (56%)]	 Batch 600 Loss: 0.550778
Train Epoch: 0 Iteration: 610 [19520/34075 (57%)]	 Batch 610 Loss: 0.896744
Train Epoch: 0 Iteration: 620 [19840/34075 (58%)]	 Batch 620 Loss: 0.638078
Train Epoch: 0 Iteration: 630 [20160/34075 (59%)]	 Batch 630 Loss: 0.431753
Train Epoch: 0 Iteration: 640 [20480/34075 (60%)]	 Batch 640 Loss: 0.298475
Train Epoch: 0 Iteration: 650 [20800/34075 (61%)]	 Batch 650 Loss: 0.504349
Train Epoch: 0 Iteration: 660 [21120/34075 (62%)]	 Batch 660 Loss: 0.493729
Train Epoch: 0 Iteration: 670 [21440/34075 (63%)]	 Batch 670 Loss: 0.683221
Train Epoch: 0 Iteration: 680 [21760/34075 (64%)]	 Batch 680 Loss: 0.472633
Train Epoch: 0 Iteration: 690 [22080/34075 (65%)]	 Batch 690 Loss: 0.332644
Train Epoch: 0 Iteration: 700 [22400/34075 (66%)]	 Batch 700 Loss: 0.516203
Train Epoch: 0 Iteration: 710 [22720/34075 (67%)]	 Batch 710 Loss: 0.350720
Train Epoch: 0 Iteration: 720 [23040/34075 (68%)]	 Batch 720 Loss: 0.711806
Train Epoch: 0 Iteration: 730 [23360/34075 (69%)]	 Batch 730 Loss: 0.393047
Train Epoch: 0 Iteration: 740 [23680/34075 (69%)]	 Batch 740 Loss: 0.523057
Train Epoch: 0 Iteration: 750 [24000/34075 (70%)]	 Batch 750 Loss: 0.475462
Train Epoch: 0 Iteration: 760 [24320/34075 (71%)]	 Batch 760 Loss: 0.275123
Train Epoch: 0 Iteration: 770 [24640/34075 (72%)]	 Batch 770 Loss: 0.390229
Train Epoch: 0 Iteration: 780 [24960/34075 (73%)]	 Batch 780 Loss: 0.459485
Train Epoch: 0 Iteration: 790 [25280/34075 (74%)]	 Batch 790 Loss: 0.259380
Train Epoch: 0 Iteration: 800 [25600/34075 (75%)]	 Batch 800 Loss: 0.365335
Train Epoch: 0 Iteration: 810 [25920/34075 (76%)]	 Batch 810 Loss: 0.273871
Train Epoch: 0 Iteration: 820 [26240/34075 (77%)]	 Batch 820 Loss: 0.352091
Train Epoch: 0 Iteration: 830 [26560/34075 (78%)]	 Batch 830 Loss: 0.406864
Train Epoch: 0 Iteration: 840 [26880/34075 (79%)]	 Batch 840 Loss: 0.743729
Train Epoch: 0 Iteration: 850 [27200/34075 (80%)]	 Batch 850 Loss: 0.582436
Train Epoch: 0 Iteration: 860 [27520/34075 (81%)]	 Batch 860 Loss: 0.330733
Train Epoch: 0 Iteration: 870 [27840/34075 (82%)]	 Batch 870 Loss: 0.542656
Train Epoch: 0 Iteration: 880 [28160/34075 (83%)]	 Batch 880 Loss: 0.376382
Train Epoch: 0 Iteration: 890 [28480/34075 (84%)]	 Batch 890 Loss: 0.320147
Train Epoch: 0 Iteration: 900 [28800/34075 (85%)]	 Batch 900 Loss: 0.272266
Train Epoch: 0 Iteration: 910 [29120/34075 (85%)]	 Batch 910 Loss: 0.412359
Train Epoch: 0 Iteration: 920 [29440/34075 (86%)]	 Batch 920 Loss: 0.505781
Train Epoch: 0 Iteration: 930 [29760/34075 (87%)]	 Batch 930 Loss: 0.489403
Train Epoch: 0 Iteration: 940 [30080/34075 (88%)]	 Batch 940 Loss: 0.472727
Train Epoch: 0 Iteration: 950 [30400/34075 (89%)]	 Batch 950 Loss: 0.377132
Train Epoch: 0 Iteration: 960 [30720/34075 (90%)]	 Batch 960 Loss: 0.288182
Train Epoch: 0 Iteration: 970 [31040/34075 (91%)]	 Batch 970 Loss: 0.188118
Train Epoch: 0 Iteration: 980 [31360/34075 (92%)]	 Batch 980 Loss: 0.367891
Train Epoch: 0 Iteration: 990 [31680/34075 (93%)]	 Batch 990 Loss: 0.633257
Train Epoch: 0 Iteration: 1000 [32000/34075 (94%)]	 Batch 1000 Loss: 0.513151
Train Epoch: 0 Iteration: 1010 [32320/34075 (95%)]	 Batch 1010 Loss: 0.414834
Train Epoch: 0 Iteration: 1020 [32640/34075 (96%)]	 Batch 1020 Loss: 0.294613
Train Epoch: 0 Iteration: 1030 [32960/34075 (97%)]	 Batch 1030 Loss: 0.467838
Train Epoch: 0 Iteration: 1040 [33280/34075 (98%)]	 Batch 1040 Loss: 0.485080
Train Epoch: 0 Iteration: 1050 [33600/34075 (99%)]	 Batch 1050 Loss: 0.405479
Train Epoch: 0 Iteration: 1060 [33920/34075 (100%)]	 Batch 1060 Loss: 0.559191


----------------- Epoch 0 -----------------

validation computation time: 12.0  minutes

Validation Loss: 1.1169
Training Loss:0.6115
Lowest Validation Loss: 1.116918
Time Elapsed: 1h 1m 53s

--------------------------------------------------------


Train Epoch: 1 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 0.506392
Train Epoch: 1 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 0.283116
Train Epoch: 1 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 0.296023
Train Epoch: 1 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 0.232763
Train Epoch: 1 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 0.520835
Train Epoch: 1 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 0.364086
Train Epoch: 1 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 0.331252
Train Epoch: 1 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 0.400401
Train Epoch: 1 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 0.450431
Train Epoch: 1 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 0.408242
Train Epoch: 1 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.461162
Train Epoch: 1 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.565586
Train Epoch: 1 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.587489
Train Epoch: 1 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.370824
Train Epoch: 1 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.462269
Train Epoch: 1 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.310419
Train Epoch: 1 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 0.311770
Train Epoch: 1 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 0.266300
Train Epoch: 1 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 0.245944
================================ QUIT ================================
 Saving Model ...
