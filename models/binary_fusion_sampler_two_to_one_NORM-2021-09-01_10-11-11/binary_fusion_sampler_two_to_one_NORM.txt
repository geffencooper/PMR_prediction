============================ Raw Args ============================
Namespace(batch_size=32, classification='n', dropout='n', droput_prob=0.0, gpu_i=0, hidden_init_rand='n', hidden_size=64, imbalanced_sampler='n', input_size=23, l2_reg='n', load_trained='n', log_dest='../models/binary_fusion_sampler_two_to_one_NORM-2021-09-01_10-11-11', loss_freq=5, lr=0.001, model_name='PMRfusionNN', normalize='y', num_classes=-1, num_epochs=3, num_layers=1, optim='RMS', regression='y', root_dir='/data/perception-working/Geffen/avec_data/', session_name='binary_fusion_sampler_two_to_one_NORM', train_data_dir='none', train_labels_csv='binary_train_metadata_two_to_one.csv', trained_path='none', val_data_dir='none', val_freq=0, val_labels_csv='binary_val_metadata_two_to_one.csv', weight_decay_amnt=0.0, weighted_loss='y')



================================ Start Training ================================

Session Name: binary_fusion_sampler_two_to_one_NORM

Model Name: PMRfusionNN

Device: 0  ---->  GeForce GTX 1080 Ti

Hyperparameters:
Batch Size: 32
Learning Rate: 0.001
Hidden Size: 64
Number of Layer: 1
Number of Epochs: 3
Normalization:y

Train Epoch: 0 Iteration: 5 [160/1447 (11%)]	 Batch 5 Loss: 1.722732
Train Epoch: 0 Iteration: 10 [320/1447 (22%)]	 Batch 10 Loss: 2.067439
Train Epoch: 0 Iteration: 15 [480/1447 (33%)]	 Batch 15 Loss: 1.763580
Train Epoch: 0 Iteration: 20 [640/1447 (43%)]	 Batch 20 Loss: 1.149951
Train Epoch: 0 Iteration: 25 [800/1447 (54%)]	 Batch 25 Loss: 1.533657
Train Epoch: 0 Iteration: 30 [960/1447 (65%)]	 Batch 30 Loss: 1.351349
Train Epoch: 0 Iteration: 35 [1120/1447 (76%)]	 Batch 35 Loss: 1.326995
Train Epoch: 0 Iteration: 40 [1280/1447 (87%)]	 Batch 40 Loss: 1.643830
Train Epoch: 0 Iteration: 45 [1440/1447 (98%)]	 Batch 45 Loss: 2.285774


----------------- Epoch 0 -----------------

validation computation time: 2.0  minutes

Validation Loss: 1.6199
Training Loss:1.6247
Lowest Validation Loss: 1.619949
Time Elapsed: 0h 10m 30s

--------------------------------------------------------


Train Epoch: 1 Iteration: 5 [160/1447 (11%)]	 Batch 5 Loss: 1.470806
Train Epoch: 1 Iteration: 10 [320/1447 (22%)]	 Batch 10 Loss: 1.583723
Train Epoch: 1 Iteration: 15 [480/1447 (33%)]	 Batch 15 Loss: 0.682176
Train Epoch: 1 Iteration: 20 [640/1447 (43%)]	 Batch 20 Loss: 0.563622
Train Epoch: 1 Iteration: 25 [800/1447 (54%)]	 Batch 25 Loss: 1.176011
Train Epoch: 1 Iteration: 30 [960/1447 (65%)]	 Batch 30 Loss: 1.163122
Train Epoch: 1 Iteration: 35 [1120/1447 (76%)]	 Batch 35 Loss: 2.238132
Train Epoch: 1 Iteration: 40 [1280/1447 (87%)]	 Batch 40 Loss: 0.685444
Train Epoch: 1 Iteration: 45 [1440/1447 (98%)]	 Batch 45 Loss: 0.895232


----------------- Epoch 1 -----------------

validation computation time: 2.0  minutes

Validation Loss: 2.0350
Training Loss:1.0111
Lowest Validation Loss: 1.619949
Time Elapsed: 0h 20m 59s

--------------------------------------------------------


Train Epoch: 2 Iteration: 5 [160/1447 (11%)]	 Batch 5 Loss: 0.231073
Train Epoch: 2 Iteration: 10 [320/1447 (22%)]	 Batch 10 Loss: 0.761349
Train Epoch: 2 Iteration: 15 [480/1447 (33%)]	 Batch 15 Loss: 0.816003
Train Epoch: 2 Iteration: 20 [640/1447 (43%)]	 Batch 20 Loss: 0.625190
Train Epoch: 2 Iteration: 25 [800/1447 (54%)]	 Batch 25 Loss: 0.608066
Train Epoch: 2 Iteration: 30 [960/1447 (65%)]	 Batch 30 Loss: 1.209316
Train Epoch: 2 Iteration: 35 [1120/1447 (76%)]	 Batch 35 Loss: 0.398624
Train Epoch: 2 Iteration: 40 [1280/1447 (87%)]	 Batch 40 Loss: 0.277948
Train Epoch: 2 Iteration: 45 [1440/1447 (98%)]	 Batch 45 Loss: 0.047336


----------------- Epoch 2 -----------------

validation computation time: 2.0  minutes

Validation Loss: 2.2406
Training Loss:0.6094
Lowest Validation Loss: 1.619949
Time Elapsed: 0h 30m 41s

--------------------------------------------------------


================================ Finished Training ================================
validation computation time: 2.0  minutes

Validation Loss: 2.2260
validation results
idx: 294, P:-0.0331 GT:0.0
idx: 368, P:1.2123 GT:3.0
idx: 347, P:2.5280 GT:0.0
idx: 80, P:-0.2843 GT:3.0
idx: 255, P:0.0870 GT:0.0
idx: 27, P:0.0163 GT:0.0
idx: 328, P:0.6328 GT:3.0
idx: 344, P:2.4282 GT:3.0
idx: 188, P:1.1295 GT:3.0
idx: 29, P:0.5058 GT:0.0
idx: 391, P:-0.0908 GT:3.0
idx: 173, P:2.5062 GT:3.0
idx: 224, P:0.1718 GT:0.0
idx: 147, P:-0.3295 GT:0.0
idx: 333, P:2.4671 GT:3.0
idx: 269, P:3.0358 GT:3.0
idx: 303, P:0.6029 GT:3.0
idx: 449, P:0.1812 GT:0.0
idx: 37, P:-0.5097 GT:3.0
idx: 379, P:1.1267 GT:3.0
idx: 74, P:0.6861 GT:0.0
idx: 266, P:0.1804 GT:3.0
idx: 238, P:0.6855 GT:0.0
idx: 83, P:0.5471 GT:0.0
idx: 327, P:0.2649 GT:0.0
idx: 468, P:-0.4461 GT:3.0
idx: 489, P:0.4798 GT:0.0
idx: 495, P:0.9829 GT:3.0
idx: 89, P:-0.1771 GT:0.0
idx: 389, P:0.0506 GT:0.0
idx: 148, P:2.2184 GT:0.0
idx: 367, P:0.4971 GT:0.0
idx: 412, P:0.8930 GT:3.0
idx: 179, P:0.8192 GT:3.0
idx: 436, P:-0.0977 GT:0.0
idx: 443, P:0.0218 GT:0.0
idx: 69, P:0.7186 GT:3.0
idx: 375, P:0.5658 GT:3.0
idx: 466, P:2.8542 GT:3.0
idx: 385, P:2.1996 GT:0.0
idx: 354, P:-0.1697 GT:0.0
idx: 66, P:0.2060 GT:0.0
idx: 376, P:0.8816 GT:3.0
idx: 476, P:-0.0054 GT:0.0
idx: 127, P:2.3287 GT:3.0
idx: 414, P:-0.2457 GT:0.0
idx: 444, P:2.9129 GT:0.0
idx: 76, P:2.2617 GT:3.0
idx: 84, P:0.2274 GT:0.0
idx: 304, P:-0.1654 GT:0.0
idx: 110, P:-0.0480 GT:0.0
idx: 22, P:0.1895 GT:0.0
idx: 296, P:0.7104 GT:0.0
idx: 409, P:-0.0090 GT:0.0
idx: 334, P:-0.2766 GT:0.0
idx: 156, P:2.7926 GT:3.0
idx: 108, P:2.8680 GT:0.0
idx: 47, P:1.4844 GT:3.0
idx: 369, P:2.6642 GT:0.0
idx: 243, P:2.7131 GT:3.0
idx: 87, P:1.8382 GT:0.0
idx: 270, P:2.4960 GT:0.0
idx: 293, P:1.8180 GT:0.0
idx: 23, P:0.1879 GT:0.0
idx: 480, P:0.6387 GT:3.0
idx: 16, P:-0.3977 GT:0.0
idx: 39, P:0.1606 GT:3.0
idx: 349, P:0.4918 GT:0.0
idx: 392, P:0.4803 GT:3.0
idx: 63, P:-0.2131 GT:0.0
idx: 33, P:2.6487 GT:0.0
idx: 437, P:2.1828 GT:3.0
idx: 330, P:0.7949 GT:3.0
idx: 283, P:0.7089 GT:0.0
idx: 323, P:0.1285 GT:0.0
idx: 133, P:0.9368 GT:0.0
idx: 10, P:-0.2450 GT:0.0
idx: 203, P:0.1205 GT:0.0
idx: 111, P:0.1899 GT:3.0
idx: 483, P:2.2944 GT:0.0
idx: 326, P:0.0592 GT:0.0
idx: 336, P:-0.2144 GT:0.0
idx: 309, P:0.0627 GT:0.0
idx: 51, P:0.1686 GT:0.0
idx: 118, P:-0.0152 GT:0.0
idx: 223, P:1.1743 GT:3.0
idx: 399, P:2.6962 GT:3.0
idx: 193, P:0.7113 GT:0.0
idx: 322, P:0.0679 GT:0.0
idx: 338, P:1.0573 GT:0.0
idx: 15, P:3.0307 GT:0.0
idx: 138, P:0.2445 GT:0.0
idx: 278, P:2.4814 GT:0.0
idx: 307, P:1.5837 GT:3.0
idx: 287, P:0.1872 GT:0.0
idx: 61, P:0.9360 GT:3.0
idx: 382, P:0.6454 GT:0.0
idx: 154, P:-0.1609 GT:0.0
idx: 132, P:-0.4257 GT:0.0
idx: 276, P:2.3578 GT:3.0
idx: 477, P:0.0229 GT:0.0
idx: 353, P:-0.1117 GT:0.0
idx: 218, P:2.4479 GT:0.0
idx: 166, P:-0.3502 GT:0.0
idx: 55, P:-0.0808 GT:0.0
idx: 57, P:0.1234 GT:3.0
idx: 0, P:0.6030 GT:0.0
idx: 129, P:-0.2954 GT:0.0
idx: 100, P:-0.0804 GT:0.0
idx: 163, P:2.9828 GT:3.0
idx: 121, P:1.7035 GT:3.0
idx: 71, P:0.2381 GT:0.0
idx: 423, P:2.4862 GT:3.0
idx: 378, P:1.8720 GT:3.0
idx: 185, P:0.3696 GT:0.0
idx: 198, P:-0.3348 GT:0.0
idx: 310, P:2.2652 GT:3.0
idx: 62, P:1.2055 GT:0.0
idx: 78, P:0.5987 GT:3.0
idx: 455, P:1.8003 GT:3.0
idx: 177, P:1.9698 GT:0.0
idx: 104, P:2.7329 GT:0.0
idx: 96, P:-0.3154 GT:0.0
idx: 34, P:2.0045 GT:0.0
idx: 311, P:2.1713 GT:3.0
idx: 60, P:2.1840 GT:3.0
idx: 462, P:2.6209 GT:0.0
idx: 306, P:0.3874 GT:0.0
idx: 153, P:2.1842 GT:0.0
idx: 265, P:-0.0703 GT:0.0
idx: 117, P:0.2042 GT:0.0
idx: 191, P:0.0835 GT:0.0
idx: 406, P:-0.3406 GT:0.0
idx: 64, P:1.5206 GT:3.0
idx: 91, P:1.0971 GT:3.0
idx: 435, P:1.8404 GT:3.0
idx: 35, P:2.7875 GT:0.0
idx: 456, P:0.8121 GT:3.0
idx: 488, P:-0.5390 GT:0.0
idx: 167, P:1.6907 GT:3.0
idx: 2, P:-0.3189 GT:0.0
idx: 335, P:0.4246 GT:3.0
idx: 242, P:0.7911 GT:3.0
idx: 374, P:2.4083 GT:3.0
idx: 288, P:1.6920 GT:0.0
idx: 115, P:1.0708 GT:0.0
idx: 77, P:3.2962 GT:0.0
idx: 371, P:1.0600 GT:3.0
idx: 431, P:0.1758 GT:0.0
idx: 82, P:1.3595 GT:3.0
idx: 442, P:2.0903 GT:0.0
idx: 448, P:0.4109 GT:3.0
idx: 314, P:1.5634 GT:0.0
idx: 212, P:1.8175 GT:0.0
idx: 259, P:1.0519 GT:0.0
idx: 420, P:1.8651 GT:3.0
idx: 453, P:0.2482 GT:3.0
idx: 481, P:0.5727 GT:0.0
idx: 289, P:2.9567 GT:3.0
idx: 358, P:-0.3304 GT:0.0
idx: 337, P:2.4001 GT:3.0
idx: 52, P:0.1463 GT:0.0
idx: 300, P:0.3616 GT:0.0
idx: 441, P:-0.3546 GT:0.0
idx: 434, P:0.2468 GT:0.0
idx: 79, P:-0.1481 GT:0.0
idx: 67, P:0.1597 GT:3.0
idx: 30, P:-0.2133 GT:0.0
idx: 141, P:0.0283 GT:0.0
idx: 402, P:0.8018 GT:3.0
idx: 346, P:0.9895 GT:3.0
idx: 348, P:0.0119 GT:0.0
idx: 101, P:-0.0573 GT:0.0
idx: 403, P:0.0608 GT:0.0
idx: 343, P:2.9454 GT:3.0
idx: 342, P:0.3681 GT:0.0
idx: 390, P:0.3304 GT:0.0
idx: 345, P:0.8723 GT:3.0
idx: 490, P:-0.5454 GT:0.0
idx: 200, P:0.1822 GT:0.0
idx: 106, P:2.1880 GT:0.0
idx: 474, P:-0.4373 GT:0.0
idx: 473, P:0.1367 GT:3.0
idx: 216, P:0.9308 GT:0.0
idx: 426, P:0.3036 GT:0.0
idx: 162, P:1.1131 GT:0.0
idx: 178, P:0.0996 GT:0.0
idx: 461, P:2.8366 GT:0.0
idx: 128, P:2.6442 GT:0.0
idx: 356, P:-0.0261 GT:0.0
idx: 419, P:0.5722 GT:0.0
idx: 92, P:2.7418 GT:3.0
idx: 315, P:0.2339 GT:0.0
idx: 202, P:0.3780 GT:3.0
idx: 447, P:-0.4326 GT:0.0
idx: 125, P:2.7028 GT:0.0
idx: 171, P:0.1872 GT:0.0
idx: 98, P:0.5664 GT:3.0
idx: 25, P:-0.3965 GT:0.0
idx: 430, P:-0.1136 GT:0.0
idx: 397, P:2.8288 GT:0.0
idx: 73, P:-0.1147 GT:0.0
idx: 81, P:-0.1377 GT:0.0
idx: 222, P:-0.6779 GT:0.0
idx: 251, P:0.2117 GT:3.0
idx: 267, P:0.2553 GT:0.0
idx: 183, P:0.2230 GT:0.0
idx: 355, P:2.7436 GT:0.0
idx: 361, P:0.0482 GT:0.0
idx: 150, P:0.2828 GT:0.0
idx: 50, P:3.0920 GT:0.0
idx: 143, P:2.6900 GT:0.0
idx: 207, P:2.6621 GT:0.0
idx: 464, P:0.0432 GT:0.0
idx: 463, P:2.2770 GT:3.0
idx: 302, P:0.5831 GT:0.0
idx: 151, P:2.5683 GT:3.0
idx: 42, P:0.3767 GT:0.0
idx: 383, P:-0.0067 GT:0.0
idx: 324, P:2.7310 GT:0.0
idx: 180, P:0.3535 GT:0.0
idx: 229, P:-0.1546 GT:0.0
idx: 123, P:0.4996 GT:0.0
idx: 43, P:0.3965 GT:0.0
idx: 116, P:-0.0248 GT:0.0
idx: 281, P:0.0047 GT:0.0
idx: 174, P:2.5912 GT:0.0
idx: 451, P:0.6992 GT:3.0
idx: 24, P:-0.1090 GT:0.0
idx: 439, P:1.3593 GT:3.0
idx: 56, P:0.5972 GT:3.0
idx: 246, P:-0.0509 GT:0.0
idx: 479, P:-0.0478 GT:0.0
idx: 149, P:1.6162 GT:3.0
idx: 331, P:-0.0029 GT:0.0
idx: 168, P:1.6642 GT:3.0
idx: 206, P:0.1907 GT:0.0
idx: 257, P:1.4938 GT:3.0
idx: 4, P:-0.1284 GT:0.0
idx: 48, P:0.0676 GT:0.0
idx: 298, P:0.0994 GT:0.0
idx: 142, P:0.3237 GT:0.0
idx: 445, P:0.9727 GT:3.0
idx: 465, P:1.8528 GT:0.0
idx: 485, P:0.5179 GT:3.0
idx: 38, P:1.9096 GT:3.0
idx: 308, P:1.0030 GT:3.0
idx: 8, P:0.1116 GT:0.0
idx: 186, P:1.1307 GT:0.0
idx: 404, P:0.0350 GT:0.0
idx: 440, P:0.5271 GT:0.0
idx: 467, P:0.5774 GT:0.0
idx: 275, P:0.0229 GT:0.0
idx: 458, P:2.4470 GT:0.0
idx: 144, P:0.6012 GT:0.0
idx: 471, P:-0.3022 GT:0.0
idx: 478, P:-0.0806 GT:3.0
idx: 279, P:0.1140 GT:0.0
idx: 49, P:2.5816 GT:0.0
idx: 230, P:2.7147 GT:3.0
idx: 491, P:2.2081 GT:0.0
idx: 233, P:0.0137 GT:0.0
idx: 290, P:2.1834 GT:3.0
idx: 381, P:2.6785 GT:3.0
idx: 235, P:-0.0018 GT:0.0
idx: 301, P:2.5300 GT:3.0
idx: 291, P:0.1098 GT:0.0
idx: 205, P:3.0865 GT:0.0
idx: 388, P:0.3248 GT:0.0
idx: 54, P:0.7454 GT:3.0
idx: 273, P:0.3130 GT:0.0
idx: 384, P:0.7421 GT:0.0
idx: 292, P:2.1922 GT:3.0
idx: 469, P:0.0923 GT:0.0
idx: 170, P:0.6005 GT:3.0
idx: 157, P:0.5296 GT:0.0
idx: 232, P:-0.1594 GT:0.0
idx: 372, P:0.1161 GT:0.0
idx: 135, P:0.0864 GT:0.0
idx: 95, P:1.4437 GT:3.0
idx: 329, P:2.1806 GT:0.0
idx: 88, P:0.0147 GT:3.0
idx: 407, P:0.7411 GT:0.0
idx: 41, P:0.2783 GT:0.0
idx: 14, P:2.3690 GT:0.0
idx: 189, P:2.5601 GT:3.0
idx: 130, P:2.2528 GT:0.0
idx: 40, P:0.5152 GT:3.0
idx: 351, P:-0.4020 GT:0.0
idx: 277, P:0.0286 GT:0.0
idx: 11, P:2.7341 GT:0.0
idx: 9, P:0.2054 GT:0.0
idx: 28, P:2.6935 GT:0.0
idx: 370, P:0.1192 GT:0.0
idx: 204, P:0.8983 GT:0.0
idx: 145, P:2.8623 GT:0.0
idx: 219, P:0.1319 GT:0.0
idx: 285, P:0.0959 GT:0.0
idx: 429, P:0.4944 GT:3.0
idx: 137, P:0.1643 GT:0.0
idx: 248, P:2.6232 GT:3.0
idx: 365, P:-0.0494 GT:0.0
idx: 282, P:2.6643 GT:0.0
idx: 109, P:0.0029 GT:0.0
idx: 299, P:2.2667 GT:3.0
idx: 264, P:-0.4775 GT:0.0
idx: 103, P:0.7658 GT:0.0
idx: 210, P:1.1439 GT:3.0
idx: 410, P:0.4668 GT:3.0
idx: 26, P:0.4308 GT:0.0
idx: 136, P:-0.1332 GT:0.0
idx: 192, P:-0.0681 GT:3.0
idx: 72, P:0.0005 GT:0.0
idx: 85, P:2.8594 GT:0.0
idx: 247, P:0.1295 GT:0.0
idx: 3, P:0.9355 GT:3.0
idx: 65, P:-0.4945 GT:0.0
idx: 113, P:1.0687 GT:0.0
idx: 12, P:2.0143 GT:0.0
idx: 126, P:2.3910 GT:0.0
idx: 32, P:0.0430 GT:0.0
idx: 459, P:0.0705 GT:0.0
idx: 272, P:0.4082 GT:0.0
idx: 105, P:-0.0967 GT:0.0
idx: 312, P:0.2174 GT:3.0
idx: 53, P:0.1432 GT:0.0
idx: 159, P:1.2487 GT:3.0
idx: 250, P:-0.1660 GT:0.0
idx: 363, P:1.8080 GT:3.0
idx: 484, P:1.8356 GT:0.0
idx: 236, P:1.4709 GT:3.0
idx: 377, P:0.0854 GT:0.0
idx: 317, P:0.0867 GT:0.0
idx: 59, P:-0.2225 GT:0.0
idx: 19, P:-0.6276 GT:0.0
idx: 146, P:2.3062 GT:3.0
idx: 286, P:1.4644 GT:0.0
idx: 472, P:-0.2692 GT:0.0
idx: 249, P:-0.1910 GT:0.0
idx: 271, P:-0.3047 GT:0.0
idx: 373, P:0.1608 GT:0.0
idx: 494, P:0.3748 GT:0.0
idx: 184, P:0.6223 GT:3.0
idx: 364, P:0.1793 GT:0.0
idx: 411, P:-0.0380 GT:0.0
idx: 415, P:1.5948 GT:3.0
idx: 387, P:2.0347 GT:0.0
idx: 208, P:0.0100 GT:0.0
idx: 427, P:-0.1900 GT:3.0
idx: 380, P:0.8976 GT:0.0
idx: 446, P:1.9580 GT:0.0
idx: 341, P:0.0571 GT:0.0
idx: 254, P:0.0711 GT:3.0
idx: 214, P:0.5041 GT:3.0
idx: 107, P:-0.0734 GT:0.0
idx: 86, P:2.6205 GT:3.0
idx: 199, P:-0.0716 GT:0.0
idx: 241, P:0.0821 GT:0.0
idx: 244, P:0.1814 GT:0.0
idx: 70, P:0.0768 GT:0.0
idx: 405, P:2.6650 GT:0.0
idx: 359, P:0.1369 GT:0.0
idx: 187, P:-0.0020 GT:0.0
idx: 17, P:0.5578 GT:3.0
idx: 228, P:0.1259 GT:0.0
idx: 225, P:-0.0214 GT:0.0
idx: 119, P:2.1627 GT:3.0
idx: 450, P:-0.1667 GT:0.0
idx: 226, P:1.0732 GT:3.0
idx: 493, P:1.8708 GT:3.0
idx: 114, P:2.3268 GT:3.0
idx: 475, P:-0.3884 GT:0.0
idx: 201, P:2.1448 GT:3.0
idx: 68, P:0.0047 GT:0.0
idx: 122, P:0.2000 GT:0.0
idx: 213, P:0.9418 GT:0.0
idx: 318, P:3.2648 GT:3.0
idx: 280, P:2.6621 GT:0.0
idx: 134, P:-0.3857 GT:0.0
idx: 386, P:2.1723 GT:0.0
idx: 181, P:0.2185 GT:0.0
idx: 215, P:0.1982 GT:3.0
idx: 90, P:0.6002 GT:0.0
idx: 313, P:-0.1164 GT:0.0
idx: 194, P:-0.1098 GT:0.0
idx: 413, P:2.6362 GT:3.0
idx: 45, P:0.0013 GT:3.0
idx: 6, P:1.3818 GT:3.0
idx: 417, P:-0.0736 GT:0.0
idx: 394, P:3.0219 GT:3.0
idx: 161, P:-0.3033 GT:0.0
idx: 457, P:1.4717 GT:3.0
idx: 305, P:0.0709 GT:0.0
idx: 297, P:1.5705 GT:3.0
idx: 492, P:1.1838 GT:0.0
idx: 418, P:-0.8186 GT:0.0
idx: 421, P:2.4806 GT:3.0
idx: 393, P:-0.2992 GT:0.0
idx: 231, P:-0.2001 GT:0.0
idx: 195, P:0.1082 GT:0.0
idx: 239, P:2.9313 GT:3.0
idx: 139, P:2.6538 GT:3.0
idx: 395, P:2.2574 GT:3.0
idx: 21, P:1.7065 GT:0.0
idx: 321, P:2.5477 GT:0.0
idx: 252, P:1.0076 GT:3.0
idx: 424, P:-0.0274 GT:0.0
idx: 433, P:1.2163 GT:3.0
idx: 274, P:1.6575 GT:0.0
idx: 260, P:1.4530 GT:3.0
idx: 13, P:0.2971 GT:3.0
idx: 261, P:1.9857 GT:3.0
idx: 362, P:2.6987 GT:0.0
idx: 220, P:2.6022 GT:0.0
idx: 175, P:0.0519 GT:0.0
idx: 20, P:0.3453 GT:0.0
idx: 190, P:-0.4049 GT:0.0
idx: 36, P:0.8663 GT:0.0
idx: 319, P:3.0412 GT:3.0
idx: 221, P:0.9212 GT:3.0
idx: 158, P:0.2637 GT:0.0
idx: 268, P:0.4712 GT:0.0
idx: 497, P:2.8050 GT:3.0
idx: 432, P:-0.6076 GT:0.0
idx: 31, P:-0.0217 GT:0.0
idx: 339, P:2.1051 GT:3.0
idx: 211, P:0.2416 GT:0.0
idx: 140, P:2.1550 GT:0.0
idx: 460, P:0.1160 GT:0.0
idx: 120, P:2.6213 GT:3.0
idx: 46, P:0.5732 GT:3.0
idx: 486, P:0.2330 GT:3.0
idx: 401, P:0.2286 GT:0.0
idx: 396, P:-0.3616 GT:0.0
idx: 182, P:-0.1257 GT:0.0
idx: 164, P:-0.0984 GT:0.0
idx: 245, P:-0.2288 GT:0.0
idx: 94, P:0.1400 GT:0.0
idx: 398, P:-0.5451 GT:0.0
idx: 482, P:2.5479 GT:3.0
idx: 124, P:-0.1911 GT:0.0
idx: 152, P:1.2967 GT:3.0
idx: 176, P:-0.0078 GT:0.0
idx: 165, P:0.2107 GT:0.0
idx: 400, P:-0.5594 GT:0.0
idx: 1, P:1.4973 GT:3.0
idx: 253, P:0.1676 GT:3.0
idx: 160, P:0.3529 GT:0.0
idx: 75, P:1.1761 GT:0.0
idx: 422, P:2.7561 GT:3.0
idx: 340, P:0.9211 GT:3.0
idx: 169, P:0.9728 GT:3.0
idx: 320, P:2.2068 GT:3.0
idx: 99, P:-0.5680 GT:0.0
idx: 155, P:-0.3463 GT:0.0
idx: 425, P:0.3225 GT:0.0
idx: 227, P:-0.2638 GT:0.0
idx: 366, P:0.7074 GT:3.0
idx: 332, P:0.5683 GT:3.0
idx: 438, P:-0.1560 GT:0.0
idx: 452, P:0.5372 GT:3.0
idx: 102, P:3.1311 GT:0.0
idx: 316, P:0.8143 GT:3.0
idx: 97, P:0.1283 GT:0.0
idx: 470, P:-0.3075 GT:0.0
idx: 7, P:0.9677 GT:3.0
idx: 237, P:0.2960 GT:3.0
idx: 196, P:-0.0739 GT:0.0
idx: 352, P:0.3898 GT:3.0
idx: 357, P:-0.2558 GT:0.0
idx: 44, P:0.1508 GT:0.0
idx: 131, P:3.1284 GT:0.0
idx: 234, P:0.1846 GT:0.0
idx: 93, P:0.2459 GT:3.0
idx: 58, P:0.3562 GT:0.0
idx: 487, P:0.7272 GT:3.0
idx: 256, P:2.2275 GT:3.0
idx: 172, P:-0.0103 GT:0.0
idx: 263, P:1.9658 GT:0.0
idx: 240, P:1.7189 GT:3.0
idx: 454, P:-0.0950 GT:0.0
idx: 217, P:-0.2133 GT:0.0
idx: 284, P:-0.0249 GT:0.0
idx: 209, P:-0.2546 GT:0.0
idx: 5, P:2.2527 GT:0.0
idx: 350, P:-0.0405 GT:0.0
idx: 258, P:-0.1357 GT:0.0
idx: 18, P:2.1403 GT:3.0
idx: 496, P:1.6609 GT:3.0
idx: 416, P:-0.3114 GT:0.0
idx: 360, P:-0.1453 GT:0.0
idx: 428, P:0.2433 GT:0.0
idx: 112, P:0.2897 GT:0.0
idx: 262, P:1.7195 GT:3.0
idx: 197, P:1.9198 GT:0.0
idx: 408, P:2.9996 GT:0.0
idx: 295, P:1.2885 GT:0.0
idx: 325, P:0.2391 GT:0.0
idx: 498, P:2.1077 GT:0.0
Lowest Validation Loss: 1.619949
Time Elapsed: 0h 32m 54s
Epochs: [0, 1, 2]
Val_Losses: [1.619948673248291, 2.035033440589905, 2.2406235774358114]
Train_Losses: [1.6246998336580065, 1.0110939780871073, 0.609430474953519]
