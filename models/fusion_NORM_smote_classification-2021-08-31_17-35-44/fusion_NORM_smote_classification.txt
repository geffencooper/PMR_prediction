============================ Raw Args ============================
Namespace(batch_size=32, classification='y', dropout='n', droput_prob=0.0, gpu_i=1, hidden_init_rand='n', hidden_size=64, imbalanced_sampler='n', input_size=23, l2_reg='n', load_trained='n', log_dest='../models/fusion_NORM_smote_classification-2021-08-31_17-35-44', loss_freq=10, lr=0.002, model_name='PMRfusionNN', normalize='y', num_classes=4, num_epochs=5, num_layers=1, optim='RMS', regression='n', root_dir='/data/perception-working/Geffen/avec_data/', session_name='fusion_NORM_smote_classification', train_data_dir='SMOTE/', train_labels_csv='labels2.csv', trained_path='none', val_data_dir='none', val_freq=0, val_labels_csv='val_metadata.csv', weight_decay_amnt=0.0, weighted_loss='n')



================================ Start Training ================================

Session Name: fusion_NORM_smote_classification

Model Name: PMRfusionNN

Device: 1  ---->  GeForce GTX 1080 Ti

Hyperparameters:
Batch Size: 32
Learning Rate: 0.002
Hidden Size: 64
Number of Layer: 1
Number of Epochs: 5
Normalization:y

Train Epoch: 0 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 1.260166
Train Epoch: 0 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 1.298059
Train Epoch: 0 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 1.299452
Train Epoch: 0 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 1.206610
Train Epoch: 0 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 1.106361
Train Epoch: 0 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 1.275758
Train Epoch: 0 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 1.125513
Train Epoch: 0 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 1.043006
Train Epoch: 0 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 1.187660
Train Epoch: 0 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 1.027575
Train Epoch: 0 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.953885
Train Epoch: 0 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.942831
Train Epoch: 0 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.909303
Train Epoch: 0 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.913521
Train Epoch: 0 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.906370
Train Epoch: 0 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.934655
Train Epoch: 0 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 0.847020
Train Epoch: 0 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 0.731154
Train Epoch: 0 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 0.931530
Train Epoch: 0 Iteration: 200 [6400/34075 (19%)]	 Batch 200 Loss: 0.764846
Train Epoch: 0 Iteration: 210 [6720/34075 (20%)]	 Batch 210 Loss: 0.745710
Train Epoch: 0 Iteration: 220 [7040/34075 (21%)]	 Batch 220 Loss: 0.817775
Train Epoch: 0 Iteration: 230 [7360/34075 (22%)]	 Batch 230 Loss: 0.837620
Train Epoch: 0 Iteration: 240 [7680/34075 (23%)]	 Batch 240 Loss: 0.800658
Train Epoch: 0 Iteration: 250 [8000/34075 (23%)]	 Batch 250 Loss: 0.632428
Train Epoch: 0 Iteration: 260 [8320/34075 (24%)]	 Batch 260 Loss: 0.754167
Train Epoch: 0 Iteration: 270 [8640/34075 (25%)]	 Batch 270 Loss: 0.580927
Train Epoch: 0 Iteration: 280 [8960/34075 (26%)]	 Batch 280 Loss: 0.680037
Train Epoch: 0 Iteration: 290 [9280/34075 (27%)]	 Batch 290 Loss: 0.800791
Train Epoch: 0 Iteration: 300 [9600/34075 (28%)]	 Batch 300 Loss: 0.620655
Train Epoch: 0 Iteration: 310 [9920/34075 (29%)]	 Batch 310 Loss: 0.805478
Train Epoch: 0 Iteration: 320 [10240/34075 (30%)]	 Batch 320 Loss: 0.863517
Train Epoch: 0 Iteration: 330 [10560/34075 (31%)]	 Batch 330 Loss: 0.971056
Train Epoch: 0 Iteration: 340 [10880/34075 (32%)]	 Batch 340 Loss: 0.675671
Train Epoch: 0 Iteration: 350 [11200/34075 (33%)]	 Batch 350 Loss: 0.512470
Train Epoch: 0 Iteration: 360 [11520/34075 (34%)]	 Batch 360 Loss: 0.486456
Train Epoch: 0 Iteration: 370 [11840/34075 (35%)]	 Batch 370 Loss: 0.372512
Train Epoch: 0 Iteration: 380 [12160/34075 (36%)]	 Batch 380 Loss: 0.392760
Train Epoch: 0 Iteration: 390 [12480/34075 (37%)]	 Batch 390 Loss: 0.559859
Train Epoch: 0 Iteration: 400 [12800/34075 (38%)]	 Batch 400 Loss: 0.528039
Train Epoch: 0 Iteration: 410 [13120/34075 (38%)]	 Batch 410 Loss: 0.646783
Train Epoch: 0 Iteration: 420 [13440/34075 (39%)]	 Batch 420 Loss: 0.629510
Train Epoch: 0 Iteration: 430 [13760/34075 (40%)]	 Batch 430 Loss: 0.496414
Train Epoch: 0 Iteration: 440 [14080/34075 (41%)]	 Batch 440 Loss: 0.512923
Train Epoch: 0 Iteration: 450 [14400/34075 (42%)]	 Batch 450 Loss: 0.464714
Train Epoch: 0 Iteration: 460 [14720/34075 (43%)]	 Batch 460 Loss: 0.733233
Train Epoch: 0 Iteration: 470 [15040/34075 (44%)]	 Batch 470 Loss: 0.472258
Train Epoch: 0 Iteration: 480 [15360/34075 (45%)]	 Batch 480 Loss: 0.493056
Train Epoch: 0 Iteration: 490 [15680/34075 (46%)]	 Batch 490 Loss: 0.537428
Train Epoch: 0 Iteration: 500 [16000/34075 (47%)]	 Batch 500 Loss: 0.398079
Train Epoch: 0 Iteration: 510 [16320/34075 (48%)]	 Batch 510 Loss: 0.454965
Train Epoch: 0 Iteration: 520 [16640/34075 (49%)]	 Batch 520 Loss: 0.399636
Train Epoch: 0 Iteration: 530 [16960/34075 (50%)]	 Batch 530 Loss: 0.409617
Train Epoch: 0 Iteration: 540 [17280/34075 (51%)]	 Batch 540 Loss: 0.389154
Train Epoch: 0 Iteration: 550 [17600/34075 (52%)]	 Batch 550 Loss: 0.328569
Train Epoch: 0 Iteration: 560 [17920/34075 (53%)]	 Batch 560 Loss: 0.471430
Train Epoch: 0 Iteration: 570 [18240/34075 (54%)]	 Batch 570 Loss: 0.412257
Train Epoch: 0 Iteration: 580 [18560/34075 (54%)]	 Batch 580 Loss: 0.509446
Train Epoch: 0 Iteration: 590 [18880/34075 (55%)]	 Batch 590 Loss: 0.599854
Train Epoch: 0 Iteration: 600 [19200/34075 (56%)]	 Batch 600 Loss: 0.482164
Train Epoch: 0 Iteration: 610 [19520/34075 (57%)]	 Batch 610 Loss: 0.286320
Train Epoch: 0 Iteration: 620 [19840/34075 (58%)]	 Batch 620 Loss: 0.470219
Train Epoch: 0 Iteration: 630 [20160/34075 (59%)]	 Batch 630 Loss: 0.477920
Train Epoch: 0 Iteration: 640 [20480/34075 (60%)]	 Batch 640 Loss: 0.313586
Train Epoch: 0 Iteration: 650 [20800/34075 (61%)]	 Batch 650 Loss: 0.406212
Train Epoch: 0 Iteration: 660 [21120/34075 (62%)]	 Batch 660 Loss: 0.593210
Train Epoch: 0 Iteration: 670 [21440/34075 (63%)]	 Batch 670 Loss: 0.425558
Train Epoch: 0 Iteration: 680 [21760/34075 (64%)]	 Batch 680 Loss: 0.321723
Train Epoch: 0 Iteration: 690 [22080/34075 (65%)]	 Batch 690 Loss: 0.452161
Train Epoch: 0 Iteration: 700 [22400/34075 (66%)]	 Batch 700 Loss: 0.309016
Train Epoch: 0 Iteration: 710 [22720/34075 (67%)]	 Batch 710 Loss: 0.468113
Train Epoch: 0 Iteration: 720 [23040/34075 (68%)]	 Batch 720 Loss: 0.386939
Train Epoch: 0 Iteration: 730 [23360/34075 (69%)]	 Batch 730 Loss: 0.291048
Train Epoch: 0 Iteration: 740 [23680/34075 (69%)]	 Batch 740 Loss: 0.417242
Train Epoch: 0 Iteration: 750 [24000/34075 (70%)]	 Batch 750 Loss: 0.213177
Train Epoch: 0 Iteration: 760 [24320/34075 (71%)]	 Batch 760 Loss: 0.531656
Train Epoch: 0 Iteration: 770 [24640/34075 (72%)]	 Batch 770 Loss: 0.433321
Train Epoch: 0 Iteration: 780 [24960/34075 (73%)]	 Batch 780 Loss: 0.346097
Train Epoch: 0 Iteration: 790 [25280/34075 (74%)]	 Batch 790 Loss: 0.302898
Train Epoch: 0 Iteration: 800 [25600/34075 (75%)]	 Batch 800 Loss: 0.491710
Train Epoch: 0 Iteration: 810 [25920/34075 (76%)]	 Batch 810 Loss: 0.244122
Train Epoch: 0 Iteration: 820 [26240/34075 (77%)]	 Batch 820 Loss: 0.430092
Train Epoch: 0 Iteration: 830 [26560/34075 (78%)]	 Batch 830 Loss: 0.285754
Train Epoch: 0 Iteration: 840 [26880/34075 (79%)]	 Batch 840 Loss: 0.339527
Train Epoch: 0 Iteration: 850 [27200/34075 (80%)]	 Batch 850 Loss: 0.376245
Train Epoch: 0 Iteration: 860 [27520/34075 (81%)]	 Batch 860 Loss: 0.427115
Train Epoch: 0 Iteration: 870 [27840/34075 (82%)]	 Batch 870 Loss: 0.292297
Train Epoch: 0 Iteration: 880 [28160/34075 (83%)]	 Batch 880 Loss: 0.268587
Train Epoch: 0 Iteration: 890 [28480/34075 (84%)]	 Batch 890 Loss: 0.323213
Train Epoch: 0 Iteration: 900 [28800/34075 (85%)]	 Batch 900 Loss: 0.425937
Train Epoch: 0 Iteration: 910 [29120/34075 (85%)]	 Batch 910 Loss: 0.470973
Train Epoch: 0 Iteration: 920 [29440/34075 (86%)]	 Batch 920 Loss: 0.248733
Train Epoch: 0 Iteration: 930 [29760/34075 (87%)]	 Batch 930 Loss: 0.325659
Train Epoch: 0 Iteration: 940 [30080/34075 (88%)]	 Batch 940 Loss: 0.416582
Train Epoch: 0 Iteration: 950 [30400/34075 (89%)]	 Batch 950 Loss: 0.323681
Train Epoch: 0 Iteration: 960 [30720/34075 (90%)]	 Batch 960 Loss: 0.347736
Train Epoch: 0 Iteration: 970 [31040/34075 (91%)]	 Batch 970 Loss: 0.348690
Train Epoch: 0 Iteration: 980 [31360/34075 (92%)]	 Batch 980 Loss: 0.299369
Train Epoch: 0 Iteration: 990 [31680/34075 (93%)]	 Batch 990 Loss: 0.421126
Train Epoch: 0 Iteration: 1000 [32000/34075 (94%)]	 Batch 1000 Loss: 0.422759
Train Epoch: 0 Iteration: 1010 [32320/34075 (95%)]	 Batch 1010 Loss: 0.188753
Train Epoch: 0 Iteration: 1020 [32640/34075 (96%)]	 Batch 1020 Loss: 0.653899
Train Epoch: 0 Iteration: 1030 [32960/34075 (97%)]	 Batch 1030 Loss: 0.269890
Train Epoch: 0 Iteration: 1040 [33280/34075 (98%)]	 Batch 1040 Loss: 0.262999
Train Epoch: 0 Iteration: 1050 [33600/34075 (99%)]	 Batch 1050 Loss: 0.298010
Train Epoch: 0 Iteration: 1060 [33920/34075 (100%)]	 Batch 1060 Loss: 0.435163


----------------- Epoch 0 -----------------

validation computation time: 10.0  minutes
Confusion Matrix
tensor([[1628,  542,  123,  117],
        [1442,  358,  136,   46],
        [  50,   12,    1,    1],
        [ 131,   15,   17,    2]])
class 0 accuracy: 50.0769%
class 1 accuracy: 38.6192%
class 2 accuracy: 0.3610%
class 3 accuracy: 1.2048%

Validation Loss: 1.5957, Accuracy: 1989/4621 (43%)
Training Loss:0.5784
Best Accuracy: 43.042631%
Time Elapsed: 0h 30m 46s

--------------------------------------------------------


Train Epoch: 1 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 0.276849
Train Epoch: 1 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 0.213453
Train Epoch: 1 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 0.153060
Train Epoch: 1 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 0.292419
Train Epoch: 1 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 0.295276
Train Epoch: 1 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 0.396595
Train Epoch: 1 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 0.232540
Train Epoch: 1 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 0.314765
Train Epoch: 1 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 0.191210
Train Epoch: 1 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 0.246661
Train Epoch: 1 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.275868
Train Epoch: 1 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.338987
Train Epoch: 1 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.214913
Train Epoch: 1 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.288178
Train Epoch: 1 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.294879
Train Epoch: 1 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.171371
Train Epoch: 1 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 0.230923
Train Epoch: 1 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 0.417789
Train Epoch: 1 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 0.143909
Train Epoch: 1 Iteration: 200 [6400/34075 (19%)]	 Batch 200 Loss: 0.251513
Train Epoch: 1 Iteration: 210 [6720/34075 (20%)]	 Batch 210 Loss: 0.203871
Train Epoch: 1 Iteration: 220 [7040/34075 (21%)]	 Batch 220 Loss: 0.347778
Train Epoch: 1 Iteration: 230 [7360/34075 (22%)]	 Batch 230 Loss: 0.392855
Train Epoch: 1 Iteration: 240 [7680/34075 (23%)]	 Batch 240 Loss: 0.162532
Train Epoch: 1 Iteration: 250 [8000/34075 (23%)]	 Batch 250 Loss: 0.143885
Train Epoch: 1 Iteration: 260 [8320/34075 (24%)]	 Batch 260 Loss: 0.215568
Train Epoch: 1 Iteration: 270 [8640/34075 (25%)]	 Batch 270 Loss: 0.117664
Train Epoch: 1 Iteration: 280 [8960/34075 (26%)]	 Batch 280 Loss: 0.197290
Train Epoch: 1 Iteration: 290 [9280/34075 (27%)]	 Batch 290 Loss: 0.132375
Train Epoch: 1 Iteration: 300 [9600/34075 (28%)]	 Batch 300 Loss: 0.120641
Train Epoch: 1 Iteration: 310 [9920/34075 (29%)]	 Batch 310 Loss: 0.169157
Train Epoch: 1 Iteration: 320 [10240/34075 (30%)]	 Batch 320 Loss: 0.282608
Train Epoch: 1 Iteration: 330 [10560/34075 (31%)]	 Batch 330 Loss: 0.225981
Train Epoch: 1 Iteration: 340 [10880/34075 (32%)]	 Batch 340 Loss: 0.285715
Train Epoch: 1 Iteration: 350 [11200/34075 (33%)]	 Batch 350 Loss: 0.127528
Train Epoch: 1 Iteration: 360 [11520/34075 (34%)]	 Batch 360 Loss: 0.247051
Train Epoch: 1 Iteration: 370 [11840/34075 (35%)]	 Batch 370 Loss: 0.129483
Train Epoch: 1 Iteration: 380 [12160/34075 (36%)]	 Batch 380 Loss: 0.118089
Train Epoch: 1 Iteration: 390 [12480/34075 (37%)]	 Batch 390 Loss: 0.203799
Train Epoch: 1 Iteration: 400 [12800/34075 (38%)]	 Batch 400 Loss: 0.283797
Train Epoch: 1 Iteration: 410 [13120/34075 (38%)]	 Batch 410 Loss: 0.171960
Train Epoch: 1 Iteration: 420 [13440/34075 (39%)]	 Batch 420 Loss: 0.237082
Train Epoch: 1 Iteration: 430 [13760/34075 (40%)]	 Batch 430 Loss: 0.143031
Train Epoch: 1 Iteration: 440 [14080/34075 (41%)]	 Batch 440 Loss: 0.317977
Train Epoch: 1 Iteration: 450 [14400/34075 (42%)]	 Batch 450 Loss: 0.279927
Train Epoch: 1 Iteration: 460 [14720/34075 (43%)]	 Batch 460 Loss: 0.248654
Train Epoch: 1 Iteration: 470 [15040/34075 (44%)]	 Batch 470 Loss: 0.105077
Train Epoch: 1 Iteration: 480 [15360/34075 (45%)]	 Batch 480 Loss: 0.310726
Train Epoch: 1 Iteration: 490 [15680/34075 (46%)]	 Batch 490 Loss: 0.096652
Train Epoch: 1 Iteration: 500 [16000/34075 (47%)]	 Batch 500 Loss: 0.396557
Train Epoch: 1 Iteration: 510 [16320/34075 (48%)]	 Batch 510 Loss: 0.122230
Train Epoch: 1 Iteration: 520 [16640/34075 (49%)]	 Batch 520 Loss: 0.187158
Train Epoch: 1 Iteration: 530 [16960/34075 (50%)]	 Batch 530 Loss: 0.110199
Train Epoch: 1 Iteration: 540 [17280/34075 (51%)]	 Batch 540 Loss: 0.351746
Train Epoch: 1 Iteration: 550 [17600/34075 (52%)]	 Batch 550 Loss: 0.394884
Train Epoch: 1 Iteration: 560 [17920/34075 (53%)]	 Batch 560 Loss: 0.139983
Train Epoch: 1 Iteration: 570 [18240/34075 (54%)]	 Batch 570 Loss: 0.301082
Train Epoch: 1 Iteration: 580 [18560/34075 (54%)]	 Batch 580 Loss: 0.239548
Train Epoch: 1 Iteration: 590 [18880/34075 (55%)]	 Batch 590 Loss: 0.165956
Train Epoch: 1 Iteration: 600 [19200/34075 (56%)]	 Batch 600 Loss: 0.282200
Train Epoch: 1 Iteration: 610 [19520/34075 (57%)]	 Batch 610 Loss: 0.156366
Train Epoch: 1 Iteration: 620 [19840/34075 (58%)]	 Batch 620 Loss: 0.086527
Train Epoch: 1 Iteration: 630 [20160/34075 (59%)]	 Batch 630 Loss: 0.268166
Train Epoch: 1 Iteration: 640 [20480/34075 (60%)]	 Batch 640 Loss: 0.160511
Train Epoch: 1 Iteration: 650 [20800/34075 (61%)]	 Batch 650 Loss: 0.216667
Train Epoch: 1 Iteration: 660 [21120/34075 (62%)]	 Batch 660 Loss: 0.098500
Train Epoch: 1 Iteration: 670 [21440/34075 (63%)]	 Batch 670 Loss: 0.131379
Train Epoch: 1 Iteration: 680 [21760/34075 (64%)]	 Batch 680 Loss: 0.298935
Train Epoch: 1 Iteration: 690 [22080/34075 (65%)]	 Batch 690 Loss: 0.345695
Train Epoch: 1 Iteration: 700 [22400/34075 (66%)]	 Batch 700 Loss: 0.130898
Train Epoch: 1 Iteration: 710 [22720/34075 (67%)]	 Batch 710 Loss: 0.172840
Train Epoch: 1 Iteration: 720 [23040/34075 (68%)]	 Batch 720 Loss: 0.143222
Train Epoch: 1 Iteration: 730 [23360/34075 (69%)]	 Batch 730 Loss: 0.561192
Train Epoch: 1 Iteration: 740 [23680/34075 (69%)]	 Batch 740 Loss: 0.257807
Train Epoch: 1 Iteration: 750 [24000/34075 (70%)]	 Batch 750 Loss: 0.237858
Train Epoch: 1 Iteration: 760 [24320/34075 (71%)]	 Batch 760 Loss: 0.230384
Train Epoch: 1 Iteration: 770 [24640/34075 (72%)]	 Batch 770 Loss: 0.242858
Train Epoch: 1 Iteration: 780 [24960/34075 (73%)]	 Batch 780 Loss: 0.132188
Train Epoch: 1 Iteration: 790 [25280/34075 (74%)]	 Batch 790 Loss: 0.272505
Train Epoch: 1 Iteration: 800 [25600/34075 (75%)]	 Batch 800 Loss: 0.275569
Train Epoch: 1 Iteration: 810 [25920/34075 (76%)]	 Batch 810 Loss: 0.156915
Train Epoch: 1 Iteration: 820 [26240/34075 (77%)]	 Batch 820 Loss: 0.335175
Train Epoch: 1 Iteration: 830 [26560/34075 (78%)]	 Batch 830 Loss: 0.114761
Train Epoch: 1 Iteration: 840 [26880/34075 (79%)]	 Batch 840 Loss: 0.355162
Train Epoch: 1 Iteration: 850 [27200/34075 (80%)]	 Batch 850 Loss: 0.141757
Train Epoch: 1 Iteration: 860 [27520/34075 (81%)]	 Batch 860 Loss: 0.101269
Train Epoch: 1 Iteration: 870 [27840/34075 (82%)]	 Batch 870 Loss: 0.151596
Train Epoch: 1 Iteration: 880 [28160/34075 (83%)]	 Batch 880 Loss: 0.072454
Train Epoch: 1 Iteration: 890 [28480/34075 (84%)]	 Batch 890 Loss: 0.373532
Train Epoch: 1 Iteration: 900 [28800/34075 (85%)]	 Batch 900 Loss: 0.174286
Train Epoch: 1 Iteration: 910 [29120/34075 (85%)]	 Batch 910 Loss: 0.119631
Train Epoch: 1 Iteration: 920 [29440/34075 (86%)]	 Batch 920 Loss: 0.271585
Train Epoch: 1 Iteration: 930 [29760/34075 (87%)]	 Batch 930 Loss: 0.296376
Train Epoch: 1 Iteration: 940 [30080/34075 (88%)]	 Batch 940 Loss: 0.062013
Train Epoch: 1 Iteration: 950 [30400/34075 (89%)]	 Batch 950 Loss: 0.183242
Train Epoch: 1 Iteration: 960 [30720/34075 (90%)]	 Batch 960 Loss: 0.073630
Train Epoch: 1 Iteration: 970 [31040/34075 (91%)]	 Batch 970 Loss: 0.266424
Train Epoch: 1 Iteration: 980 [31360/34075 (92%)]	 Batch 980 Loss: 0.081102
Train Epoch: 1 Iteration: 990 [31680/34075 (93%)]	 Batch 990 Loss: 0.191821
Train Epoch: 1 Iteration: 1000 [32000/34075 (94%)]	 Batch 1000 Loss: 0.135024
Train Epoch: 1 Iteration: 1010 [32320/34075 (95%)]	 Batch 1010 Loss: 0.145903
Train Epoch: 1 Iteration: 1020 [32640/34075 (96%)]	 Batch 1020 Loss: 0.309440
Train Epoch: 1 Iteration: 1030 [32960/34075 (97%)]	 Batch 1030 Loss: 0.237809
Train Epoch: 1 Iteration: 1040 [33280/34075 (98%)]	 Batch 1040 Loss: 0.133893
Train Epoch: 1 Iteration: 1050 [33600/34075 (99%)]	 Batch 1050 Loss: 0.180312
Train Epoch: 1 Iteration: 1060 [33920/34075 (100%)]	 Batch 1060 Loss: 0.175546


----------------- Epoch 1 -----------------

validation computation time: 8.0  minutes
Confusion Matrix
tensor([[2567,  756,  223,  148],
        [ 482,  112,   36,    6],
        [ 131,   39,    4,    9],
        [  71,   20,   14,    3]])
class 0 accuracy: 78.9603%
class 1 accuracy: 12.0820%
class 2 accuracy: 1.4440%
class 3 accuracy: 1.8072%

Validation Loss: 1.7184, Accuracy: 2686/4621 (58%)
Training Loss:0.2213
Best Accuracy: 58.125947%
Time Elapsed: 1h 3m 53s

--------------------------------------------------------


Train Epoch: 2 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 0.101109
Train Epoch: 2 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 0.044923
Train Epoch: 2 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 0.075208
Train Epoch: 2 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 0.046473
Train Epoch: 2 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 0.054277
Train Epoch: 2 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 0.144233
Train Epoch: 2 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 0.292074
Train Epoch: 2 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 0.123504
Train Epoch: 2 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 0.270715
Train Epoch: 2 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 0.208872
Train Epoch: 2 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.142874
Train Epoch: 2 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.194692
Train Epoch: 2 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.094865
Train Epoch: 2 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.069862
Train Epoch: 2 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.189598
Train Epoch: 2 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.107554
Train Epoch: 2 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 0.113728
Train Epoch: 2 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 0.320331
Train Epoch: 2 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 0.139214
Train Epoch: 2 Iteration: 200 [6400/34075 (19%)]	 Batch 200 Loss: 0.082177
Train Epoch: 2 Iteration: 210 [6720/34075 (20%)]	 Batch 210 Loss: 0.123806
Train Epoch: 2 Iteration: 220 [7040/34075 (21%)]	 Batch 220 Loss: 0.178499
Train Epoch: 2 Iteration: 230 [7360/34075 (22%)]	 Batch 230 Loss: 0.123239
Train Epoch: 2 Iteration: 240 [7680/34075 (23%)]	 Batch 240 Loss: 0.036519
Train Epoch: 2 Iteration: 250 [8000/34075 (23%)]	 Batch 250 Loss: 0.049000
Train Epoch: 2 Iteration: 260 [8320/34075 (24%)]	 Batch 260 Loss: 0.073999
Train Epoch: 2 Iteration: 270 [8640/34075 (25%)]	 Batch 270 Loss: 0.076979
Train Epoch: 2 Iteration: 280 [8960/34075 (26%)]	 Batch 280 Loss: 0.100682
Train Epoch: 2 Iteration: 290 [9280/34075 (27%)]	 Batch 290 Loss: 0.099723
Train Epoch: 2 Iteration: 300 [9600/34075 (28%)]	 Batch 300 Loss: 0.115377
Train Epoch: 2 Iteration: 310 [9920/34075 (29%)]	 Batch 310 Loss: 0.137495
Train Epoch: 2 Iteration: 320 [10240/34075 (30%)]	 Batch 320 Loss: 0.023285
Train Epoch: 2 Iteration: 330 [10560/34075 (31%)]	 Batch 330 Loss: 0.189604
Train Epoch: 2 Iteration: 340 [10880/34075 (32%)]	 Batch 340 Loss: 0.052159
Train Epoch: 2 Iteration: 350 [11200/34075 (33%)]	 Batch 350 Loss: 0.082109
Train Epoch: 2 Iteration: 360 [11520/34075 (34%)]	 Batch 360 Loss: 0.227946
Train Epoch: 2 Iteration: 370 [11840/34075 (35%)]	 Batch 370 Loss: 0.139668
Train Epoch: 2 Iteration: 380 [12160/34075 (36%)]	 Batch 380 Loss: 0.031195
Train Epoch: 2 Iteration: 390 [12480/34075 (37%)]	 Batch 390 Loss: 0.124000
Train Epoch: 2 Iteration: 400 [12800/34075 (38%)]	 Batch 400 Loss: 0.059622
Train Epoch: 2 Iteration: 410 [13120/34075 (38%)]	 Batch 410 Loss: 0.093794
Train Epoch: 2 Iteration: 420 [13440/34075 (39%)]	 Batch 420 Loss: 0.182619
Train Epoch: 2 Iteration: 430 [13760/34075 (40%)]	 Batch 430 Loss: 0.128759
Train Epoch: 2 Iteration: 440 [14080/34075 (41%)]	 Batch 440 Loss: 0.115006
Train Epoch: 2 Iteration: 450 [14400/34075 (42%)]	 Batch 450 Loss: 0.062451
Train Epoch: 2 Iteration: 460 [14720/34075 (43%)]	 Batch 460 Loss: 0.223305
Train Epoch: 2 Iteration: 470 [15040/34075 (44%)]	 Batch 470 Loss: 0.382533
Train Epoch: 2 Iteration: 480 [15360/34075 (45%)]	 Batch 480 Loss: 0.119173
Train Epoch: 2 Iteration: 490 [15680/34075 (46%)]	 Batch 490 Loss: 0.120092
Train Epoch: 2 Iteration: 500 [16000/34075 (47%)]	 Batch 500 Loss: 0.078977
Train Epoch: 2 Iteration: 510 [16320/34075 (48%)]	 Batch 510 Loss: 0.065304
Train Epoch: 2 Iteration: 520 [16640/34075 (49%)]	 Batch 520 Loss: 0.198794
Train Epoch: 2 Iteration: 530 [16960/34075 (50%)]	 Batch 530 Loss: 0.089598
Train Epoch: 2 Iteration: 540 [17280/34075 (51%)]	 Batch 540 Loss: 0.039852
Train Epoch: 2 Iteration: 550 [17600/34075 (52%)]	 Batch 550 Loss: 0.117896
Train Epoch: 2 Iteration: 560 [17920/34075 (53%)]	 Batch 560 Loss: 0.352500
Train Epoch: 2 Iteration: 570 [18240/34075 (54%)]	 Batch 570 Loss: 0.103635
Train Epoch: 2 Iteration: 580 [18560/34075 (54%)]	 Batch 580 Loss: 0.144496
Train Epoch: 2 Iteration: 590 [18880/34075 (55%)]	 Batch 590 Loss: 0.179676
Train Epoch: 2 Iteration: 600 [19200/34075 (56%)]	 Batch 600 Loss: 0.022156
Train Epoch: 2 Iteration: 610 [19520/34075 (57%)]	 Batch 610 Loss: 0.091597
Train Epoch: 2 Iteration: 620 [19840/34075 (58%)]	 Batch 620 Loss: 0.015546
Train Epoch: 2 Iteration: 630 [20160/34075 (59%)]	 Batch 630 Loss: 0.111920
Train Epoch: 2 Iteration: 640 [20480/34075 (60%)]	 Batch 640 Loss: 0.059949
Train Epoch: 2 Iteration: 650 [20800/34075 (61%)]	 Batch 650 Loss: 0.058491
Train Epoch: 2 Iteration: 660 [21120/34075 (62%)]	 Batch 660 Loss: 0.097663
Train Epoch: 2 Iteration: 670 [21440/34075 (63%)]	 Batch 670 Loss: 0.039152
Train Epoch: 2 Iteration: 680 [21760/34075 (64%)]	 Batch 680 Loss: 0.066894
Train Epoch: 2 Iteration: 690 [22080/34075 (65%)]	 Batch 690 Loss: 0.073972
Train Epoch: 2 Iteration: 700 [22400/34075 (66%)]	 Batch 700 Loss: 0.055423
Train Epoch: 2 Iteration: 710 [22720/34075 (67%)]	 Batch 710 Loss: 0.063409
Train Epoch: 2 Iteration: 720 [23040/34075 (68%)]	 Batch 720 Loss: 0.042316
Train Epoch: 2 Iteration: 730 [23360/34075 (69%)]	 Batch 730 Loss: 0.042104
Train Epoch: 2 Iteration: 740 [23680/34075 (69%)]	 Batch 740 Loss: 0.090122
Train Epoch: 2 Iteration: 750 [24000/34075 (70%)]	 Batch 750 Loss: 0.352011
Train Epoch: 2 Iteration: 760 [24320/34075 (71%)]	 Batch 760 Loss: 0.352068
Train Epoch: 2 Iteration: 770 [24640/34075 (72%)]	 Batch 770 Loss: 0.232268
Train Epoch: 2 Iteration: 780 [24960/34075 (73%)]	 Batch 780 Loss: 0.137696
Train Epoch: 2 Iteration: 790 [25280/34075 (74%)]	 Batch 790 Loss: 0.126542
Train Epoch: 2 Iteration: 800 [25600/34075 (75%)]	 Batch 800 Loss: 0.021553
Train Epoch: 2 Iteration: 810 [25920/34075 (76%)]	 Batch 810 Loss: 0.127139
Train Epoch: 2 Iteration: 820 [26240/34075 (77%)]	 Batch 820 Loss: 0.072637
Train Epoch: 2 Iteration: 830 [26560/34075 (78%)]	 Batch 830 Loss: 0.122478
Train Epoch: 2 Iteration: 840 [26880/34075 (79%)]	 Batch 840 Loss: 0.064649
Train Epoch: 2 Iteration: 850 [27200/34075 (80%)]	 Batch 850 Loss: 0.038767
Train Epoch: 2 Iteration: 860 [27520/34075 (81%)]	 Batch 860 Loss: 0.068245
Train Epoch: 2 Iteration: 870 [27840/34075 (82%)]	 Batch 870 Loss: 0.234782
Train Epoch: 2 Iteration: 880 [28160/34075 (83%)]	 Batch 880 Loss: 0.077910
Train Epoch: 2 Iteration: 890 [28480/34075 (84%)]	 Batch 890 Loss: 0.272052
Train Epoch: 2 Iteration: 900 [28800/34075 (85%)]	 Batch 900 Loss: 0.016212
Train Epoch: 2 Iteration: 910 [29120/34075 (85%)]	 Batch 910 Loss: 0.050351
Train Epoch: 2 Iteration: 920 [29440/34075 (86%)]	 Batch 920 Loss: 0.107082
Train Epoch: 2 Iteration: 930 [29760/34075 (87%)]	 Batch 930 Loss: 0.046439
Train Epoch: 2 Iteration: 940 [30080/34075 (88%)]	 Batch 940 Loss: 0.143061
Train Epoch: 2 Iteration: 950 [30400/34075 (89%)]	 Batch 950 Loss: 0.101547
Train Epoch: 2 Iteration: 960 [30720/34075 (90%)]	 Batch 960 Loss: 0.126701
Train Epoch: 2 Iteration: 970 [31040/34075 (91%)]	 Batch 970 Loss: 0.090998
Train Epoch: 2 Iteration: 980 [31360/34075 (92%)]	 Batch 980 Loss: 0.086963
Train Epoch: 2 Iteration: 990 [31680/34075 (93%)]	 Batch 990 Loss: 0.040373
Train Epoch: 2 Iteration: 1000 [32000/34075 (94%)]	 Batch 1000 Loss: 0.087240
Train Epoch: 2 Iteration: 1010 [32320/34075 (95%)]	 Batch 1010 Loss: 0.195165
Train Epoch: 2 Iteration: 1020 [32640/34075 (96%)]	 Batch 1020 Loss: 0.042522
Train Epoch: 2 Iteration: 1030 [32960/34075 (97%)]	 Batch 1030 Loss: 0.042125
Train Epoch: 2 Iteration: 1040 [33280/34075 (98%)]	 Batch 1040 Loss: 0.159745
Train Epoch: 2 Iteration: 1050 [33600/34075 (99%)]	 Batch 1050 Loss: 0.180273
Train Epoch: 2 Iteration: 1060 [33920/34075 (100%)]	 Batch 1060 Loss: 0.180360


----------------- Epoch 2 -----------------

validation computation time: 8.0  minutes
Confusion Matrix
tensor([[2236,  672,  185,  123],
        [ 814,  201,   65,   26],
        [ 122,   33,    8,   14],
        [  79,   21,   19,    3]])
class 0 accuracy: 68.7788%
class 1 accuracy: 21.6828%
class 2 accuracy: 2.8881%
class 3 accuracy: 1.8072%

Validation Loss: 2.0562, Accuracy: 2448/4621 (53%)
Training Loss:0.1283
Best Accuracy: 58.125947%
Time Elapsed: 1h 40m 54s

--------------------------------------------------------


Train Epoch: 3 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 0.139454
Train Epoch: 3 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 0.050450
Train Epoch: 3 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 0.128532
Train Epoch: 3 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 0.050018
Train Epoch: 3 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 0.102290
Train Epoch: 3 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 0.015226
Train Epoch: 3 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 0.017434
Train Epoch: 3 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 0.017928
Train Epoch: 3 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 0.178782
Train Epoch: 3 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 0.223677
Train Epoch: 3 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.052277
Train Epoch: 3 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.120573
Train Epoch: 3 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.024860
Train Epoch: 3 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.075830
Train Epoch: 3 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.063727
Train Epoch: 3 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.080465
Train Epoch: 3 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 0.231481
Train Epoch: 3 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 0.052721
Train Epoch: 3 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 0.050604
Train Epoch: 3 Iteration: 200 [6400/34075 (19%)]	 Batch 200 Loss: 0.045802
Train Epoch: 3 Iteration: 210 [6720/34075 (20%)]	 Batch 210 Loss: 0.052923
Train Epoch: 3 Iteration: 220 [7040/34075 (21%)]	 Batch 220 Loss: 0.137682
Train Epoch: 3 Iteration: 230 [7360/34075 (22%)]	 Batch 230 Loss: 0.260062
Train Epoch: 3 Iteration: 240 [7680/34075 (23%)]	 Batch 240 Loss: 0.135511
Train Epoch: 3 Iteration: 250 [8000/34075 (23%)]	 Batch 250 Loss: 0.124021
Train Epoch: 3 Iteration: 260 [8320/34075 (24%)]	 Batch 260 Loss: 0.063246
Train Epoch: 3 Iteration: 270 [8640/34075 (25%)]	 Batch 270 Loss: 0.023274
Train Epoch: 3 Iteration: 280 [8960/34075 (26%)]	 Batch 280 Loss: 0.115221
Train Epoch: 3 Iteration: 290 [9280/34075 (27%)]	 Batch 290 Loss: 0.077631
Train Epoch: 3 Iteration: 300 [9600/34075 (28%)]	 Batch 300 Loss: 0.063165
Train Epoch: 3 Iteration: 310 [9920/34075 (29%)]	 Batch 310 Loss: 0.044180
Train Epoch: 3 Iteration: 320 [10240/34075 (30%)]	 Batch 320 Loss: 0.090783
Train Epoch: 3 Iteration: 330 [10560/34075 (31%)]	 Batch 330 Loss: 0.042000
Train Epoch: 3 Iteration: 340 [10880/34075 (32%)]	 Batch 340 Loss: 0.075724
Train Epoch: 3 Iteration: 350 [11200/34075 (33%)]	 Batch 350 Loss: 0.019502
Train Epoch: 3 Iteration: 360 [11520/34075 (34%)]	 Batch 360 Loss: 0.041109
Train Epoch: 3 Iteration: 370 [11840/34075 (35%)]	 Batch 370 Loss: 0.093539
Train Epoch: 3 Iteration: 380 [12160/34075 (36%)]	 Batch 380 Loss: 0.063699
Train Epoch: 3 Iteration: 390 [12480/34075 (37%)]	 Batch 390 Loss: 0.045408
Train Epoch: 3 Iteration: 400 [12800/34075 (38%)]	 Batch 400 Loss: 0.073348
Train Epoch: 3 Iteration: 410 [13120/34075 (38%)]	 Batch 410 Loss: 0.035627
Train Epoch: 3 Iteration: 420 [13440/34075 (39%)]	 Batch 420 Loss: 0.071483
Train Epoch: 3 Iteration: 430 [13760/34075 (40%)]	 Batch 430 Loss: 0.011878
Train Epoch: 3 Iteration: 440 [14080/34075 (41%)]	 Batch 440 Loss: 0.041530
Train Epoch: 3 Iteration: 450 [14400/34075 (42%)]	 Batch 450 Loss: 0.055480
Train Epoch: 3 Iteration: 460 [14720/34075 (43%)]	 Batch 460 Loss: 0.106646
Train Epoch: 3 Iteration: 470 [15040/34075 (44%)]	 Batch 470 Loss: 0.050107
Train Epoch: 3 Iteration: 480 [15360/34075 (45%)]	 Batch 480 Loss: 0.017153
Train Epoch: 3 Iteration: 490 [15680/34075 (46%)]	 Batch 490 Loss: 0.076767
Train Epoch: 3 Iteration: 500 [16000/34075 (47%)]	 Batch 500 Loss: 0.016623
Train Epoch: 3 Iteration: 510 [16320/34075 (48%)]	 Batch 510 Loss: 0.053386
Train Epoch: 3 Iteration: 520 [16640/34075 (49%)]	 Batch 520 Loss: 0.143298
Train Epoch: 3 Iteration: 530 [16960/34075 (50%)]	 Batch 530 Loss: 0.027144
Train Epoch: 3 Iteration: 540 [17280/34075 (51%)]	 Batch 540 Loss: 0.084653
Train Epoch: 3 Iteration: 550 [17600/34075 (52%)]	 Batch 550 Loss: 0.076633
Train Epoch: 3 Iteration: 560 [17920/34075 (53%)]	 Batch 560 Loss: 0.109252
Train Epoch: 3 Iteration: 570 [18240/34075 (54%)]	 Batch 570 Loss: 0.059378
Train Epoch: 3 Iteration: 580 [18560/34075 (54%)]	 Batch 580 Loss: 0.155502
Train Epoch: 3 Iteration: 590 [18880/34075 (55%)]	 Batch 590 Loss: 0.128485
Train Epoch: 3 Iteration: 600 [19200/34075 (56%)]	 Batch 600 Loss: 0.028775
Train Epoch: 3 Iteration: 610 [19520/34075 (57%)]	 Batch 610 Loss: 0.018375
Train Epoch: 3 Iteration: 620 [19840/34075 (58%)]	 Batch 620 Loss: 0.033451
Train Epoch: 3 Iteration: 630 [20160/34075 (59%)]	 Batch 630 Loss: 0.096592
Train Epoch: 3 Iteration: 640 [20480/34075 (60%)]	 Batch 640 Loss: 0.145135
Train Epoch: 3 Iteration: 650 [20800/34075 (61%)]	 Batch 650 Loss: 0.076829
Train Epoch: 3 Iteration: 660 [21120/34075 (62%)]	 Batch 660 Loss: 0.023301
Train Epoch: 3 Iteration: 670 [21440/34075 (63%)]	 Batch 670 Loss: 0.059909
Train Epoch: 3 Iteration: 680 [21760/34075 (64%)]	 Batch 680 Loss: 0.076326
Train Epoch: 3 Iteration: 690 [22080/34075 (65%)]	 Batch 690 Loss: 0.059451
Train Epoch: 3 Iteration: 700 [22400/34075 (66%)]	 Batch 700 Loss: 0.037281
Train Epoch: 3 Iteration: 710 [22720/34075 (67%)]	 Batch 710 Loss: 0.050164
Train Epoch: 3 Iteration: 720 [23040/34075 (68%)]	 Batch 720 Loss: 0.067479
Train Epoch: 3 Iteration: 730 [23360/34075 (69%)]	 Batch 730 Loss: 0.237525
Train Epoch: 3 Iteration: 740 [23680/34075 (69%)]	 Batch 740 Loss: 0.034662
Train Epoch: 3 Iteration: 750 [24000/34075 (70%)]	 Batch 750 Loss: 0.017322
Train Epoch: 3 Iteration: 760 [24320/34075 (71%)]	 Batch 760 Loss: 0.341883
Train Epoch: 3 Iteration: 770 [24640/34075 (72%)]	 Batch 770 Loss: 0.097441
Train Epoch: 3 Iteration: 780 [24960/34075 (73%)]	 Batch 780 Loss: 0.012284
Train Epoch: 3 Iteration: 790 [25280/34075 (74%)]	 Batch 790 Loss: 0.041370
Train Epoch: 3 Iteration: 800 [25600/34075 (75%)]	 Batch 800 Loss: 0.087581
Train Epoch: 3 Iteration: 810 [25920/34075 (76%)]	 Batch 810 Loss: 0.118168
Train Epoch: 3 Iteration: 820 [26240/34075 (77%)]	 Batch 820 Loss: 0.119982
Train Epoch: 3 Iteration: 830 [26560/34075 (78%)]	 Batch 830 Loss: 0.073752
Train Epoch: 3 Iteration: 840 [26880/34075 (79%)]	 Batch 840 Loss: 0.052175
Train Epoch: 3 Iteration: 850 [27200/34075 (80%)]	 Batch 850 Loss: 0.040982
Train Epoch: 3 Iteration: 860 [27520/34075 (81%)]	 Batch 860 Loss: 0.036853
Train Epoch: 3 Iteration: 870 [27840/34075 (82%)]	 Batch 870 Loss: 0.168526
Train Epoch: 3 Iteration: 880 [28160/34075 (83%)]	 Batch 880 Loss: 0.056743
Train Epoch: 3 Iteration: 890 [28480/34075 (84%)]	 Batch 890 Loss: 0.022984
Train Epoch: 3 Iteration: 900 [28800/34075 (85%)]	 Batch 900 Loss: 0.264900
Train Epoch: 3 Iteration: 910 [29120/34075 (85%)]	 Batch 910 Loss: 0.061549
Train Epoch: 3 Iteration: 920 [29440/34075 (86%)]	 Batch 920 Loss: 0.014198
Train Epoch: 3 Iteration: 930 [29760/34075 (87%)]	 Batch 930 Loss: 0.075693
Train Epoch: 3 Iteration: 940 [30080/34075 (88%)]	 Batch 940 Loss: 0.096263
Train Epoch: 3 Iteration: 950 [30400/34075 (89%)]	 Batch 950 Loss: 0.055947
Train Epoch: 3 Iteration: 960 [30720/34075 (90%)]	 Batch 960 Loss: 0.124129
Train Epoch: 3 Iteration: 970 [31040/34075 (91%)]	 Batch 970 Loss: 0.043794
Train Epoch: 3 Iteration: 980 [31360/34075 (92%)]	 Batch 980 Loss: 0.105035
Train Epoch: 3 Iteration: 990 [31680/34075 (93%)]	 Batch 990 Loss: 0.136070
Train Epoch: 3 Iteration: 1000 [32000/34075 (94%)]	 Batch 1000 Loss: 0.317432
Train Epoch: 3 Iteration: 1010 [32320/34075 (95%)]	 Batch 1010 Loss: 0.016017
Train Epoch: 3 Iteration: 1020 [32640/34075 (96%)]	 Batch 1020 Loss: 0.110388
Train Epoch: 3 Iteration: 1030 [32960/34075 (97%)]	 Batch 1030 Loss: 0.017354
Train Epoch: 3 Iteration: 1040 [33280/34075 (98%)]	 Batch 1040 Loss: 0.097766
Train Epoch: 3 Iteration: 1050 [33600/34075 (99%)]	 Batch 1050 Loss: 0.049276
Train Epoch: 3 Iteration: 1060 [33920/34075 (100%)]	 Batch 1060 Loss: 0.038788


----------------- Epoch 3 -----------------

validation computation time: 9.0  minutes
Confusion Matrix
tensor([[2489,  767,  211,  144],
        [ 608,  121,   47,   11],
        [  81,   27,    7,    7],
        [  73,   12,   12,    4]])
class 0 accuracy: 76.5611%
class 1 accuracy: 13.0529%
class 2 accuracy: 2.5271%
class 3 accuracy: 2.4096%

Validation Loss: 2.3009, Accuracy: 2621/4621 (57%)
Training Loss:0.0836
Best Accuracy: 58.125947%
Time Elapsed: 2h 18m 14s

--------------------------------------------------------


Train Epoch: 4 Iteration: 10 [320/34075 (1%)]	 Batch 10 Loss: 0.042198
Train Epoch: 4 Iteration: 20 [640/34075 (2%)]	 Batch 20 Loss: 0.072586
Train Epoch: 4 Iteration: 30 [960/34075 (3%)]	 Batch 30 Loss: 0.045329
Train Epoch: 4 Iteration: 40 [1280/34075 (4%)]	 Batch 40 Loss: 0.036274
Train Epoch: 4 Iteration: 50 [1600/34075 (5%)]	 Batch 50 Loss: 0.130452
Train Epoch: 4 Iteration: 60 [1920/34075 (6%)]	 Batch 60 Loss: 0.033889
Train Epoch: 4 Iteration: 70 [2240/34075 (7%)]	 Batch 70 Loss: 0.009393
Train Epoch: 4 Iteration: 80 [2560/34075 (8%)]	 Batch 80 Loss: 0.017201
Train Epoch: 4 Iteration: 90 [2880/34075 (8%)]	 Batch 90 Loss: 0.026061
Train Epoch: 4 Iteration: 100 [3200/34075 (9%)]	 Batch 100 Loss: 0.020632
Train Epoch: 4 Iteration: 110 [3520/34075 (10%)]	 Batch 110 Loss: 0.017154
Train Epoch: 4 Iteration: 120 [3840/34075 (11%)]	 Batch 120 Loss: 0.006473
Train Epoch: 4 Iteration: 130 [4160/34075 (12%)]	 Batch 130 Loss: 0.032531
Train Epoch: 4 Iteration: 140 [4480/34075 (13%)]	 Batch 140 Loss: 0.006966
Train Epoch: 4 Iteration: 150 [4800/34075 (14%)]	 Batch 150 Loss: 0.011991
Train Epoch: 4 Iteration: 160 [5120/34075 (15%)]	 Batch 160 Loss: 0.027388
Train Epoch: 4 Iteration: 170 [5440/34075 (16%)]	 Batch 170 Loss: 0.109344
Train Epoch: 4 Iteration: 180 [5760/34075 (17%)]	 Batch 180 Loss: 0.086441
Train Epoch: 4 Iteration: 190 [6080/34075 (18%)]	 Batch 190 Loss: 0.050061
Train Epoch: 4 Iteration: 200 [6400/34075 (19%)]	 Batch 200 Loss: 0.040840
Train Epoch: 4 Iteration: 210 [6720/34075 (20%)]	 Batch 210 Loss: 0.030213
Train Epoch: 4 Iteration: 220 [7040/34075 (21%)]	 Batch 220 Loss: 0.049897
Train Epoch: 4 Iteration: 230 [7360/34075 (22%)]	 Batch 230 Loss: 0.015414
Train Epoch: 4 Iteration: 240 [7680/34075 (23%)]	 Batch 240 Loss: 0.039374
Train Epoch: 4 Iteration: 250 [8000/34075 (23%)]	 Batch 250 Loss: 0.029137
Train Epoch: 4 Iteration: 260 [8320/34075 (24%)]	 Batch 260 Loss: 0.287237
Train Epoch: 4 Iteration: 270 [8640/34075 (25%)]	 Batch 270 Loss: 0.123767
Train Epoch: 4 Iteration: 280 [8960/34075 (26%)]	 Batch 280 Loss: 0.068156
Train Epoch: 4 Iteration: 290 [9280/34075 (27%)]	 Batch 290 Loss: 0.009065
Train Epoch: 4 Iteration: 300 [9600/34075 (28%)]	 Batch 300 Loss: 0.021067
Train Epoch: 4 Iteration: 310 [9920/34075 (29%)]	 Batch 310 Loss: 0.085518
Train Epoch: 4 Iteration: 320 [10240/34075 (30%)]	 Batch 320 Loss: 0.067466
Train Epoch: 4 Iteration: 330 [10560/34075 (31%)]	 Batch 330 Loss: 0.091722
Train Epoch: 4 Iteration: 340 [10880/34075 (32%)]	 Batch 340 Loss: 0.071126
Train Epoch: 4 Iteration: 350 [11200/34075 (33%)]	 Batch 350 Loss: 0.033434
Train Epoch: 4 Iteration: 360 [11520/34075 (34%)]	 Batch 360 Loss: 0.021256
Train Epoch: 4 Iteration: 370 [11840/34075 (35%)]	 Batch 370 Loss: 0.012599
Train Epoch: 4 Iteration: 380 [12160/34075 (36%)]	 Batch 380 Loss: 0.083735
Train Epoch: 4 Iteration: 390 [12480/34075 (37%)]	 Batch 390 Loss: 0.009578
Train Epoch: 4 Iteration: 400 [12800/34075 (38%)]	 Batch 400 Loss: 0.005009
Train Epoch: 4 Iteration: 410 [13120/34075 (38%)]	 Batch 410 Loss: 0.019190
Train Epoch: 4 Iteration: 420 [13440/34075 (39%)]	 Batch 420 Loss: 0.074729
Train Epoch: 4 Iteration: 430 [13760/34075 (40%)]	 Batch 430 Loss: 0.209568
Train Epoch: 4 Iteration: 440 [14080/34075 (41%)]	 Batch 440 Loss: 0.017634
Train Epoch: 4 Iteration: 450 [14400/34075 (42%)]	 Batch 450 Loss: 0.074668
Train Epoch: 4 Iteration: 460 [14720/34075 (43%)]	 Batch 460 Loss: 0.060938
Train Epoch: 4 Iteration: 470 [15040/34075 (44%)]	 Batch 470 Loss: 0.021394
Train Epoch: 4 Iteration: 480 [15360/34075 (45%)]	 Batch 480 Loss: 0.029978
Train Epoch: 4 Iteration: 490 [15680/34075 (46%)]	 Batch 490 Loss: 0.025308
Train Epoch: 4 Iteration: 500 [16000/34075 (47%)]	 Batch 500 Loss: 0.005136
Train Epoch: 4 Iteration: 510 [16320/34075 (48%)]	 Batch 510 Loss: 0.024727
Train Epoch: 4 Iteration: 520 [16640/34075 (49%)]	 Batch 520 Loss: 0.209649
Train Epoch: 4 Iteration: 530 [16960/34075 (50%)]	 Batch 530 Loss: 0.047320
Train Epoch: 4 Iteration: 540 [17280/34075 (51%)]	 Batch 540 Loss: 0.061506
Train Epoch: 4 Iteration: 550 [17600/34075 (52%)]	 Batch 550 Loss: 0.036749
Train Epoch: 4 Iteration: 560 [17920/34075 (53%)]	 Batch 560 Loss: 0.005704
Train Epoch: 4 Iteration: 570 [18240/34075 (54%)]	 Batch 570 Loss: 0.005161
Train Epoch: 4 Iteration: 580 [18560/34075 (54%)]	 Batch 580 Loss: 0.019077
Train Epoch: 4 Iteration: 590 [18880/34075 (55%)]	 Batch 590 Loss: 0.070837
Train Epoch: 4 Iteration: 600 [19200/34075 (56%)]	 Batch 600 Loss: 0.010597
Train Epoch: 4 Iteration: 610 [19520/34075 (57%)]	 Batch 610 Loss: 0.059259
Train Epoch: 4 Iteration: 620 [19840/34075 (58%)]	 Batch 620 Loss: 0.007044
Train Epoch: 4 Iteration: 630 [20160/34075 (59%)]	 Batch 630 Loss: 0.074957
Train Epoch: 4 Iteration: 640 [20480/34075 (60%)]	 Batch 640 Loss: 0.123837
Train Epoch: 4 Iteration: 650 [20800/34075 (61%)]	 Batch 650 Loss: 0.148699
Train Epoch: 4 Iteration: 660 [21120/34075 (62%)]	 Batch 660 Loss: 0.217213
Train Epoch: 4 Iteration: 670 [21440/34075 (63%)]	 Batch 670 Loss: 0.005455
Train Epoch: 4 Iteration: 680 [21760/34075 (64%)]	 Batch 680 Loss: 0.086771
Train Epoch: 4 Iteration: 690 [22080/34075 (65%)]	 Batch 690 Loss: 0.012562
Train Epoch: 4 Iteration: 700 [22400/34075 (66%)]	 Batch 700 Loss: 0.044851
Train Epoch: 4 Iteration: 710 [22720/34075 (67%)]	 Batch 710 Loss: 0.082313
Train Epoch: 4 Iteration: 720 [23040/34075 (68%)]	 Batch 720 Loss: 0.012547
Train Epoch: 4 Iteration: 730 [23360/34075 (69%)]	 Batch 730 Loss: 0.028970
Train Epoch: 4 Iteration: 740 [23680/34075 (69%)]	 Batch 740 Loss: 0.009948
Train Epoch: 4 Iteration: 750 [24000/34075 (70%)]	 Batch 750 Loss: 0.027912
Train Epoch: 4 Iteration: 760 [24320/34075 (71%)]	 Batch 760 Loss: 0.040701
Train Epoch: 4 Iteration: 770 [24640/34075 (72%)]	 Batch 770 Loss: 0.111218
Train Epoch: 4 Iteration: 780 [24960/34075 (73%)]	 Batch 780 Loss: 0.015208
Train Epoch: 4 Iteration: 790 [25280/34075 (74%)]	 Batch 790 Loss: 0.032923
Train Epoch: 4 Iteration: 800 [25600/34075 (75%)]	 Batch 800 Loss: 0.002539
Train Epoch: 4 Iteration: 810 [25920/34075 (76%)]	 Batch 810 Loss: 0.040157
Train Epoch: 4 Iteration: 820 [26240/34075 (77%)]	 Batch 820 Loss: 0.168707
Train Epoch: 4 Iteration: 830 [26560/34075 (78%)]	 Batch 830 Loss: 0.111658
Train Epoch: 4 Iteration: 840 [26880/34075 (79%)]	 Batch 840 Loss: 0.020304
Train Epoch: 4 Iteration: 850 [27200/34075 (80%)]	 Batch 850 Loss: 0.015993
Train Epoch: 4 Iteration: 860 [27520/34075 (81%)]	 Batch 860 Loss: 0.035504
Train Epoch: 4 Iteration: 870 [27840/34075 (82%)]	 Batch 870 Loss: 0.015499
Train Epoch: 4 Iteration: 880 [28160/34075 (83%)]	 Batch 880 Loss: 0.082122
Train Epoch: 4 Iteration: 890 [28480/34075 (84%)]	 Batch 890 Loss: 0.125771
Train Epoch: 4 Iteration: 900 [28800/34075 (85%)]	 Batch 900 Loss: 0.053985
Train Epoch: 4 Iteration: 910 [29120/34075 (85%)]	 Batch 910 Loss: 0.030499
Train Epoch: 4 Iteration: 920 [29440/34075 (86%)]	 Batch 920 Loss: 0.158059
Train Epoch: 4 Iteration: 930 [29760/34075 (87%)]	 Batch 930 Loss: 0.077406
Train Epoch: 4 Iteration: 940 [30080/34075 (88%)]	 Batch 940 Loss: 0.054156
Train Epoch: 4 Iteration: 950 [30400/34075 (89%)]	 Batch 950 Loss: 0.070813
Train Epoch: 4 Iteration: 960 [30720/34075 (90%)]	 Batch 960 Loss: 0.014831
Train Epoch: 4 Iteration: 970 [31040/34075 (91%)]	 Batch 970 Loss: 0.449844
Train Epoch: 4 Iteration: 980 [31360/34075 (92%)]	 Batch 980 Loss: 0.083272
Train Epoch: 4 Iteration: 990 [31680/34075 (93%)]	 Batch 990 Loss: 0.051545
Train Epoch: 4 Iteration: 1000 [32000/34075 (94%)]	 Batch 1000 Loss: 0.037179
Train Epoch: 4 Iteration: 1010 [32320/34075 (95%)]	 Batch 1010 Loss: 0.017430
Train Epoch: 4 Iteration: 1020 [32640/34075 (96%)]	 Batch 1020 Loss: 0.026932
Train Epoch: 4 Iteration: 1030 [32960/34075 (97%)]	 Batch 1030 Loss: 0.048621
Train Epoch: 4 Iteration: 1040 [33280/34075 (98%)]	 Batch 1040 Loss: 0.009735
Train Epoch: 4 Iteration: 1050 [33600/34075 (99%)]	 Batch 1050 Loss: 0.179292
Train Epoch: 4 Iteration: 1060 [33920/34075 (100%)]	 Batch 1060 Loss: 0.076177


----------------- Epoch 4 -----------------

validation computation time: 8.0  minutes
Confusion Matrix
tensor([[2377,  722,  185,  134],
        [ 684,  143,   73,   14],
        [ 104,   42,    6,   13],
        [  86,   20,   13,    5]])
class 0 accuracy: 73.1160%
class 1 accuracy: 15.4261%
class 2 accuracy: 2.1661%
class 3 accuracy: 3.0120%

Validation Loss: 2.5952, Accuracy: 2531/4621 (55%)
Training Loss:0.0591
Best Accuracy: 58.125947%
Time Elapsed: 2h 59m 5s

--------------------------------------------------------


================================ Finished Training ================================
validation computation time: 9.0  minutes
Incorrect Samples: [(3869, 'P:0 GT:1'), (4453, 'P:0 GT:3'), (698, 'P:0 GT:1'), (2080, 'P:1 GT:0'), (3872, 'P:0 GT:1'), (295, 'P:0 GT:1'), (3192, 'P:1 GT:0'), (2879, 'P:1 GT:2'), (3769, 'P:2 GT:1'), (2478, 'P:0 GT:1'), (2466, 'P:0 GT:1'), (316, 'P:0 GT:1'), (2800, 'P:0 GT:2'), (59, 'P:1 GT:0'), (10, 'P:3 GT:0'), (2931, 'P:0 GT:2'), (3374, 'P:3 GT:0'), (3648, 'P:1 GT:0'), (3832, 'P:0 GT:1'), (4542, 'P:0 GT:3'), (4108, 'P:1 GT:0'), (2852, 'P:1 GT:2'), (4244, 'P:1 GT:0'), (3896, 'P:0 GT:1'), (763, 'P:0 GT:1'), (117, 'P:2 GT:0'), (2446, 'P:0 GT:1'), (1496, 'P:0 GT:1'), (2679, 'P:1 GT:0'), (3493, 'P:2 GT:0'), (4338, 'P:1 GT:0'), (3798, 'P:0 GT:1'), (2038, 'P:1 GT:0'), (4547, 'P:0 GT:3'), (1806, 'P:0 GT:1'), (4012, 'P:1 GT:0'), (2915, 'P:1 GT:2'), (3569, 'P:1 GT:0'), (4490, 'P:0 GT:3'), (3583, 'P:1 GT:0'), (320, 'P:0 GT:1'), (1893, 'P:1 GT:0'), (3773, 'P:0 GT:1'), (2246, 'P:1 GT:0'), (3865, 'P:0 GT:1'), (103, 'P:3 GT:0'), (885, 'P:0 GT:1'), (1724, 'P:0 GT:2'), (706, 'P:0 GT:1'), (905, 'P:0 GT:1'), (3545, 'P:1 GT:0'), (2253, 'P:1 GT:0'), (2406, 'P:3 GT:1'), (4583, 'P:1 GT:0'), (1756, 'P:0 GT:2'), (227, 'P:2 GT:0'), (2939, 'P:1 GT:2'), (1821, 'P:0 GT:1'), (2615, 'P:1 GT:0'), (4339, 'P:2 GT:0'), (2073, 'P:1 GT:0'), (2045, 'P:1 GT:0'), (1405, 'P:1 GT:0'), (2779, 'P:1 GT:0'), (4520, 'P:1 GT:3'), (910, 'P:0 GT:1'), (2787, 'P:1 GT:2'), (2950, 'P:1 GT:2'), (2628, 'P:1 GT:0'), (2916, 'P:1 GT:2'), (4503, 'P:0 GT:3'), (2910, 'P:1 GT:2'), (1127, 'P:2 GT:0'), (3525, 'P:0 GT:1'), (872, 'P:0 GT:1'), (1721, 'P:0 GT:2'), (812, 'P:0 GT:1'), (1566, 'P:0 GT:1'), (3787, 'P:0 GT:1'), (964, 'P:1 GT:0'), (1116, 'P:1 GT:0'), (4178, 'P:2 GT:0'), (3175, 'P:1 GT:0'), (3306, 'P:1 GT:0'), (1562, 'P:0 GT:1'), (115, 'P:1 GT:0'), (4208, 'P:1 GT:0'), (2043, 'P:1 GT:0'), (2828, 'P:0 GT:2'), (2033, 'P:1 GT:0'), (3354, 'P:1 GT:0'), (4485, 'P:0 GT:3'), (1613, 'P:1 GT:0'), (3917, 'P:3 GT:1'), (1030, 'P:1 GT:0'), (2081, 'P:1 GT:0'), (455, 'P:0 GT:1'), (1312, 'P:1 GT:0'), (4433, 'P:0 GT:3'), (2467, 'P:0 GT:1'), (3232, 'P:1 GT:0'), (1525, 'P:0 GT:1'), (2449, 'P:0 GT:1'), (4032, 'P:1 GT:0'), (4478, 'P:0 GT:3'), (32, 'P:1 GT:0'), (17, 'P:1 GT:0'), (196, 'P:1 GT:0'), (2645, 'P:1 GT:0'), (2665, 'P:1 GT:0'), (2245, 'P:1 GT:0'), (2846, 'P:0 GT:2'), (4463, 'P:0 GT:3'), (2410, 'P:0 GT:1'), (4554, 'P:0 GT:3'), (4359, 'P:1 GT:0'), (3836, 'P:0 GT:1'), (1783, 'P:2 GT:1'), (3728, 'P:3 GT:0'), (922, 'P:0 GT:1'), (1788, 'P:0 GT:1'), (1568, 'P:0 GT:1'), (3821, 'P:0 GT:1'), (4156, 'P:3 GT:0'), (2326, 'P:1 GT:0'), (3818, 'P:0 GT:1'), (1719, 'P:1 GT:2'), (523, 'P:1 GT:0'), (4545, 'P:0 GT:3'), (2212, 'P:1 GT:0'), (699, 'P:0 GT:1'), (802, 'P:0 GT:1'), (4404, 'P:1 GT:0'), (2076, 'P:1 GT:0'), (1906, 'P:1 GT:0'), (1886, 'P:1 GT:0'), (2513, 'P:2 GT:1'), (1675, 'P:1 GT:0'), (300, 'P:0 GT:1'), (4506, 'P:0 GT:3'), (3343, 'P:1 GT:0'), (2501, 'P:0 GT:1'), (51, 'P:3 GT:0'), (3775, 'P:0 GT:1'), (1339, 'P:1 GT:0'), (1917, 'P:2 GT:0'), (1183, 'P:1 GT:0'), (35, 'P:1 GT:0'), (2128, 'P:1 GT:0'), (3739, 'P:1 GT:0'), (1819, 'P:0 GT:1'), (4146, 'P:3 GT:0'), (1641, 'P:0 GT:2'), (3938, 'P:1 GT:0'), (3922, 'P:0 GT:1'), (790, 'P:0 GT:1'), (521, 'P:1 GT:0'), (3864, 'P:0 GT:1'), (127, 'P:1 GT:0'), (4462, 'P:0 GT:3'), (3812, 'P:0 GT:1'), (784, 'P:0 GT:1'), (2607, 'P:1 GT:0'), (2336, 'P:1 GT:0'), (849, 'P:0 GT:1'), (2817, 'P:0 GT:2'), (4374, 'P:1 GT:0'), (1379, 'P:1 GT:0'), (2875, 'P:0 GT:2'), (4537, 'P:0 GT:3'), (4500, 'P:0 GT:3'), (540, 'P:3 GT:0'), (3496, 'P:0 GT:1'), (2427, 'P:0 GT:1'), (2231, 'P:1 GT:0'), (4581, 'P:0 GT:3'), (312, 'P:0 GT:1'), (2792, 'P:1 GT:2'), (3520, 'P:0 GT:1'), (571, 'P:1 GT:0'), (4486, 'P:0 GT:3'), (392, 'P:0 GT:1'), (2520, 'P:0 GT:1'), (1695, 'P:1 GT:0'), (2487, 'P:2 GT:1'), (648, 'P:1 GT:0'), (2181, 'P:1 GT:0'), (171, 'P:2 GT:0'), (1772, 'P:1 GT:2'), (1567, 'P:0 GT:1'), (4384, 'P:1 GT:0'), (4102, 'P:1 GT:0'), (2903, 'P:0 GT:2'), (1929, 'P:2 GT:0'), (2273, 'P:1 GT:0'), (2626, 'P:1 GT:0'), (650, 'P:1 GT:0'), (144, 'P:1 GT:0'), (4281, 'P:3 GT:0'), (4380, 'P:1 GT:0'), (1682, 'P:1 GT:0'), (1688, 'P:1 GT:0'), (3106, 'P:1 GT:0'), (3519, 'P:0 GT:1'), (2492, 'P:0 GT:1'), (2218, 'P:1 GT:0'), (714, 'P:0 GT:1'), (900, 'P:0 GT:1'), (128, 'P:1 GT:0'), (3840, 'P:0 GT:1'), (827, 'P:0 GT:1'), (2551, 'P:1 GT:0'), (262, 'P:3 GT:0'), (4578, 'P:2 GT:3'), (224, 'P:1 GT:0'), (3277, 'P:3 GT:0'), (3885, 'P:0 GT:1'), (1817, 'P:0 GT:1'), (1714, 'P:1 GT:2'), (4389, 'P:1 GT:0'), (2190, 'P:1 GT:0'), (887, 'P:0 GT:1'), (4539, 'P:0 GT:3'), (1857, 'P:0 GT:1'), (1578, 'P:1 GT:0'), (307, 'P:0 GT:1'), (2647, 'P:1 GT:0'), (1429, 'P:1 GT:0'), (1090, 'P:1 GT:0'), (3965, 'P:1 GT:0'), (3789, 'P:0 GT:1'), (2804, 'P:0 GT:2'), (3867, 'P:0 GT:1'), (4494, 'P:0 GT:3'), (715, 'P:0 GT:1'), (2894, 'P:0 GT:2'), (2360, 'P:1 GT:0'), (739, 'P:0 GT:1'), (2428, 'P:0 GT:1'), (1521, 'P:0 GT:1'), (287, 'P:0 GT:1'), (4492, 'P:0 GT:3'), (195, 'P:2 GT:0'), (2748, 'P:1 GT:0'), (3803, 'P:0 GT:1'), (3521, 'P:0 GT:1'), (3269, 'P:1 GT:0'), (1583, 'P:1 GT:0'), (1217, 'P:3 GT:0'), (3233, 'P:1 GT:0'), (2343, 'P:1 GT:0'), (29, 'P:1 GT:0'), (1653, 'P:0 GT:2'), (2953, 'P:0 GT:2'), (1750, 'P:1 GT:2'), (1491, 'P:0 GT:1'), (1912, 'P:3 GT:0'), (2738, 'P:1 GT:0'), (2928, 'P:0 GT:2'), (272, 'P:1 GT:0'), (4181, 'P:2 GT:0'), (2368, 'P:0 GT:1'), (25, 'P:1 GT:0'), (2318, 'P:1 GT:0'), (1895, 'P:2 GT:0'), (241, 'P:2 GT:0'), (3510, 'P:0 GT:1'), (3013, 'P:1 GT:0'), (2414, 'P:0 GT:1'), (3901, 'P:0 GT:1'), (1557, 'P:0 GT:1'), (3900, 'P:0 GT:1'), (4421, 'P:0 GT:3'), (743, 'P:0 GT:1'), (199, 'P:2 GT:0'), (211, 'P:2 GT:0'), (1610, 'P:1 GT:0'), (897, 'P:0 GT:1'), (2667, 'P:1 GT:0'), (712, 'P:0 GT:1'), (380, 'P:1 GT:0'), (2899, 'P:0 GT:2'), (750, 'P:0 GT:1'), (803, 'P:0 GT:1'), (2078, 'P:1 GT:0'), (424, 'P:0 GT:1'), (2704, 'P:3 GT:0'), (867, 'P:0 GT:1'), (541, 'P:1 GT:0'), (4544, 'P:2 GT:3'), (865, 'P:0 GT:1'), (267, 'P:2 GT:0'), (4378, 'P:1 GT:0'), (4029, 'P:1 GT:0'), (858, 'P:0 GT:1'), (783, 'P:0 GT:1'), (729, 'P:0 GT:1'), (649, 'P:1 GT:0'), (2363, 'P:0 GT:1'), (1925, 'P:3 GT:0'), (781, 'P:0 GT:1'), (1822, 'P:0 GT:1'), (2470, 'P:0 GT:1'), (2948, 'P:0 GT:2'), (2865, 'P:3 GT:2'), (3928, 'P:0 GT:1'), (2632, 'P:1 GT:0'), (477, 'P:1 GT:0'), (3924, 'P:0 GT:1'), (3919, 'P:0 GT:1'), (3531, 'P:0 GT:1'), (4491, 'P:0 GT:3'), (839, 'P:0 GT:1'), (226, 'P:1 GT:0'), (2860, 'P:0 GT:2'), (4502, 'P:0 GT:3'), (3052, 'P:1 GT:0'), (4394, 'P:1 GT:0'), (546, 'P:1 GT:0'), (4530, 'P:0 GT:3'), (3287, 'P:1 GT:0'), (1686, 'P:1 GT:0'), (2217, 'P:1 GT:0'), (52, 'P:1 GT:0'), (4006, 'P:1 GT:0'), (2935, 'P:0 GT:2'), (2035, 'P:1 GT:0'), (3827, 'P:0 GT:1'), (816, 'P:0 GT:1'), (702, 'P:0 GT:1'), (303, 'P:0 GT:1'), (3838, 'P:0 GT:1'), (1841, 'P:0 GT:1'), (3829, 'P:0 GT:1'), (1916, 'P:2 GT:0'), (934, 'P:0 GT:1'), (1896, 'P:1 GT:0'), (2872, 'P:1 GT:2'), (4472, 'P:0 GT:3'), (4569, 'P:0 GT:3'), (857, 'P:0 GT:1'), (3833, 'P:0 GT:1'), (1123, 'P:1 GT:0'), (2936, 'P:0 GT:2'), (1645, 'P:0 GT:2'), (4419, 'P:0 GT:3'), (3844, 'P:0 GT:1'), (4395, 'P:1 GT:0'), (2803, 'P:0 GT:2'), (806, 'P:0 GT:1'), (3494, 'P:0 GT:1'), (3195, 'P:1 GT:0'), (2558, 'P:1 GT:0'), (2898, 'P:0 GT:2'), (109, 'P:1 GT:0'), (3516, 'P:0 GT:1'), (319, 'P:0 GT:1'), (1742, 'P:0 GT:2'), (3823, 'P:0 GT:1'), (682, 'P:1 GT:0'), (3491, 'P:2 GT:0'), (1787, 'P:0 GT:1'), (1919, 'P:3 GT:0'), (1523, 'P:0 GT:1'), (3877, 'P:0 GT:1'), (1157, 'P:1 GT:0'), (4556, 'P:0 GT:3'), (951, 'P:1 GT:0'), (4534, 'P:0 GT:3'), (2914, 'P:3 GT:2'), (110, 'P:1 GT:0'), (3026, 'P:1 GT:0'), (4154, 'P:1 GT:0'), (2007, 'P:1 GT:0'), (4567, 'P:0 GT:3'), (4443, 'P:0 GT:3'), (2700, 'P:1 GT:0'), (2398, 'P:0 GT:1'), (28, 'P:3 GT:0'), (3873, 'P:0 GT:1'), (4448, 'P:0 GT:3'), (440, 'P:0 GT:1'), (3072, 'P:2 GT:0'), (2424, 'P:0 GT:1'), (4363, 'P:2 GT:0'), (809, 'P:0 GT:1'), (4370, 'P:1 GT:0'), (3547, 'P:1 GT:0'), (3495, 'P:0 GT:1'), (833, 'P:0 GT:1'), (271, 'P:2 GT:0'), (2957, 'P:0 GT:2'), (2714, 'P:1 GT:0'), (4487, 'P:0 GT:3'), (4532, 'P:0 GT:3'), (3753, 'P:1 GT:0'), (3499, 'P:0 GT:1'), (4434, 'P:0 GT:3'), (2870, 'P:1 GT:2'), (3039, 'P:1 GT:0'), (2516, 'P:0 GT:1'), (1397, 'P:1 GT:0'), (2402, 'P:0 GT:1'), (2876, 'P:0 GT:2'), (2785, 'P:1 GT:0'), (1656, 'P:0 GT:2'), (2502, 'P:0 GT:1'), (2677, 'P:1 GT:0'), (4536, 'P:0 GT:3'), (3768, 'P:0 GT:1'), (1651, 'P:0 GT:2'), (1826, 'P:0 GT:1'), (3770, 'P:0 GT:1'), (753, 'P:0 GT:1'), (2802, 'P:0 GT:2'), (896, 'P:0 GT:1'), (4241, 'P:1 GT:0'), (2822, 'P:1 GT:2'), (1825, 'P:2 GT:1'), (980, 'P:1 GT:0'), (1473, 'P:0 GT:1'), (1655, 'P:3 GT:2'), (3847, 'P:0 GT:1'), (305, 'P:0 GT:1'), (2837, 'P:0 GT:2'), (3776, 'P:0 GT:1'), (2649, 'P:1 GT:0'), (1735, 'P:1 GT:2'), (1710, 'P:0 GT:2'), (3385, 'P:3 GT:0'), (1883, 'P:2 GT:0'), (1402, 'P:1 GT:0'), (2438, 'P:0 GT:1'), (2669, 'P:1 GT:0'), (4616, 'P:3 GT:0'), (3508, 'P:0 GT:1'), (401, 'P:0 GT:1'), (3763, 'P:0 GT:1'), (1796, 'P:0 GT:1'), (4515, 'P:0 GT:3'), (2816, 'P:0 GT:2'), (2825, 'P:0 GT:2'), (3778, 'P:0 GT:1'), (317, 'P:0 GT:1'), (3346, 'P:2 GT:0'), (2047, 'P:1 GT:0'), (1600, 'P:1 GT:0'), (1490, 'P:0 GT:1'), (943, 'P:1 GT:0'), (4187, 'P:1 GT:0'), (3748, 'P:1 GT:0'), (916, 'P:2 GT:1'), (2798, 'P:1 GT:2'), (2943, 'P:1 GT:2'), (3532, 'P:0 GT:1'), (4027, 'P:1 GT:0'), (1033, 'P:1 GT:0'), (4474, 'P:0 GT:3'), (1804, 'P:2 GT:1'), (3831, 'P:0 GT:1'), (3518, 'P:0 GT:1'), (308, 'P:0 GT:1'), (2286, 'P:1 GT:0'), (3779, 'P:0 GT:1'), (233, 'P:2 GT:0'), (899, 'P:0 GT:1'), (2909, 'P:0 GT:2'), (2812, 'P:3 GT:2'), (877, 'P:0 GT:1'), (4232, 'P:1 GT:0'), (3860, 'P:0 GT:1'), (1697, 'P:0 GT:2'), (1969, 'P:1 GT:0'), (1374, 'P:1 GT:0'), (3253, 'P:1 GT:0'), (874, 'P:0 GT:1'), (1712, 'P:0 GT:2'), (2394, 'P:0 GT:1'), (835, 'P:0 GT:1'), (2646, 'P:1 GT:0'), (4139, 'P:1 GT:0'), (2797, 'P:0 GT:2'), (2925, 'P:0 GT:2'), (1869, 'P:2 GT:0'), (2690, 'P:3 GT:0'), (1494, 'P:0 GT:1'), (4165, 'P:2 GT:0'), (4417, 'P:0 GT:3'), (2271, 'P:1 GT:0'), (399, 'P:0 GT:1'), (4561, 'P:0 GT:3'), (3804, 'P:0 GT:1'), (4329, 'P:1 GT:0'), (4586, 'P:3 GT:0'), (4483, 'P:1 GT:3'), (846, 'P:0 GT:1'), (2509, 'P:0 GT:1'), (3895, 'P:0 GT:1'), (789, 'P:0 GT:1'), (1794, 'P:0 GT:1'), (2806, 'P:0 GT:2'), (2399, 'P:0 GT:1'), (1052, 'P:1 GT:0'), (4501, 'P:0 GT:3'), (2840, 'P:1 GT:2'), (4555, 'P:2 GT:3'), (476, 'P:1 GT:0'), (70, 'P:1 GT:0'), (1732, 'P:1 GT:2'), (4142, 'P:1 GT:0'), (4239, 'P:1 GT:0'), (4226, 'P:1 GT:0'), (1932, 'P:1 GT:0'), (318, 'P:0 GT:1'), (1749, 'P:0 GT:2'), (2267, 'P:1 GT:0'), (3064, 'P:1 GT:0'), (2576, 'P:1 GT:0'), (416, 'P:3 GT:1'), (840, 'P:0 GT:1'), (2380, 'P:0 GT:1'), (2255, 'P:2 GT:0'), (1760, 'P:3 GT:2'), (2448, 'P:0 GT:1'), (2880, 'P:0 GT:2'), (4514, 'P:0 GT:3'), (2224, 'P:1 GT:0'), (335, 'P:2 GT:0'), (124, 'P:1 GT:0'), (2456, 'P:0 GT:1'), (1066, 'P:1 GT:0'), (4535, 'P:0 GT:3'), (2811, 'P:0 GT:2'), (2239, 'P:1 GT:0'), (1845, 'P:0 GT:1'), (3530, 'P:0 GT:1'), (1820, 'P:2 GT:1'), (3509, 'P:0 GT:1'), (4617, 'P:3 GT:0'), (2961, 'P:1 GT:0'), (314, 'P:0 GT:1'), (2108, 'P:1 GT:0'), (3181, 'P:3 GT:0'), (2871, 'P:3 GT:2'), (861, 'P:0 GT:1'), (2699, 'P:3 GT:0'), (2510, 'P:0 GT:1'), (2830, 'P:1 GT:2'), (912, 'P:0 GT:1'), (2417, 'P:0 GT:1'), (1908, 'P:1 GT:0'), (2662, 'P:1 GT:0'), (4111, 'P:3 GT:0'), (1448, 'P:1 GT:0'), (2200, 'P:1 GT:0'), (1858, 'P:0 GT:1'), (1837, 'P:0 GT:1'), (3300, 'P:1 GT:0'), (2400, 'P:0 GT:1'), (2742, 'P:1 GT:0'), (2688, 'P:3 GT:0'), (1673, 'P:1 GT:0'), (2652, 'P:1 GT:0'), (2902, 'P:1 GT:2'), (4201, 'P:1 GT:0'), (3489, 'P:3 GT:0'), (646, 'P:1 GT:0'), (3902, 'P:0 GT:1'), (864, 'P:0 GT:1'), (1649, 'P:0 GT:2'), (471, 'P:0 GT:1'), (2907, 'P:0 GT:2'), (3861, 'P:0 GT:1'), (1870, 'P:2 GT:0'), (4129, 'P:1 GT:0'), (1144, 'P:3 GT:0'), (2269, 'P:2 GT:0'), (2352, 'P:1 GT:0'), (2771, 'P:1 GT:0'), (3977, 'P:1 GT:0'), (3765, 'P:0 GT:1'), (288, 'P:0 GT:1'), (2198, 'P:1 GT:0'), (2384, 'P:0 GT:1'), (2521, 'P:0 GT:1'), (2834, 'P:0 GT:2'), (4511, 'P:0 GT:3'), (4218, 'P:2 GT:0'), (1812, 'P:0 GT:1'), (1683, 'P:1 GT:0'), (1582, 'P:1 GT:0'), (2016, 'P:1 GT:0'), (4599, 'P:2 GT:0'), (417, 'P:3 GT:1'), (3795, 'P:0 GT:1'), (1733, 'P:1 GT:2'), (2287, 'P:1 GT:0'), (3801, 'P:0 GT:1'), (2896, 'P:0 GT:2'), (3868, 'P:3 GT:1'), (3216, 'P:1 GT:0'), (3522, 'P:0 GT:1'), (687, 'P:3 GT:0'), (2504, 'P:2 GT:1'), (4464, 'P:0 GT:3'), (3945, 'P:1 GT:0'), (2233, 'P:1 GT:0'), (2592, 'P:1 GT:0'), (1744, 'P:0 GT:2'), (826, 'P:0 GT:1'), (736, 'P:0 GT:1'), (1770, 'P:0 GT:2'), (3167, 'P:1 GT:0'), (2040, 'P:1 GT:0'), (3932, 'P:0 GT:1'), (4477, 'P:0 GT:3'), (141, 'P:1 GT:0'), (3905, 'P:0 GT:1'), (2071, 'P:1 GT:0'), (1014, 'P:2 GT:0'), (894, 'P:3 GT:1'), (177, 'P:3 GT:0'), (2357, 'P:1 GT:0'), (1618, 'P:1 GT:0'), (627, 'P:1 GT:0'), (3998, 'P:1 GT:0'), (3564, 'P:1 GT:0'), (1700, 'P:1 GT:2'), (731, 'P:0 GT:1'), (787, 'P:0 GT:1'), (4420, 'P:0 GT:3'), (4451, 'P:2 GT:3'), (1726, 'P:1 GT:2'), (2954, 'P:1 GT:2'), (414, 'P:0 GT:1'), (3342, 'P:1 GT:0'), (1569, 'P:0 GT:1'), (3482, 'P:3 GT:0'), (383, 'P:1 GT:0'), (4036, 'P:1 GT:0'), (3194, 'P:1 GT:0'), (3771, 'P:0 GT:1'), (2930, 'P:0 GT:2'), (3802, 'P:0 GT:1'), (3715, 'P:1 GT:0'), (2819, 'P:1 GT:2'), (3504, 'P:0 GT:1'), (2172, 'P:1 GT:0'), (1847, 'P:0 GT:1'), (3824, 'P:0 GT:1'), (4476, 'P:2 GT:3'), (145, 'P:2 GT:0'), (3574, 'P:1 GT:0'), (6, 'P:3 GT:0'), (1639, 'P:1 GT:2'), (466, 'P:0 GT:1'), (3507, 'P:0 GT:1'), (3298, 'P:1 GT:0'), (3792, 'P:0 GT:1'), (3349, 'P:1 GT:0'), (425, 'P:0 GT:1'), (813, 'P:0 GT:1'), (4315, 'P:1 GT:0'), (1024, 'P:1 GT:0'), (1640, 'P:0 GT:2'), (336, 'P:1 GT:0'), (214, 'P:1 GT:0'), (1723, 'P:0 GT:2'), (882, 'P:0 GT:1'), (1488, 'P:0 GT:1'), (3866, 'P:0 GT:1'), (2813, 'P:1 GT:2'), (2820, 'P:0 GT:2'), (1816, 'P:0 GT:1'), (3081, 'P:1 GT:0'), (2173, 'P:1 GT:0'), (2515, 'P:0 GT:1'), (2627, 'P:1 GT:0'), (129, 'P:2 GT:0'), (1662, 'P:1 GT:0'), (1861, 'P:1 GT:0'), (4558, 'P:0 GT:3'), (2671, 'P:1 GT:0'), (1056, 'P:1 GT:0'), (722, 'P:0 GT:1'), (2194, 'P:1 GT:0'), (705, 'P:0 GT:1'), (2421, 'P:0 GT:1'), (2477, 'P:0 GT:1'), (4557, 'P:0 GT:3'), (2444, 'P:0 GT:1'), (3683, 'P:3 GT:0'), (4085, 'P:1 GT:0'), (169, 'P:1 GT:0'), (983, 'P:1 GT:0'), (1954, 'P:1 GT:0'), (2236, 'P:1 GT:0'), (3085, 'P:1 GT:0'), (2464, 'P:0 GT:1'), (4, 'P:1 GT:0'), (2912, 'P:0 GT:2'), (1427, 'P:1 GT:0'), (978, 'P:1 GT:0'), (4484, 'P:0 GT:3'), (2890, 'P:0 GT:2'), (4498, 'P:0 GT:3'), (398, 'P:0 GT:1'), (889, 'P:0 GT:1'), (4430, 'P:0 GT:3'), (2893, 'P:0 GT:2'), (2, 'P:1 GT:0'), (2940, 'P:0 GT:2'), (2789, 'P:0 GT:2'), (1555, 'P:0 GT:1'), (1705, 'P:1 GT:2'), (3851, 'P:0 GT:1'), (1876, 'P:1 GT:0'), (4124, 'P:1 GT:0'), (4572, 'P:0 GT:3'), (3886, 'P:0 GT:1'), (2796, 'P:0 GT:2'), (1727, 'P:0 GT:2'), (760, 'P:0 GT:1'), (108, 'P:1 GT:0'), (3863, 'P:0 GT:1'), (672, 'P:1 GT:0'), (3562, 'P:1 GT:0'), (237, 'P:2 GT:0'), (2808, 'P:0 GT:2'), (1475, 'P:2 GT:1'), (2393, 'P:0 GT:1'), (1740, 'P:0 GT:2'), (1728, 'P:0 GT:2'), (741, 'P:0 GT:1'), (798, 'P:0 GT:1'), (4541, 'P:0 GT:3'), (2918, 'P:0 GT:2'), (1121, 'P:2 GT:0'), (2334, 'P:1 GT:0'), (3317, 'P:1 GT:0'), (1430, 'P:1 GT:0'), (2248, 'P:1 GT:0'), (3019, 'P:1 GT:0'), (786, 'P:0 GT:1'), (4438, 'P:1 GT:3'), (779, 'P:0 GT:1'), (2557, 'P:1 GT:0'), (4565, 'P:0 GT:3'), (744, 'P:0 GT:1'), (771, 'P:0 GT:1'), (1774, 'P:1 GT:2'), (3882, 'P:0 GT:1'), (2437, 'P:0 GT:1'), (1457, 'P:1 GT:0'), (3483, 'P:2 GT:0'), (1036, 'P:1 GT:0'), (3149, 'P:1 GT:0'), (9, 'P:3 GT:0'), (1713, 'P:0 GT:2'), (2465, 'P:0 GT:1'), (121, 'P:1 GT:0'), (3078, 'P:1 GT:0'), (4068, 'P:3 GT:0'), (1701, 'P:0 GT:2'), (2265, 'P:1 GT:0'), (4010, 'P:1 GT:0'), (3907, 'P:0 GT:1'), (738, 'P:0 GT:1'), (1699, 'P:1 GT:2'), (7, 'P:1 GT:0'), (2676, 'P:1 GT:0'), (1516, 'P:0 GT:1'), (933, 'P:0 GT:1'), (4522, 'P:0 GT:3'), (841, 'P:0 GT:1'), (2666, 'P:1 GT:0'), (2945, 'P:0 GT:2'), (2178, 'P:1 GT:0'), (1843, 'P:2 GT:1'), (2788, 'P:0 GT:2'), (3876, 'P:0 GT:1'), (296, 'P:0 GT:1'), (724, 'P:0 GT:1'), (4543, 'P:0 GT:3'), (1596, 'P:3 GT:0'), (4291, 'P:2 GT:0'), (4393, 'P:1 GT:0'), (2408, 'P:0 GT:1'), (2906, 'P:0 GT:2'), (2344, 'P:1 GT:0'), (4518, 'P:0 GT:3'), (3940, 'P:2 GT:0'), (3849, 'P:0 GT:1'), (925, 'P:0 GT:1'), (2801, 'P:0 GT:2'), (3859, 'P:0 GT:1'), (3523, 'P:0 GT:1'), (2900, 'P:1 GT:2'), (1958, 'P:3 GT:0'), (696, 'P:0 GT:1'), (2790, 'P:0 GT:2'), (3143, 'P:1 GT:0'), (2791, 'P:1 GT:2'), (759, 'P:0 GT:1'), (4099, 'P:1 GT:0'), (2460, 'P:0 GT:1'), (1629, 'P:1 GT:0'), (4030, 'P:1 GT:0'), (879, 'P:0 GT:1'), (2003, 'P:1 GT:0'), (2933, 'P:1 GT:2'), (2575, 'P:1 GT:0'), (2486, 'P:2 GT:1'), (851, 'P:0 GT:1'), (681, 'P:2 GT:0'), (3782, 'P:0 GT:1'), (2032, 'P:1 GT:0'), (3925, 'P:0 GT:1'), (2839, 'P:0 GT:2'), (4121, 'P:1 GT:0'), (2390, 'P:0 GT:1'), (2923, 'P:0 GT:2'), (901, 'P:0 GT:1'), (4418, 'P:1 GT:3'), (4584, 'P:2 GT:0'), (1320, 'P:2 GT:0'), (800, 'P:0 GT:1'), (1779, 'P:0 GT:2'), (3841, 'P:0 GT:1'), (1524, 'P:0 GT:1'), (2815, 'P:0 GT:2'), (2450, 'P:0 GT:1'), (4467, 'P:0 GT:3'), (2955, 'P:0 GT:2'), (2257, 'P:1 GT:0'), (2039, 'P:1 GT:0'), (4528, 'P:0 GT:3'), (2309, 'P:1 GT:0'), (1513, 'P:0 GT:1'), (683, 'P:1 GT:0'), (1985, 'P:1 GT:0'), (884, 'P:0 GT:1'), (730, 'P:0 GT:1'), (1863, 'P:1 GT:0'), (2256, 'P:1 GT:0'), (3500, 'P:0 GT:1'), (1666, 'P:1 GT:0'), (3228, 'P:1 GT:0'), (2201, 'P:1 GT:0'), (3800, 'P:0 GT:1'), (1426, 'P:1 GT:0'), (1594, 'P:1 GT:0'), (3215, 'P:1 GT:0'), (4314, 'P:1 GT:0'), (2841, 'P:1 GT:2'), (1093, 'P:1 GT:0'), (2441, 'P:0 GT:1'), (3784, 'P:0 GT:1'), (2203, 'P:1 GT:0'), (3527, 'P:0 GT:1'), (3517, 'P:0 GT:1'), (249, 'P:2 GT:0'), (2184, 'P:1 GT:0'), (2285, 'P:1 GT:0'), (1298, 'P:2 GT:0'), (1512, 'P:0 GT:1'), (4352, 'P:3 GT:0'), (3935, 'P:0 GT:1'), (1354, 'P:1 GT:0'), (3772, 'P:0 GT:1'), (102, 'P:1 GT:0'), (2952, 'P:0 GT:2'), (923, 'P:0 GT:1'), (3219, 'P:1 GT:0'), (2235, 'P:1 GT:0'), (4459, 'P:0 GT:3'), (4577, 'P:0 GT:3'), (4482, 'P:1 GT:3'), (3921, 'P:0 GT:1'), (3341, 'P:1 GT:0'), (3819, 'P:0 GT:1'), (294, 'P:0 GT:1'), (4436, 'P:0 GT:3'), (3177, 'P:1 GT:0'), (386, 'P:0 GT:1'), (2603, 'P:1 GT:0'), (2877, 'P:1 GT:2'), (2036, 'P:1 GT:0'), (2905, 'P:1 GT:2'), (1664, 'P:1 GT:0'), (2663, 'P:1 GT:0'), (1535, 'P:0 GT:1'), (2175, 'P:2 GT:0'), (1921, 'P:2 GT:0'), (1531, 'P:0 GT:1'), (1008, 'P:1 GT:0'), (1542, 'P:0 GT:1'), (1506, 'P:0 GT:1'), (3736, 'P:1 GT:0'), (31, 'P:1 GT:0'), (1718, 'P:0 GT:2'), (4473, 'P:1 GT:3'), (3164, 'P:1 GT:0'), (3179, 'P:1 GT:0'), (1522, 'P:0 GT:1'), (1777, 'P:0 GT:2'), (875, 'P:0 GT:1'), (703, 'P:0 GT:1'), (4166, 'P:1 GT:0'), (1564, 'P:0 GT:1'), (3147, 'P:1 GT:0'), (821, 'P:0 GT:1'), (2556, 'P:1 GT:0'), (1644, 'P:0 GT:2'), (96, 'P:1 GT:0'), (671, 'P:1 GT:0'), (4145, 'P:1 GT:0'), (27, 'P:1 GT:0'), (4550, 'P:2 GT:3'), (87, 'P:1 GT:0'), (136, 'P:2 GT:0'), (4432, 'P:0 GT:3'), (16, 'P:1 GT:0'), (776, 'P:0 GT:1'), (1239, 'P:1 GT:0'), (3558, 'P:1 GT:0'), (4392, 'P:1 GT:0'), (4169, 'P:1 GT:0'), (850, 'P:0 GT:1'), (830, 'P:0 GT:1'), (1703, 'P:3 GT:2'), (2451, 'P:0 GT:1'), (3774, 'P:0 GT:1'), (713, 'P:0 GT:1'), (3202, 'P:1 GT:0'), (1527, 'P:0 GT:1'), (1853, 'P:2 GT:1'), (1792, 'P:0 GT:1'), (2920, 'P:0 GT:2'), (4429, 'P:0 GT:3'), (4439, 'P:0 GT:3'), (2369, 'P:0 GT:1'), (2499, 'P:0 GT:1'), (3614, 'P:1 GT:0'), (3153, 'P:2 GT:0'), (942, 'P:1 GT:0'), (746, 'P:0 GT:1'), (4540, 'P:0 GT:3'), (1588, 'P:1 GT:0'), (2605, 'P:1 GT:0'), (2947, 'P:0 GT:2'), (204, 'P:1 GT:0'), (163, 'P:1 GT:0'), (3874, 'P:0 GT:1'), (1805, 'P:0 GT:1'), (1763, 'P:0 GT:2'), (3883, 'P:0 GT:1'), (3893, 'P:0 GT:1'), (1761, 'P:0 GT:2'), (2851, 'P:0 GT:2'), (4294, 'P:3 GT:0'), (464, 'P:0 GT:1'), (3843, 'P:0 GT:1'), (4387, 'P:1 GT:0'), (3887, 'P:0 GT:1'), (1782, 'P:0 GT:2'), (400, 'P:0 GT:1'), (4444, 'P:0 GT:3'), (212, 'P:1 GT:0'), (2049, 'P:1 GT:0'), (2908, 'P:0 GT:2'), (582, 'P:2 GT:0'), (2252, 'P:1 GT:0'), (2989, 'P:1 GT:0'), (4271, 'P:1 GT:0'), (1559, 'P:0 GT:1'), (1635, 'P:1 GT:0'), (2263, 'P:1 GT:0'), (1758, 'P:1 GT:2'), (176, 'P:1 GT:0'), (2199, 'P:1 GT:0'), (726, 'P:0 GT:1'), (4529, 'P:0 GT:3'), (1633, 'P:1 GT:0'), (4186, 'P:1 GT:0'), (2901, 'P:0 GT:2'), (2814, 'P:0 GT:2'), (3579, 'P:1 GT:0'), (2472, 'P:0 GT:1'), (1492, 'P:0 GT:1'), (754, 'P:0 GT:1'), (458, 'P:0 GT:1'), (2347, 'P:1 GT:0'), (4497, 'P:0 GT:3'), (4379, 'P:1 GT:0'), (208, 'P:2 GT:0'), (2418, 'P:0 GT:1'), (304, 'P:0 GT:1'), (1076, 'P:1 GT:0'), (869, 'P:0 GT:1'), (3205, 'P:1 GT:0'), (1467, 'P:1 GT:0'), (2333, 'P:2 GT:0'), (3952, 'P:1 GT:0'), (3786, 'P:0 GT:1'), (928, 'P:0 GT:1'), (3892, 'P:0 GT:1'), (2861, 'P:0 GT:2'), (2643, 'P:2 GT:0'), (2254, 'P:1 GT:0'), (2838, 'P:3 GT:2'), (1607, 'P:1 GT:0'), (4342, 'P:1 GT:0'), (831, 'P:0 GT:1'), (259, 'P:2 GT:0'), (1711, 'P:0 GT:2'), (4495, 'P:0 GT:3'), (728, 'P:0 GT:1'), (2364, 'P:0 GT:1'), (2756, 'P:1 GT:0'), (791, 'P:0 GT:1'), (2631, 'P:1 GT:0'), (2186, 'P:2 GT:0'), (2882, 'P:1 GT:2'), (764, 'P:0 GT:1'), (1854, 'P:2 GT:1'), (2946, 'P:0 GT:2'), (1672, 'P:1 GT:0'), (2566, 'P:1 GT:0'), (1461, 'P:1 GT:0'), (775, 'P:0 GT:1'), (1759, 'P:1 GT:2'), (718, 'P:0 GT:1'), (1082, 'P:3 GT:0'), (3858, 'P:0 GT:1'), (4450, 'P:0 GT:3'), (2494, 'P:0 GT:1'), (873, 'P:0 GT:1'), (428, 'P:0 GT:1'), (2443, 'P:0 GT:1'), (1097, 'P:1 GT:0'), (2262, 'P:1 GT:0'), (817, 'P:0 GT:1'), (3073, 'P:1 GT:0'), (4435, 'P:1 GT:3'), (3586, 'P:1 GT:0'), (1561, 'P:0 GT:1'), (406, 'P:0 GT:1'), (2395, 'P:0 GT:1'), (15, 'P:1 GT:0'), (1483, 'P:3 GT:1'), (2904, 'P:3 GT:2'), (1574, 'P:2 GT:0'), (4552, 'P:0 GT:3'), (1668, 'P:1 GT:0'), (4234, 'P:3 GT:0'), (3200, 'P:1 GT:0'), (339, 'P:2 GT:0'), (2383, 'P:2 GT:1'), (2601, 'P:1 GT:0'), (4015, 'P:1 GT:0'), (2874, 'P:0 GT:2'), (2204, 'P:1 GT:0'), (2885, 'P:0 GT:2'), (2519, 'P:2 GT:1'), (2382, 'P:0 GT:1'), (2004, 'P:1 GT:0'), (3051, 'P:1 GT:0'), (2842, 'P:3 GT:2'), (2648, 'P:1 GT:0'), (941, 'P:0 GT:1'), (3487, 'P:3 GT:0'), (845, 'P:0 GT:1'), (866, 'P:0 GT:1'), (4343, 'P:1 GT:0'), (2104, 'P:2 GT:0'), (3835, 'P:0 GT:1'), (4471, 'P:0 GT:3'), (765, 'P:0 GT:1'), (662, 'P:1 GT:0'), (4582, 'P:0 GT:3'), (4449, 'P:2 GT:3'), (3767, 'P:0 GT:1'), (2447, 'P:3 GT:1'), (4163, 'P:2 GT:0'), (3644, 'P:1 GT:0'), (544, 'P:1 GT:0'), (4044, 'P:1 GT:0'), (3652, 'P:1 GT:0'), (2739, 'P:3 GT:0'), (2911, 'P:0 GT:2'), (2794, 'P:0 GT:2'), (940, 'P:0 GT:1'), (2824, 'P:0 GT:2'), (3903, 'P:0 GT:1'), (4479, 'P:0 GT:3'), (3268, 'P:1 GT:0'), (3799, 'P:0 GT:1'), (898, 'P:0 GT:1'), (733, 'P:0 GT:1'), (1717, 'P:0 GT:2'), (2927, 'P:0 GT:2'), (2219, 'P:1 GT:0'), (1851, 'P:0 GT:1'), (2146, 'P:2 GT:0'), (1773, 'P:3 GT:2'), (4022, 'P:1 GT:0'), (2061, 'P:1 GT:0'), (2482, 'P:0 GT:1'), (2274, 'P:1 GT:0'), (1864, 'P:2 GT:0'), (2776, 'P:3 GT:0'), (311, 'P:0 GT:1'), (2857, 'P:0 GT:2'), (2459, 'P:0 GT:1'), (209, 'P:2 GT:0'), (2232, 'P:1 GT:0'), (2237, 'P:1 GT:0'), (2767, 'P:3 GT:0'), (2289, 'P:1 GT:0'), (2366, 'P:0 GT:1'), (795, 'P:0 GT:1'), (1829, 'P:0 GT:1'), (3533, 'P:0 GT:1'), (991, 'P:1 GT:0'), (2717, 'P:1 GT:0'), (1518, 'P:0 GT:1'), (2868, 'P:0 GT:2'), (4422, 'P:0 GT:3'), (2240, 'P:1 GT:0'), (4424, 'P:0 GT:3'), (908, 'P:0 GT:1'), (756, 'P:0 GT:1'), (748, 'P:0 GT:1'), (2853, 'P:1 GT:2'), (1099, 'P:1 GT:0'), (2067, 'P:1 GT:0'), (1543, 'P:0 GT:1'), (178, 'P:2 GT:0'), (4130, 'P:1 GT:0'), (4248, 'P:3 GT:0'), (3809, 'P:0 GT:1'), (2457, 'P:0 GT:1'), (2498, 'P:2 GT:1'), (3119, 'P:1 GT:0'), (293, 'P:0 GT:1'), (4171, 'P:1 GT:0'), (757, 'P:0 GT:1'), (3191, 'P:1 GT:0'), (4031, 'P:1 GT:0'), (1551, 'P:3 GT:1'), (1407, 'P:1 GT:0'), (4358, 'P:1 GT:0'), (363, 'P:1 GT:0'), (2734, 'P:3 GT:0'), (911, 'P:0 GT:1'), (2484, 'P:2 GT:1'), (2544, 'P:1 GT:0'), (3002, 'P:1 GT:0'), (578, 'P:2 GT:0'), (1753, 'P:1 GT:2'), (1047, 'P:2 GT:0'), (4493, 'P:2 GT:3'), (3207, 'P:2 GT:0'), (4375, 'P:1 GT:0'), (151, 'P:1 GT:0'), (3828, 'P:0 GT:1'), (4566, 'P:0 GT:3'), (4377, 'P:3 GT:0'), (4488, 'P:0 GT:3'), (301, 'P:0 GT:1'), (4475, 'P:2 GT:3'), (403, 'P:0 GT:1'), (3503, 'P:0 GT:1'), (4007, 'P:2 GT:0'), (822, 'P:0 GT:1'), (1745, 'P:0 GT:2'), (3816, 'P:0 GT:1'), (2177, 'P:1 GT:0'), (418, 'P:3 GT:1'), (3788, 'P:0 GT:1'), (4092, 'P:1 GT:0'), (2821, 'P:0 GT:2'), (1871, 'P:2 GT:0'), (4470, 'P:0 GT:3'), (436, 'P:2 GT:1'), (3881, 'P:0 GT:1'), (3630, 'P:1 GT:0'), (3133, 'P:1 GT:0'), (799, 'P:0 GT:1'), (3908, 'P:0 GT:1'), (4185, 'P:1 GT:0'), (130, 'P:3 GT:0'), (3272, 'P:1 GT:0'), (2517, 'P:2 GT:1'), (1422, 'P:1 GT:0'), (322, 'P:0 GT:1'), (142, 'P:2 GT:0'), (981, 'P:1 GT:0'), (938, 'P:0 GT:1'), (81, 'P:3 GT:0'), (2888, 'P:1 GT:2'), (3186, 'P:1 GT:0'), (408, 'P:0 GT:1'), (963, 'P:1 GT:0'), (1657, 'P:1 GT:2'), (1734, 'P:0 GT:2'), (710, 'P:0 GT:1'), (664, 'P:1 GT:0'), (3623, 'P:1 GT:0'), (2617, 'P:1 GT:0'), (3890, 'P:0 GT:1'), (2956, 'P:0 GT:2'), (2849, 'P:1 GT:2'), (437, 'P:0 GT:1'), (2463, 'P:2 GT:1'), (2292, 'P:1 GT:0'), (3926, 'P:0 GT:1'), (4454, 'P:0 GT:3'), (313, 'P:0 GT:1'), (2296, 'P:1 GT:0'), (3796, 'P:0 GT:1'), (4620, 'P:1 GT:0'), (364, 'P:1 GT:0'), (4446, 'P:0 GT:3'), (3327, 'P:1 GT:0'), (2432, 'P:0 GT:1'), (1778, 'P:0 GT:2'), (3822, 'P:2 GT:1'), (2775, 'P:1 GT:0'), (2490, 'P:0 GT:1'), (2379, 'P:0 GT:1'), (829, 'P:0 GT:1'), (930, 'P:0 GT:1'), (4213, 'P:3 GT:0'), (3793, 'P:0 GT:1'), (793, 'P:0 GT:1'), (1616, 'P:1 GT:0'), (2892, 'P:1 GT:2'), (3528, 'P:0 GT:1'), (2508, 'P:0 GT:1'), (3813, 'P:0 GT:1'), (453, 'P:0 GT:1'), (2511, 'P:0 GT:1'), (2339, 'P:1 GT:0'), (2747, 'P:3 GT:0'), (2060, 'P:1 GT:0'), (4167, 'P:2 GT:0'), (2207, 'P:1 GT:0'), (3310, 'P:1 GT:0'), (792, 'P:0 GT:1'), (1378, 'P:1 GT:0'), (405, 'P:0 GT:1'), (2404, 'P:0 GT:1'), (709, 'P:0 GT:1'), (3912, 'P:0 GT:1'), (20, 'P:1 GT:0'), (1031, 'P:1 GT:0'), (2013, 'P:1 GT:0'), (3871, 'P:0 GT:1'), (3857, 'P:0 GT:1'), (4175, 'P:1 GT:0'), (4516, 'P:0 GT:3'), (936, 'P:0 GT:1'), (929, 'P:0 GT:1'), (415, 'P:0 GT:1'), (174, 'P:2 GT:0'), (2288, 'P:1 GT:0'), (769, 'P:0 GT:1'), (3852, 'P:0 GT:1'), (3199, 'P:3 GT:0'), (1643, 'P:0 GT:2'), (1001, 'P:1 GT:0'), (4003, 'P:1 GT:0'), (4160, 'P:2 GT:0'), (2440, 'P:2 GT:1'), (3878, 'P:0 GT:1'), (904, 'P:0 GT:1'), (4480, 'P:0 GT:3'), (2420, 'P:0 GT:1'), (880, 'P:0 GT:1'), (30, 'P:1 GT:0'), (2469, 'P:0 GT:1'), (2491, 'P:0 GT:1'), (1914, 'P:1 GT:0'), (4350, 'P:1 GT:0'), (4525, 'P:1 GT:3'), (778, 'P:0 GT:1'), (4414, 'P:1 GT:0'), (310, 'P:0 GT:1'), (2455, 'P:2 GT:1'), (1479, 'P:0 GT:1'), (4423, 'P:0 GT:3'), (3274, 'P:1 GT:0'), (3549, 'P:1 GT:0'), (3550, 'P:1 GT:0'), (974, 'P:1 GT:0'), (2932, 'P:0 GT:2'), (3839, 'P:0 GT:1'), (4087, 'P:1 GT:0'), (2230, 'P:1 GT:0'), (2161, 'P:1 GT:0'), (3790, 'P:0 GT:1'), (2189, 'P:1 GT:0'), (1694, 'P:1 GT:0'), (1818, 'P:0 GT:1'), (389, 'P:0 GT:1'), (4164, 'P:1 GT:0'), (2944, 'P:0 GT:2'), (3501, 'P:0 GT:1'), (4096, 'P:1 GT:0'), (3906, 'P:0 GT:1'), (4521, 'P:0 GT:3'), (685, 'P:1 GT:0'), (1495, 'P:0 GT:1'), (4496, 'P:0 GT:3'), (4562, 'P:0 GT:3'), (2941, 'P:0 GT:2'), (2926, 'P:3 GT:2'), (1652, 'P:0 GT:2'), (1898, 'P:2 GT:0'), (3556, 'P:1 GT:0'), (4442, 'P:1 GT:3'), (3230, 'P:1 GT:0'), (3825, 'P:0 GT:1'), (24, 'P:1 GT:0'), (1620, 'P:1 GT:0'), (4437, 'P:0 GT:3'), (2854, 'P:0 GT:2'), (2012, 'P:1 GT:0'), (2992, 'P:1 GT:0'), (2283, 'P:1 GT:0'), (3884, 'P:0 GT:1'), (221, 'P:1 GT:0'), (932, 'P:0 GT:1'), (2270, 'P:1 GT:0'), (4368, 'P:1 GT:0'), (1128, 'P:3 GT:0'), (2412, 'P:0 GT:1'), (4075, 'P:1 GT:0'), (828, 'P:0 GT:1'), (762, 'P:0 GT:1'), (918, 'P:0 GT:1'), (2488, 'P:0 GT:1'), (2942, 'P:0 GT:2'), (2376, 'P:0 GT:1'), (422, 'P:0 GT:1'), (4431, 'P:0 GT:3'), (309, 'P:0 GT:1'), (1642, 'P:0 GT:2'), (2638, 'P:1 GT:0'), (3848, 'P:0 GT:1'), (2485, 'P:0 GT:1'), (2889, 'P:0 GT:2'), (2878, 'P:0 GT:2'), (1406, 'P:1 GT:0'), (2503, 'P:0 GT:1'), (2023, 'P:1 GT:0'), (2454, 'P:0 GT:1'), (924, 'P:0 GT:1'), (810, 'P:0 GT:1'), (3808, 'P:0 GT:1'), (2687, 'P:3 GT:0'), (290, 'P:0 GT:1'), (1038, 'P:1 GT:0'), (3608, 'P:1 GT:0'), (1480, 'P:0 GT:1'), (2843, 'P:3 GT:2'), (3345, 'P:3 GT:0'), (721, 'P:0 GT:1'), (1884, 'P:1 GT:0'), (1722, 'P:0 GT:2'), (84, 'P:3 GT:0'), (3899, 'P:3 GT:1'), (1032, 'P:1 GT:0'), (495, 'P:1 GT:0'), (1650, 'P:0 GT:2'), (3794, 'P:0 GT:1'), (3140, 'P:1 GT:0'), (253, 'P:1 GT:0'), (3011, 'P:1 GT:0'), (1048, 'P:1 GT:0'), (1754, 'P:1 GT:2'), (1795, 'P:0 GT:1'), (1565, 'P:0 GT:1'), (4548, 'P:0 GT:3'), (3853, 'P:0 GT:1'), (1597, 'P:1 GT:0'), (3626, 'P:1 GT:0'), (4512, 'P:0 GT:3'), (3567, 'P:1 GT:0'), (984, 'P:1 GT:0'), (4391, 'P:1 GT:0'), (2619, 'P:1 GT:0'), (1572, 'P:0 GT:1'), (4035, 'P:1 GT:0'), (2372, 'P:0 GT:1'), (2514, 'P:0 GT:1'), (1507, 'P:0 GT:1'), (758, 'P:0 GT:1'), (1831, 'P:0 GT:1'), (2378, 'P:0 GT:1'), (3834, 'P:0 GT:1'), (2249, 'P:1 GT:0'), (727, 'P:0 GT:1'), (3524, 'P:0 GT:1'), (1768, 'P:0 GT:2'), (2827, 'P:1 GT:2'), (4408, 'P:1 GT:0'), (3183, 'P:1 GT:0'), (2668, 'P:1 GT:0'), (837, 'P:0 GT:1'), (3359, 'P:1 GT:0'), (3497, 'P:0 GT:1'), (4517, 'P:1 GT:3'), (1399, 'P:3 GT:0'), (3845, 'P:0 GT:1'), (1879, 'P:2 GT:0'), (4563, 'P:0 GT:3'), (2277, 'P:1 GT:0'), (236, 'P:1 GT:0'), (2431, 'P:0 GT:1'), (4401, 'P:1 GT:0'), (3338, 'P:1 GT:0'), (3814, 'P:0 GT:1'), (3030, 'P:1 GT:0'), (4538, 'P:0 GT:3'), (1840, 'P:0 GT:1'), (986, 'P:1 GT:0'), (4409, 'P:1 GT:0'), (1693, 'P:1 GT:0'), (1359, 'P:1 GT:0'), (3764, 'P:0 GT:1'), (1862, 'P:1 GT:0'), (2549, 'P:1 GT:0'), (1050, 'P:1 GT:0'), (3785, 'P:0 GT:1'), (2625, 'P:2 GT:0'), (1856, 'P:0 GT:1'), (2606, 'P:1 GT:0'), (2293, 'P:1 GT:0'), (3797, 'P:0 GT:1'), (3783, 'P:0 GT:1'), (2826, 'P:0 GT:2'), (892, 'P:0 GT:1'), (847, 'P:0 GT:1'), (2281, 'P:3 GT:0'), (2322, 'P:2 GT:0'), (2389, 'P:0 GT:1'), (4325, 'P:2 GT:0'), (717, 'P:0 GT:1'), (443, 'P:0 GT:1'), (4580, 'P:0 GT:3'), (2848, 'P:0 GT:2'), (1073, 'P:1 GT:0'), (891, 'P:0 GT:1'), (4456, 'P:1 GT:3'), (4527, 'P:0 GT:3'), (1865, 'P:2 GT:0'), (285, 'P:0 GT:1'), (4509, 'P:0 GT:3'), (74, 'P:1 GT:0'), (1755, 'P:1 GT:2'), (745, 'P:0 GT:1'), (2919, 'P:0 GT:2'), (4481, 'P:2 GT:3'), (3842, 'P:0 GT:1'), (3632, 'P:1 GT:0'), (3631, 'P:1 GT:0'), (2411, 'P:0 GT:1'), (1838, 'P:0 GT:1'), (1545, 'P:0 GT:1'), (3910, 'P:3 GT:1'), (4381, 'P:1 GT:0'), (823, 'P:2 GT:1'), (1411, 'P:1 GT:0'), (2859, 'P:0 GT:2'), (768, 'P:0 GT:1'), (1702, 'P:0 GT:2'), (962, 'P:1 GT:0'), (2568, 'P:1 GT:0'), (4466, 'P:0 GT:3'), (2731, 'P:1 GT:0'), (4447, 'P:0 GT:3'), (1739, 'P:0 GT:2'), (4564, 'P:0 GT:3'), (707, 'P:0 GT:1'), (4573, 'P:0 GT:3'), (917, 'P:0 GT:1'), (3934, 'P:0 GT:1'), (118, 'P:2 GT:0'), (2845, 'P:0 GT:2'), (832, 'P:0 GT:1'), (2512, 'P:0 GT:1'), (818, 'P:2 GT:1'), (1690, 'P:1 GT:0'), (2522, 'P:2 GT:1'), (2759, 'P:1 GT:0'), (3894, 'P:0 GT:1'), (4533, 'P:0 GT:3'), (80, 'P:3 GT:0'), (3238, 'P:1 GT:0'), (407, 'P:2 GT:1'), (2833, 'P:0 GT:2'), (2375, 'P:3 GT:1'), (3640, 'P:1 GT:0'), (2371, 'P:0 GT:1'), (921, 'P:2 GT:1'), (947, 'P:1 GT:0'), (3854, 'P:0 GT:1'), (1687, 'P:1 GT:0'), (1741, 'P:1 GT:2'), (1365, 'P:3 GT:0'), (3898, 'P:0 GT:1'), (4576, 'P:0 GT:3'), (915, 'P:0 GT:1'), (4531, 'P:0 GT:3'), (1951, 'P:1 GT:0'), (3918, 'P:0 GT:1'), (1844, 'P:0 GT:1'), (807, 'P:0 GT:1'), (3252, 'P:1 GT:0'), (1658, 'P:0 GT:2'), (2481, 'P:2 GT:1'), (1060, 'P:1 GT:0'), (815, 'P:0 GT:1'), (1577, 'P:1 GT:0'), (805, 'P:0 GT:1'), (2401, 'P:0 GT:1'), (3004, 'P:1 GT:0'), (2844, 'P:0 GT:2'), (4465, 'P:1 GT:3'), (2453, 'P:0 GT:1'), (3806, 'P:0 GT:1'), (3511, 'P:0 GT:1'), (1544, 'P:0 GT:1'), (1743, 'P:0 GT:2'), (83, 'P:1 GT:0'), (3904, 'P:0 GT:1'), (2937, 'P:0 GT:2'), (1798, 'P:0 GT:1'), (4402, 'P:1 GT:0'), (2862, 'P:0 GT:2'), (2818, 'P:0 GT:2'), (2867, 'P:0 GT:2'), (2689, 'P:1 GT:0'), (859, 'P:2 GT:1'), (3915, 'P:0 GT:1'), (1830, 'P:0 GT:1'), (1736, 'P:0 GT:2'), (1424, 'P:1 GT:0'), (773, 'P:0 GT:1'), (4510, 'P:0 GT:3'), (711, 'P:0 GT:1'), (64, 'P:1 GT:0'), (97, 'P:1 GT:0'), (412, 'P:0 GT:1'), (3529, 'P:0 GT:1'), (1725, 'P:1 GT:2'), (927, 'P:0 GT:1'), (2020, 'P:1 GT:0'), (808, 'P:0 GT:1'), (2922, 'P:1 GT:2'), (2275, 'P:1 GT:0'), (995, 'P:1 GT:0'), (2895, 'P:0 GT:2'), (1961, 'P:1 GT:0'), (2429, 'P:0 GT:1'), (2475, 'P:0 GT:1'), (856, 'P:0 GT:1'), (2763, 'P:1 GT:0'), (3502, 'P:0 GT:1'), (4362, 'P:1 GT:0'), (2426, 'P:2 GT:1'), (969, 'P:1 GT:0'), (2001, 'P:1 GT:0'), (404, 'P:0 GT:1'), (2442, 'P:0 GT:1'), (4017, 'P:1 GT:0'), (2185, 'P:1 GT:0'), (751, 'P:0 GT:1'), (811, 'P:0 GT:1'), (3837, 'P:0 GT:1'), (3879, 'P:0 GT:1'), (107, 'P:1 GT:0'), (2435, 'P:0 GT:1'), (3862, 'P:0 GT:1'), (1931, 'P:3 GT:0'), (4037, 'P:1 GT:0'), (914, 'P:0 GT:1'), (1463, 'P:1 GT:0'), (3805, 'P:0 GT:1'), (41, 'P:1 GT:0'), (3711, 'P:1 GT:0'), (1647, 'P:0 GT:2'), (90, 'P:1 GT:0'), (1852, 'P:0 GT:1'), (3826, 'P:0 GT:1'), (450, 'P:0 GT:1'), (4441, 'P:0 GT:3'), (21, 'P:1 GT:0'), (2462, 'P:0 GT:1'), (4426, 'P:0 GT:3'), (4428, 'P:0 GT:3'), (1888, 'P:1 GT:0'), (1517, 'P:0 GT:1'), (377, 'P:1 GT:0'), (1549, 'P:0 GT:1'), (902, 'P:0 GT:1'), (112, 'P:3 GT:0'), (4553, 'P:0 GT:3'), (704, 'P:0 GT:1'), (720, 'P:0 GT:1'), (4579, 'P:0 GT:3'), (777, 'P:0 GT:1'), (3870, 'P:0 GT:1'), (814, 'P:0 GT:1'), (1730, 'P:1 GT:2'), (1573, 'P:0 GT:1'), (2856, 'P:0 GT:2'), (3761, 'P:3 GT:1'), (104, 'P:1 GT:0'), (836, 'P:0 GT:1'), (2192, 'P:1 GT:0'), (4386, 'P:1 GT:0'), (3951, 'P:1 GT:0'), (883, 'P:0 GT:1'), (33, 'P:1 GT:0'), (1022, 'P:1 GT:0'), (782, 'P:0 GT:1'), (2799, 'P:0 GT:2'), (3430, 'P:2 GT:0'), (410, 'P:0 GT:1'), (1554, 'P:0 GT:1'), (3536, 'P:1 GT:0'), (1437, 'P:1 GT:0'), (4353, 'P:1 GT:0'), (747, 'P:0 GT:1'), (292, 'P:0 GT:1'), (421, 'P:0 GT:1'), (198, 'P:1 GT:0'), (2367, 'P:0 GT:1'), (2258, 'P:1 GT:0'), (2280, 'P:1 GT:0'), (2810, 'P:0 GT:2'), (1587, 'P:1 GT:0'), (1000, 'P:1 GT:0'), (2445, 'P:0 GT:1'), (452, 'P:0 GT:1'), (3703, 'P:1 GT:0'), (2951, 'P:1 GT:2'), (3846, 'P:0 GT:1'), (3254, 'P:1 GT:0'), (203, 'P:2 GT:0'), (868, 'P:0 GT:1'), (2260, 'P:1 GT:0'), (1748, 'P:1 GT:2'), (1636, 'P:1 GT:0'), (1556, 'P:0 GT:1'), (4316, 'P:3 GT:0'), (2385, 'P:0 GT:1'), (2468, 'P:0 GT:1'), (842, 'P:0 GT:1'), (3130, 'P:3 GT:0'), (4571, 'P:2 GT:3'), (907, 'P:0 GT:1'), (725, 'P:3 GT:1'), (1709, 'P:0 GT:2'), (3916, 'P:0 GT:1'), (2847, 'P:0 GT:2'), (1769, 'P:0 GT:2'), (1444, 'P:1 GT:0'), (890, 'P:0 GT:1'), (772, 'P:0 GT:1'), (4376, 'P:1 GT:0'), (824, 'P:0 GT:1'), (1747, 'P:1 GT:2'), (1815, 'P:0 GT:1'), (1403, 'P:2 GT:0'), (3537, 'P:1 GT:0'), (1923, 'P:1 GT:0'), (2850, 'P:0 GT:2'), (935, 'P:0 GT:1'), (159, 'P:1 GT:0'), (4425, 'P:0 GT:3'), (820, 'P:0 GT:1'), (447, 'P:0 GT:1'), (2917, 'P:0 GT:2'), (2174, 'P:1 GT:0'), (2934, 'P:0 GT:2'), (1875, 'P:2 GT:0'), (1663, 'P:1 GT:0'), (1118, 'P:3 GT:0'), (3888, 'P:0 GT:1'), (895, 'P:0 GT:1'), (3937, 'P:2 GT:0'), (1621, 'P:1 GT:0'), (4455, 'P:0 GT:3'), (1729, 'P:1 GT:2'), (1529, 'P:0 GT:1'), (2897, 'P:0 GT:2'), (4489, 'P:0 GT:3'), (442, 'P:3 GT:1'), (1530, 'P:0 GT:1'), (2227, 'P:1 GT:0'), (2881, 'P:0 GT:2'), (2381, 'P:0 GT:1'), (113, 'P:1 GT:0'), (2022, 'P:1 GT:0'), (4159, 'P:2 GT:0'), (2214, 'P:1 GT:0'), (2766, 'P:3 GT:0'), (697, 'P:0 GT:1'), (2500, 'P:0 GT:1'), (4074, 'P:1 GT:0'), (2370, 'P:0 GT:1'), (801, 'P:0 GT:1'), (2886, 'P:1 GT:2'), (4041, 'P:1 GT:0'), (2518, 'P:0 GT:1'), (3505, 'P:0 GT:1'), (3909, 'P:0 GT:1'), (1541, 'P:0 GT:1'), (1646, 'P:1 GT:2'), (2675, 'P:1 GT:0'), (387, 'P:0 GT:1'), (4217, 'P:1 GT:0'), (1007, 'P:1 GT:0'), (1980, 'P:1 GT:0'), (1654, 'P:0 GT:2'), (299, 'P:0 GT:1'), (2921, 'P:0 GT:2'), (3911, 'P:0 GT:1'), (2228, 'P:1 GT:0'), (2577, 'P:1 GT:0'), (394, 'P:0 GT:1'), (122, 'P:3 GT:0'), (888, 'P:0 GT:1'), (1546, 'P:0 GT:1'), (1849, 'P:0 GT:1'), (2624, 'P:1 GT:0'), (661, 'P:1 GT:0'), (2088, 'P:3 GT:0'), (1762, 'P:0 GT:2'), (3225, 'P:1 GT:0'), (4106, 'P:1 GT:0'), (3913, 'P:0 GT:1'), (1889, 'P:2 GT:0'), (420, 'P:0 GT:1'), (3807, 'P:0 GT:1'), (368, 'P:2 GT:0'), (3590, 'P:1 GT:0'), (2403, 'P:0 GT:1'), (1767, 'P:0 GT:2'), (358, 'P:1 GT:0'), (1775, 'P:1 GT:2'), (1487, 'P:0 GT:1'), (2489, 'P:0 GT:1'), (2374, 'P:0 GT:1'), (2495, 'P:0 GT:1'), (281, 'P:2 GT:0'), (2707, 'P:1 GT:0'), (2476, 'P:0 GT:1'), (2835, 'P:0 GT:2'), (4427, 'P:1 GT:3'), (550, 'P:1 GT:0'), (393, 'P:0 GT:1'), (4019, 'P:1 GT:0'), (2471, 'P:0 GT:1'), (1648, 'P:0 GT:2'), (871, 'P:0 GT:1'), (4360, 'P:1 GT:0'), (3350, 'P:1 GT:0'), (854, 'P:0 GT:1'), (1009, 'P:1 GT:0'), (2483, 'P:0 GT:1'), (4507, 'P:0 GT:3'), (63, 'P:3 GT:0'), (3490, 'P:1 GT:0'), (2866, 'P:0 GT:2'), (2553, 'P:3 GT:0'), (4574, 'P:0 GT:3'), (2636, 'P:1 GT:0'), (3044, 'P:1 GT:0'), (388, 'P:0 GT:1'), (4138, 'P:1 GT:0'), (3551, 'P:1 GT:0'), (1716, 'P:0 GT:2'), (1696, 'P:1 GT:0'), (985, 'P:1 GT:0'), (3923, 'P:0 GT:1'), (1515, 'P:0 GT:1'), (3526, 'P:0 GT:1'), (3512, 'P:0 GT:1'), (2425, 'P:0 GT:1'), (2873, 'P:1 GT:2'), (2041, 'P:1 GT:0'), (1537, 'P:0 GT:1'), (3930, 'P:0 GT:1'), (3571, 'P:1 GT:0'), (3546, 'P:3 GT:0'), (1204, 'P:2 GT:0'), (1880, 'P:3 GT:0'), (4204, 'P:1 GT:0'), (785, 'P:0 GT:1'), (796, 'P:0 GT:1'), (4523, 'P:0 GT:3'), (3280, 'P:3 GT:0'), (1684, 'P:1 GT:0'), (119, 'P:2 GT:0'), (1793, 'P:0 GT:1'), (53, 'P:1 GT:0'), (1902, 'P:2 GT:0'), (843, 'P:0 GT:1'), (825, 'P:0 GT:1'), (3086, 'P:1 GT:0'), (862, 'P:2 GT:1'), (903, 'P:0 GT:1'), (766, 'P:0 GT:1'), (2422, 'P:0 GT:1'), (2505, 'P:0 GT:1'), (2213, 'P:1 GT:0'), (2391, 'P:3 GT:1'), (38, 'P:1 GT:0'), (853, 'P:2 GT:1'), (767, 'P:0 GT:1'), (819, 'P:0 GT:1'), (735, 'P:0 GT:1'), (1757, 'P:0 GT:2'), (4575, 'P:0 GT:3'), (3762, 'P:0 GT:1'), (1691, 'P:1 GT:0'), (3514, 'P:3 GT:1'), (3498, 'P:0 GT:1'), (1401, 'P:1 GT:0'), (1720, 'P:0 GT:2'), (2793, 'P:0 GT:2'), (794, 'P:0 GT:1'), (742, 'P:0 GT:1'), (2670, 'P:2 GT:0'), (774, 'P:0 GT:1'), (3198, 'P:3 GT:0'), (2373, 'P:0 GT:1'), (976, 'P:1 GT:0'), (1771, 'P:1 GT:2'), (4559, 'P:0 GT:3'), (708, 'P:0 GT:1'), (655, 'P:1 GT:0'), (2452, 'P:0 GT:1'), (2461, 'P:0 GT:1'), (2415, 'P:0 GT:1'), (4526, 'P:0 GT:3'), (2506, 'P:0 GT:1'), (2744, 'P:2 GT:0'), (2229, 'P:1 GT:0'), (4445, 'P:0 GT:3'), (1797, 'P:0 GT:1'), (797, 'P:0 GT:1'), (3217, 'P:3 GT:0'), (1540, 'P:0 GT:1'), (105, 'P:1 GT:0'), (4469, 'P:0 GT:3'), (804, 'P:0 GT:1'), (4505, 'P:2 GT:3'), (878, 'P:2 GT:1'), (3929, 'P:0 GT:1'), (116, 'P:1 GT:0'), (4407, 'P:1 GT:0'), (1938, 'P:1 GT:0'), (1746, 'P:1 GT:2'), (1765, 'P:0 GT:2'), (2832, 'P:0 GT:2'), (4604, 'P:1 GT:0'), (2397, 'P:0 GT:1'), (1539, 'P:0 GT:1'), (2660, 'P:1 GT:0'), (1550, 'P:0 GT:1'), (3780, 'P:0 GT:1'), (3275, 'P:1 GT:0'), (465, 'P:0 GT:1'), (749, 'P:0 GT:1'), (3781, 'P:0 GT:1'), (881, 'P:0 GT:1'), (1486, 'P:0 GT:1'), (3506, 'P:0 GT:1'), (2430, 'P:0 GT:1'), (1126, 'P:3 GT:0'), (324, 'P:2 GT:0'), (2829, 'P:0 GT:2'), (4546, 'P:0 GT:3'), (1780, 'P:1 GT:2'), (2656, 'P:1 GT:0'), (788, 'P:0 GT:1'), (3297, 'P:2 GT:0'), (2831, 'P:0 GT:2'), (1903, 'P:2 GT:0'), (2929, 'P:0 GT:2'), (3492, 'P:2 GT:0'), (659, 'P:1 GT:0'), (1485, 'P:0 GT:1'), (4468, 'P:0 GT:3'), (4458, 'P:0 GT:3'), (2479, 'P:2 GT:1'), (1766, 'P:0 GT:2'), (968, 'P:1 GT:0'), (1493, 'P:0 GT:1'), (3931, 'P:0 GT:1'), (669, 'P:1 GT:0'), (2532, 'P:1 GT:0'), (893, 'P:0 GT:1'), (2439, 'P:2 GT:1'), (2807, 'P:0 GT:2'), (2433, 'P:0 GT:1'), (844, 'P:0 GT:1'), (231, 'P:3 GT:0'), (3606, 'P:1 GT:0'), (1764, 'P:1 GT:2'), (1706, 'P:0 GT:2'), (120, 'P:2 GT:0'), (755, 'P:0 GT:1'), (2823, 'P:1 GT:2'), (4016, 'P:1 GT:0'), (280, 'P:2 GT:0'), (3218, 'P:1 GT:0'), (1872, 'P:2 GT:0'), (2222, 'P:1 GT:0'), (700, 'P:0 GT:1'), (4601, 'P:1 GT:0'), (256, 'P:2 GT:0'), (2795, 'P:0 GT:2'), (3097, 'P:3 GT:0'), (716, 'P:0 GT:1'), (761, 'P:0 GT:1'), (2836, 'P:0 GT:2'), (3592, 'P:1 GT:0'), (2507, 'P:0 GT:1'), (2858, 'P:0 GT:2'), (3054, 'P:1 GT:0'), (4440, 'P:0 GT:3'), (4570, 'P:0 GT:3'), (3627, 'P:1 GT:0'), (752, 'P:0 GT:1'), (1106, 'P:1 GT:0'), (4161, 'P:3 GT:0'), (2034, 'P:1 GT:0'), (321, 'P:0 GT:1'), (1070, 'P:1 GT:0'), (939, 'P:0 GT:1'), (3076, 'P:1 GT:0'), (34, 'P:1 GT:0'), (3196, 'P:1 GT:0'), (4150, 'P:1 GT:0'), (3515, 'P:0 GT:1'), (2526, 'P:1 GT:0'), (834, 'P:0 GT:1'), (1503, 'P:3 GT:1'), (2302, 'P:1 GT:0'), (1067, 'P:1 GT:0'), (2072, 'P:1 GT:0'), (2349, 'P:1 GT:0'), (4406, 'P:1 GT:0'), (2949, 'P:0 GT:2'), (3815, 'P:0 GT:1'), (3889, 'P:0 GT:1'), (1737, 'P:1 GT:2'), (3041, 'P:1 GT:0'), (46, 'P:1 GT:0'), (719, 'P:0 GT:1'), (1715, 'P:0 GT:2'), (2855, 'P:0 GT:2'), (4513, 'P:0 GT:3'), (852, 'P:2 GT:1'), (855, 'P:2 GT:1'), (4612, 'P:1 GT:0'), (4568, 'P:0 GT:3'), (1707, 'P:0 GT:2'), (3320, 'P:1 GT:0'), (1738, 'P:1 GT:2'), (926, 'P:0 GT:1'), (3751, 'P:1 GT:0'), (2595, 'P:1 GT:0'), (931, 'P:0 GT:1'), (1892, 'P:1 GT:0'), (473, 'P:1 GT:0'), (1913, 'P:2 GT:0'), (2244, 'P:1 GT:0'), (3040, 'P:1 GT:0'), (1832, 'P:2 GT:1'), (2365, 'P:0 GT:1'), (2392, 'P:0 GT:1'), (3474, 'P:1 GT:0'), (89, 'P:1 GT:0'), (188, 'P:2 GT:0'), (2416, 'P:3 GT:1'), (2413, 'P:0 GT:1'), (3211, 'P:1 GT:0'), (2913, 'P:0 GT:2'), (1080, 'P:1 GT:0'), (2569, 'P:1 GT:0'), (1828, 'P:0 GT:1'), (101, 'P:1 GT:0'), (2588, 'P:1 GT:0'), (734, 'P:0 GT:1'), (3990, 'P:1 GT:0'), (1536, 'P:0 GT:1'), (1708, 'P:0 GT:2'), (2458, 'P:0 GT:1'), (1627, 'P:1 GT:0'), (2436, 'P:0 GT:1'), (1839, 'P:0 GT:1'), (863, 'P:0 GT:1'), (2284, 'P:1 GT:0'), (2869, 'P:1 GT:2'), (2809, 'P:0 GT:2'), (1842, 'P:2 GT:1'), (1803, 'P:0 GT:1'), (3372, 'P:1 GT:0'), (2863, 'P:0 GT:2'), (2480, 'P:0 GT:1'), (396, 'P:0 GT:1'), (701, 'P:0 GT:1'), (838, 'P:0 GT:1'), (4508, 'P:0 GT:3'), (4113, 'P:1 GT:0'), (4461, 'P:0 GT:3'), (2884, 'P:1 GT:2'), (4457, 'P:0 GT:3'), (2264, 'P:1 GT:0'), (1799, 'P:0 GT:1'), (740, 'P:0 GT:1'), (1823, 'P:0 GT:1'), (2387, 'P:0 GT:1'), (2546, 'P:1 GT:0'), (2290, 'P:1 GT:0'), (1509, 'P:0 GT:1'), (3897, 'P:0 GT:1'), (1704, 'P:0 GT:2'), (4311, 'P:1 GT:0'), (3791, 'P:0 GT:1'), (2887, 'P:0 GT:2'), (1881, 'P:2 GT:0'), (1692, 'P:1 GT:0'), (1481, 'P:0 GT:1'), (193, 'P:1 GT:0'), (2377, 'P:0 GT:1'), (737, 'P:0 GT:1'), (1752, 'P:1 GT:2'), (1833, 'P:0 GT:1'), (2716, 'P:3 GT:0'), (2682, 'P:3 GT:0'), (2864, 'P:0 GT:2'), (1827, 'P:2 GT:1'), (4412, 'P:1 GT:0'), (1781, 'P:0 GT:2'), (1511, 'P:0 GT:1'), (12, 'P:1 GT:0'), (2552, 'P:1 GT:0'), (55, 'P:3 GT:0'), (3856, 'P:0 GT:1'), (2938, 'P:0 GT:2'), (3933, 'P:0 GT:1'), (3850, 'P:0 GT:1'), (886, 'P:0 GT:1'), (4452, 'P:2 GT:3'), (2388, 'P:0 GT:1'), (2924, 'P:0 GT:2'), (4551, 'P:0 GT:3'), (2247, 'P:1 GT:0'), (960, 'P:1 GT:0'), (1520, 'P:0 GT:1'), (860, 'P:0 GT:1'), (1563, 'P:0 GT:1'), (2496, 'P:0 GT:1'), (2971, 'P:1 GT:0'), (3766, 'P:0 GT:1'), (2018, 'P:3 GT:0'), (906, 'P:0 GT:1'), (1824, 'P:0 GT:1'), (2547, 'P:1 GT:0'), (133, 'P:1 GT:0'), (1575, 'P:3 GT:0'), (3125, 'P:1 GT:0'), (297, 'P:0 GT:1'), (3920, 'P:0 GT:1'), (4460, 'P:0 GT:3'), (3513, 'P:0 GT:1'), (2405, 'P:0 GT:1'), (1915, 'P:3 GT:0'), (2883, 'P:0 GT:2'), (4504, 'P:1 GT:3')]
Confusion Matrix
tensor([[2377,  722,  185,  134],
        [ 684,  143,   73,   14],
        [ 104,   42,    6,   13],
        [  86,   20,   13,    5]])
class 0 accuracy: 73.1160%
class 1 accuracy: 15.4261%
class 2 accuracy: 2.1661%
class 3 accuracy: 3.0120%

Validation Loss: 2.6000, Accuracy: 2531/4621 (55%)
Best Accuracy: 58.125947%
Time Elapsed: 3h 8m 36s
Epochs: [0, 1, 2, 3, 4]
Val_Accuracies: [43.04263146505085, 58.12594676476953, 52.97554641852413, 56.71932482146722, 54.77169443843324]
Val_Losses: [1.595660190201468, 1.7183999754488468, 2.0561865141822233, 2.300857144097487, 2.595222224791845]
Train_Losses: [0.5783770936296174, 0.22127515547453686, 0.12832225349913806, 0.08362198022939965, 0.059067605990593505]
