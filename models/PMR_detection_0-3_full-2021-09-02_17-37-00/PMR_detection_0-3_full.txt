============================ Raw Args ============================
Namespace(batch_size=32, classification='y', dropout='n', droput_prob=0.0, gpu_i=0, hidden_init_rand='n', hidden_size=64, imbalanced_sampler='y', input_size=23, l2_reg='n', load_trained='n', log_dest='../models/PMR_detection_0-3_full-2021-09-02_17-37-00', loss_freq=10, lr=0.002, model_name='PMRfusionNN', normalize='y', num_classes=2, num_epochs=5, num_layers=1, optim='RMS', regression='n', root_dir='/data/perception-working/Geffen/avec_data/', session_name='PMR_detection_0-3_full', train_data_dir='none', train_labels_csv='binary_sampled_train_metadata.csv', trained_path='../models/PMR_detection_0-3_two_to_one-2021-09-01_12-09-14/BEST_model.pth', val_data_dir='none', val_freq=0, val_labels_csv='binary_sampled_val_metadata.csv', weight_decay_amnt=0.0, weighted_loss='n')



================================ Start Training ================================

Session Name: PMR_detection_0-3_full

Model Name: PMRfusionNN

Device: 0  ---->  GeForce GTX 1080 Ti

Hyperparameters:
Batch Size: 32
Learning Rate: 0.002
Hidden Size: 64
Number of Layer: 1
Number of Epochs: 5
Normalization:y

Train Epoch: 0 Iteration: 10 [320/13084 (2%)]	 Batch 10 Loss: 0.697036
Train Epoch: 0 Iteration: 20 [640/13084 (5%)]	 Batch 20 Loss: 0.648817
Train Epoch: 0 Iteration: 30 [960/13084 (7%)]	 Batch 30 Loss: 0.620123
Train Epoch: 0 Iteration: 40 [1280/13084 (10%)]	 Batch 40 Loss: 0.533597
Train Epoch: 0 Iteration: 50 [1600/13084 (12%)]	 Batch 50 Loss: 0.562970
Train Epoch: 0 Iteration: 60 [1920/13084 (15%)]	 Batch 60 Loss: 0.496724
Train Epoch: 0 Iteration: 70 [2240/13084 (17%)]	 Batch 70 Loss: 0.547548
Train Epoch: 0 Iteration: 80 [2560/13084 (20%)]	 Batch 80 Loss: 0.464951
Train Epoch: 0 Iteration: 90 [2880/13084 (22%)]	 Batch 90 Loss: 0.286133
Train Epoch: 0 Iteration: 100 [3200/13084 (24%)]	 Batch 100 Loss: 0.424157
Train Epoch: 0 Iteration: 110 [3520/13084 (27%)]	 Batch 110 Loss: 0.317038
Train Epoch: 0 Iteration: 120 [3840/13084 (29%)]	 Batch 120 Loss: 0.334045
Train Epoch: 0 Iteration: 130 [4160/13084 (32%)]	 Batch 130 Loss: 0.472345
Train Epoch: 0 Iteration: 140 [4480/13084 (34%)]	 Batch 140 Loss: 0.430231
Train Epoch: 0 Iteration: 150 [4800/13084 (37%)]	 Batch 150 Loss: 0.161621
Train Epoch: 0 Iteration: 160 [5120/13084 (39%)]	 Batch 160 Loss: 0.212156
Train Epoch: 0 Iteration: 170 [5440/13084 (42%)]	 Batch 170 Loss: 0.231290
Train Epoch: 0 Iteration: 180 [5760/13084 (44%)]	 Batch 180 Loss: 0.258524
Train Epoch: 0 Iteration: 190 [6080/13084 (46%)]	 Batch 190 Loss: 0.241506
Train Epoch: 0 Iteration: 200 [6400/13084 (49%)]	 Batch 200 Loss: 0.164989
Train Epoch: 0 Iteration: 210 [6720/13084 (51%)]	 Batch 210 Loss: 0.211559
Train Epoch: 0 Iteration: 220 [7040/13084 (54%)]	 Batch 220 Loss: 0.139654
Train Epoch: 0 Iteration: 230 [7360/13084 (56%)]	 Batch 230 Loss: 0.126654
Train Epoch: 0 Iteration: 240 [7680/13084 (59%)]	 Batch 240 Loss: 0.126483
Train Epoch: 0 Iteration: 250 [8000/13084 (61%)]	 Batch 250 Loss: 0.123738
Train Epoch: 0 Iteration: 260 [8320/13084 (64%)]	 Batch 260 Loss: 0.108281
Train Epoch: 0 Iteration: 270 [8640/13084 (66%)]	 Batch 270 Loss: 0.191062
Train Epoch: 0 Iteration: 280 [8960/13084 (68%)]	 Batch 280 Loss: 0.127153
Train Epoch: 0 Iteration: 290 [9280/13084 (71%)]	 Batch 290 Loss: 0.129500
Train Epoch: 0 Iteration: 300 [9600/13084 (73%)]	 Batch 300 Loss: 0.024908
Train Epoch: 0 Iteration: 310 [9920/13084 (76%)]	 Batch 310 Loss: 0.092960
Train Epoch: 0 Iteration: 320 [10240/13084 (78%)]	 Batch 320 Loss: 0.312166
Train Epoch: 0 Iteration: 330 [10560/13084 (81%)]	 Batch 330 Loss: 0.145477
Train Epoch: 0 Iteration: 340 [10880/13084 (83%)]	 Batch 340 Loss: 0.263514
Train Epoch: 0 Iteration: 350 [11200/13084 (86%)]	 Batch 350 Loss: 0.034200
Train Epoch: 0 Iteration: 360 [11520/13084 (88%)]	 Batch 360 Loss: 0.150813
Train Epoch: 0 Iteration: 370 [11840/13084 (90%)]	 Batch 370 Loss: 0.163770
Train Epoch: 0 Iteration: 380 [12160/13084 (93%)]	 Batch 380 Loss: 0.050820
Train Epoch: 0 Iteration: 390 [12480/13084 (95%)]	 Batch 390 Loss: 0.140011
Train Epoch: 0 Iteration: 400 [12800/13084 (98%)]	 Batch 400 Loss: 0.235091


----------------- Epoch 0 -----------------

validation computation time: 7.0  minutes
Confusion Matrix
tensor([[1635, 1722],
        [  43,   17]])
class 0 accuracy: 97.4374%
class 1 accuracy: 0.9776%

Validation Loss: 2.9252, Accuracy: 1652/3417 (48%)
Training Loss:0.2779
Best Accuracy: 48.346503%
Time Elapsed: 0h 42m 50s

--------------------------------------------------------


Train Epoch: 1 Iteration: 10 [320/13084 (2%)]	 Batch 10 Loss: 0.173333
Train Epoch: 1 Iteration: 20 [640/13084 (5%)]	 Batch 20 Loss: 0.120086
Train Epoch: 1 Iteration: 30 [960/13084 (7%)]	 Batch 30 Loss: 0.131064
Train Epoch: 1 Iteration: 40 [1280/13084 (10%)]	 Batch 40 Loss: 0.042969
Train Epoch: 1 Iteration: 50 [1600/13084 (12%)]	 Batch 50 Loss: 0.094828
Train Epoch: 1 Iteration: 60 [1920/13084 (15%)]	 Batch 60 Loss: 0.356685
Train Epoch: 1 Iteration: 70 [2240/13084 (17%)]	 Batch 70 Loss: 0.080518
Train Epoch: 1 Iteration: 80 [2560/13084 (20%)]	 Batch 80 Loss: 0.103075
