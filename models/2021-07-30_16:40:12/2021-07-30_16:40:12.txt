


================================ Start Training ================================

Device: 2  ---->  GeForce GTX 1080 Ti

Hyperparameters:
Batch Size: 64
Learning Rate: 0.001
Hidden Size: 64
Number of Layer: 1
Number of Epochs: 2
Normalization:True
Train Epoch: 0 Iteration: 40 [2560/195776 (1%)]	 Loss: 1.054644
Train Epoch: 0 Iteration: 80 [5120/195776 (3%)]	 Loss: 0.998770
Train Epoch: 0 Iteration: 120 [7680/195776 (4%)]	 Loss: 1.011428
Train Epoch: 0 Iteration: 160 [10240/195776 (5%)]	 Loss: 1.064973
Train Epoch: 0 Iteration: 200 [12800/195776 (7%)]	 Loss: 1.013803
Train Epoch: 0 Iteration: 240 [15360/195776 (8%)]	 Loss: 1.007228
Train Epoch: 0 Iteration: 280 [17920/195776 (9%)]	 Loss: 0.957166


----------------- Epoch 0 Iteration 300 -----------------

Confusion Matrix
tensor([[ 358,  157,  186],
        [ 726, 1057,  332],
        [ 274,  144,  842]])

Validation Loss: nan, Accuracy: 2257/4076 (55%)
Best Accuracy:  55.372914622178605 %
Time Elapsed: 0h 4m 44s

--------------------------------------------------------


Train Epoch: 0 Iteration: 320 [20480/195776 (10%)]	 Loss: 1.060550
Train Epoch: 0 Iteration: 360 [23040/195776 (12%)]	 Loss: 0.929183
Train Epoch: 0 Iteration: 400 [25600/195776 (13%)]	 Loss: 1.027541
Train Epoch: 0 Iteration: 440 [28160/195776 (14%)]	 Loss: 0.937790
Train Epoch: 0 Iteration: 480 [30720/195776 (16%)]	 Loss: 0.953932
Train Epoch: 0 Iteration: 520 [33280/195776 (17%)]	 Loss: 0.972963
Train Epoch: 0 Iteration: 560 [35840/195776 (18%)]	 Loss: 0.935477
Train Epoch: 0 Iteration: 600 [38400/195776 (20%)]	 Loss: 0.971755


----------------- Epoch 0 Iteration 600 -----------------

Confusion Matrix
tensor([[607, 309, 315],
        [592, 973, 219],
        [159,  76, 826]])

Validation Loss: nan, Accuracy: 2406/4076 (59%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 0h 12m 23s

--------------------------------------------------------


Train Epoch: 0 Iteration: 640 [40960/195776 (21%)]	 Loss: 1.023082
Train Epoch: 0 Iteration: 680 [43520/195776 (22%)]	 Loss: 0.987207
Train Epoch: 0 Iteration: 720 [46080/195776 (24%)]	 Loss: 0.999128
Train Epoch: 0 Iteration: 760 [48640/195776 (25%)]	 Loss: nan
Train Epoch: 0 Iteration: 800 [51200/195776 (26%)]	 Loss: nan
Train Epoch: 0 Iteration: 840 [53760/195776 (27%)]	 Loss: nan
Train Epoch: 0 Iteration: 880 [56320/195776 (29%)]	 Loss: nan


----------------- Epoch 0 Iteration 900 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 0h 15m 58s

--------------------------------------------------------


Train Epoch: 0 Iteration: 920 [58880/195776 (30%)]	 Loss: nan
Train Epoch: 0 Iteration: 960 [61440/195776 (31%)]	 Loss: nan
Train Epoch: 0 Iteration: 1000 [64000/195776 (33%)]	 Loss: nan
Train Epoch: 0 Iteration: 1040 [66560/195776 (34%)]	 Loss: nan
Train Epoch: 0 Iteration: 1080 [69120/195776 (35%)]	 Loss: nan
Train Epoch: 0 Iteration: 1120 [71680/195776 (37%)]	 Loss: nan
Train Epoch: 0 Iteration: 1160 [74240/195776 (38%)]	 Loss: nan
Train Epoch: 0 Iteration: 1200 [76800/195776 (39%)]	 Loss: nan


----------------- Epoch 0 Iteration 1200 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 0h 25m 50s

--------------------------------------------------------


Train Epoch: 0 Iteration: 1240 [79360/195776 (41%)]	 Loss: nan
Train Epoch: 0 Iteration: 1280 [81920/195776 (42%)]	 Loss: nan
Train Epoch: 0 Iteration: 1320 [84480/195776 (43%)]	 Loss: nan
Train Epoch: 0 Iteration: 1360 [87040/195776 (44%)]	 Loss: nan
Train Epoch: 0 Iteration: 1400 [89600/195776 (46%)]	 Loss: nan
Train Epoch: 0 Iteration: 1440 [92160/195776 (47%)]	 Loss: nan
Train Epoch: 0 Iteration: 1480 [94720/195776 (48%)]	 Loss: nan


----------------- Epoch 0 Iteration 1500 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 0h 33m 10s

--------------------------------------------------------


Train Epoch: 0 Iteration: 1520 [97280/195776 (50%)]	 Loss: nan
Train Epoch: 0 Iteration: 1560 [99840/195776 (51%)]	 Loss: nan
Train Epoch: 0 Iteration: 1600 [102400/195776 (52%)]	 Loss: nan
Train Epoch: 0 Iteration: 1640 [104960/195776 (54%)]	 Loss: nan
Train Epoch: 0 Iteration: 1680 [107520/195776 (55%)]	 Loss: nan
Train Epoch: 0 Iteration: 1720 [110080/195776 (56%)]	 Loss: nan
Train Epoch: 0 Iteration: 1760 [112640/195776 (58%)]	 Loss: nan
Train Epoch: 0 Iteration: 1800 [115200/195776 (59%)]	 Loss: nan


----------------- Epoch 0 Iteration 1800 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 0h 40m 13s

--------------------------------------------------------


Train Epoch: 0 Iteration: 1840 [117760/195776 (60%)]	 Loss: nan
Train Epoch: 0 Iteration: 1880 [120320/195776 (61%)]	 Loss: nan
Train Epoch: 0 Iteration: 1920 [122880/195776 (63%)]	 Loss: nan
Train Epoch: 0 Iteration: 1960 [125440/195776 (64%)]	 Loss: nan
Train Epoch: 0 Iteration: 2000 [128000/195776 (65%)]	 Loss: nan
Train Epoch: 0 Iteration: 2040 [130560/195776 (67%)]	 Loss: nan
Train Epoch: 0 Iteration: 2080 [133120/195776 (68%)]	 Loss: nan


----------------- Epoch 0 Iteration 2100 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 0h 47m 56s

--------------------------------------------------------


Train Epoch: 0 Iteration: 2120 [135680/195776 (69%)]	 Loss: nan
Train Epoch: 0 Iteration: 2160 [138240/195776 (71%)]	 Loss: nan
Train Epoch: 0 Iteration: 2200 [140800/195776 (72%)]	 Loss: nan
Train Epoch: 0 Iteration: 2240 [143360/195776 (73%)]	 Loss: nan
Train Epoch: 0 Iteration: 2280 [145920/195776 (75%)]	 Loss: nan
Train Epoch: 0 Iteration: 2320 [148480/195776 (76%)]	 Loss: nan
Train Epoch: 0 Iteration: 2360 [151040/195776 (77%)]	 Loss: nan
Train Epoch: 0 Iteration: 2400 [153600/195776 (78%)]	 Loss: nan


----------------- Epoch 0 Iteration 2400 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 0h 55m 15s

--------------------------------------------------------


Train Epoch: 0 Iteration: 2440 [156160/195776 (80%)]	 Loss: nan
Train Epoch: 0 Iteration: 2480 [158720/195776 (81%)]	 Loss: nan
Train Epoch: 0 Iteration: 2520 [161280/195776 (82%)]	 Loss: nan
Train Epoch: 0 Iteration: 2560 [163840/195776 (84%)]	 Loss: nan
Train Epoch: 0 Iteration: 2600 [166400/195776 (85%)]	 Loss: nan
Train Epoch: 0 Iteration: 2640 [168960/195776 (86%)]	 Loss: nan
Train Epoch: 0 Iteration: 2680 [171520/195776 (88%)]	 Loss: nan


----------------- Epoch 0 Iteration 2700 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 1h 4m 14s

--------------------------------------------------------


Train Epoch: 0 Iteration: 2720 [174080/195776 (89%)]	 Loss: nan
Train Epoch: 0 Iteration: 2760 [176640/195776 (90%)]	 Loss: nan
Train Epoch: 0 Iteration: 2800 [179200/195776 (92%)]	 Loss: nan
Train Epoch: 0 Iteration: 2840 [181760/195776 (93%)]	 Loss: nan
Train Epoch: 0 Iteration: 2880 [184320/195776 (94%)]	 Loss: nan
Train Epoch: 0 Iteration: 2920 [186880/195776 (95%)]	 Loss: nan
Train Epoch: 0 Iteration: 2960 [189440/195776 (97%)]	 Loss: nan
Train Epoch: 0 Iteration: 3000 [192000/195776 (98%)]	 Loss: nan


----------------- Epoch 0 Iteration 3000 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 1h 11m 35s

--------------------------------------------------------


Train Epoch: 0 Iteration: 3040 [194560/195776 (99%)]	 Loss: nan
Train Epoch: 1 Iteration: 40 [2560/195776 (1%)]	 Loss: nan
Train Epoch: 1 Iteration: 80 [5120/195776 (3%)]	 Loss: nan
Train Epoch: 1 Iteration: 120 [7680/195776 (4%)]	 Loss: nan
Train Epoch: 1 Iteration: 160 [10240/195776 (5%)]	 Loss: nan
Train Epoch: 1 Iteration: 200 [12800/195776 (7%)]	 Loss: nan
Train Epoch: 1 Iteration: 240 [15360/195776 (8%)]	 Loss: nan
Train Epoch: 1 Iteration: 280 [17920/195776 (9%)]	 Loss: nan


----------------- Epoch 1 Iteration 300 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 1h 24m 52s

--------------------------------------------------------


Train Epoch: 1 Iteration: 320 [20480/195776 (10%)]	 Loss: nan
Train Epoch: 1 Iteration: 360 [23040/195776 (12%)]	 Loss: nan
Train Epoch: 1 Iteration: 400 [25600/195776 (13%)]	 Loss: nan
Train Epoch: 1 Iteration: 440 [28160/195776 (14%)]	 Loss: nan
Train Epoch: 1 Iteration: 480 [30720/195776 (16%)]	 Loss: nan
Train Epoch: 1 Iteration: 520 [33280/195776 (17%)]	 Loss: nan
Train Epoch: 1 Iteration: 560 [35840/195776 (18%)]	 Loss: nan
Train Epoch: 1 Iteration: 600 [38400/195776 (20%)]	 Loss: nan


----------------- Epoch 1 Iteration 600 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 1h 32m 34s

--------------------------------------------------------


Train Epoch: 1 Iteration: 640 [40960/195776 (21%)]	 Loss: nan
Train Epoch: 1 Iteration: 680 [43520/195776 (22%)]	 Loss: nan
Train Epoch: 1 Iteration: 720 [46080/195776 (24%)]	 Loss: nan
Train Epoch: 1 Iteration: 760 [48640/195776 (25%)]	 Loss: nan
Train Epoch: 1 Iteration: 800 [51200/195776 (26%)]	 Loss: nan
Train Epoch: 1 Iteration: 840 [53760/195776 (27%)]	 Loss: nan
Train Epoch: 1 Iteration: 880 [56320/195776 (29%)]	 Loss: nan


----------------- Epoch 1 Iteration 900 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 1h 42m 40s

--------------------------------------------------------


Train Epoch: 1 Iteration: 920 [58880/195776 (30%)]	 Loss: nan
Train Epoch: 1 Iteration: 960 [61440/195776 (31%)]	 Loss: nan
Train Epoch: 1 Iteration: 1000 [64000/195776 (33%)]	 Loss: nan
Train Epoch: 1 Iteration: 1040 [66560/195776 (34%)]	 Loss: nan
Train Epoch: 1 Iteration: 1080 [69120/195776 (35%)]	 Loss: nan
Train Epoch: 1 Iteration: 1120 [71680/195776 (37%)]	 Loss: nan
Train Epoch: 1 Iteration: 1160 [74240/195776 (38%)]	 Loss: nan
Train Epoch: 1 Iteration: 1200 [76800/195776 (39%)]	 Loss: nan


----------------- Epoch 1 Iteration 1200 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 1h 50m 1s

--------------------------------------------------------


Train Epoch: 1 Iteration: 1240 [79360/195776 (41%)]	 Loss: nan
Train Epoch: 1 Iteration: 1280 [81920/195776 (42%)]	 Loss: nan
Train Epoch: 1 Iteration: 1320 [84480/195776 (43%)]	 Loss: nan
Train Epoch: 1 Iteration: 1360 [87040/195776 (44%)]	 Loss: nan
Train Epoch: 1 Iteration: 1400 [89600/195776 (46%)]	 Loss: nan
Train Epoch: 1 Iteration: 1440 [92160/195776 (47%)]	 Loss: nan
Train Epoch: 1 Iteration: 1480 [94720/195776 (48%)]	 Loss: nan


----------------- Epoch 1 Iteration 1500 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 1h 58m 8s

--------------------------------------------------------


Train Epoch: 1 Iteration: 1520 [97280/195776 (50%)]	 Loss: nan
Train Epoch: 1 Iteration: 1560 [99840/195776 (51%)]	 Loss: nan
Train Epoch: 1 Iteration: 1600 [102400/195776 (52%)]	 Loss: nan
Train Epoch: 1 Iteration: 1640 [104960/195776 (54%)]	 Loss: nan
Train Epoch: 1 Iteration: 1680 [107520/195776 (55%)]	 Loss: nan
Train Epoch: 1 Iteration: 1720 [110080/195776 (56%)]	 Loss: nan
Train Epoch: 1 Iteration: 1760 [112640/195776 (58%)]	 Loss: nan
Train Epoch: 1 Iteration: 1800 [115200/195776 (59%)]	 Loss: nan


----------------- Epoch 1 Iteration 1800 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 2h 6m 54s

--------------------------------------------------------


Train Epoch: 1 Iteration: 1840 [117760/195776 (60%)]	 Loss: nan
Train Epoch: 1 Iteration: 1880 [120320/195776 (61%)]	 Loss: nan
Train Epoch: 1 Iteration: 1920 [122880/195776 (63%)]	 Loss: nan
Train Epoch: 1 Iteration: 1960 [125440/195776 (64%)]	 Loss: nan
Train Epoch: 1 Iteration: 2000 [128000/195776 (65%)]	 Loss: nan
Train Epoch: 1 Iteration: 2040 [130560/195776 (67%)]	 Loss: nan
Train Epoch: 1 Iteration: 2080 [133120/195776 (68%)]	 Loss: nan


----------------- Epoch 1 Iteration 2100 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 2h 11m 45s

--------------------------------------------------------


Train Epoch: 1 Iteration: 2120 [135680/195776 (69%)]	 Loss: nan
Train Epoch: 1 Iteration: 2160 [138240/195776 (71%)]	 Loss: nan
Train Epoch: 1 Iteration: 2200 [140800/195776 (72%)]	 Loss: nan
Train Epoch: 1 Iteration: 2240 [143360/195776 (73%)]	 Loss: nan
Train Epoch: 1 Iteration: 2280 [145920/195776 (75%)]	 Loss: nan
Train Epoch: 1 Iteration: 2320 [148480/195776 (76%)]	 Loss: nan
Train Epoch: 1 Iteration: 2360 [151040/195776 (77%)]	 Loss: nan
Train Epoch: 1 Iteration: 2400 [153600/195776 (78%)]	 Loss: nan


----------------- Epoch 1 Iteration 2400 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 2h 18m 38s

--------------------------------------------------------


Train Epoch: 1 Iteration: 2440 [156160/195776 (80%)]	 Loss: nan
Train Epoch: 1 Iteration: 2480 [158720/195776 (81%)]	 Loss: nan
Train Epoch: 1 Iteration: 2520 [161280/195776 (82%)]	 Loss: nan
Train Epoch: 1 Iteration: 2560 [163840/195776 (84%)]	 Loss: nan
Train Epoch: 1 Iteration: 2600 [166400/195776 (85%)]	 Loss: nan
Train Epoch: 1 Iteration: 2640 [168960/195776 (86%)]	 Loss: nan
Train Epoch: 1 Iteration: 2680 [171520/195776 (88%)]	 Loss: nan


----------------- Epoch 1 Iteration 2700 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 2h 22m 34s

--------------------------------------------------------


Train Epoch: 1 Iteration: 2720 [174080/195776 (89%)]	 Loss: nan
Train Epoch: 1 Iteration: 2760 [176640/195776 (90%)]	 Loss: nan
Train Epoch: 1 Iteration: 2800 [179200/195776 (92%)]	 Loss: nan
Train Epoch: 1 Iteration: 2840 [181760/195776 (93%)]	 Loss: nan
Train Epoch: 1 Iteration: 2880 [184320/195776 (94%)]	 Loss: nan
Train Epoch: 1 Iteration: 2920 [186880/195776 (95%)]	 Loss: nan
Train Epoch: 1 Iteration: 2960 [189440/195776 (97%)]	 Loss: nan
Train Epoch: 1 Iteration: 3000 [192000/195776 (98%)]	 Loss: nan


----------------- Epoch 1 Iteration 3000 -----------------

Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 2h 27m 42s

--------------------------------------------------------


Train Epoch: 1 Iteration: 3040 [194560/195776 (99%)]	 Loss: nan
================================ Finished Training ================================
Incorrect Samples: [57, 27, 33, 48, 8, 23, 35, 52, 17, 6, 30, 2, 3, 44, 20, 25, 9, 34, 24, 29, 43, 59, 7, 36, 38, 40, 19, 47, 0, 16, 32, 18, 5, 45, 10, 12, 60, 55, 61, 63, 26, 15, 62, 22, 37, 28, 54, 41, 50, 46, 110, 79, 121, 98, 96, 91, 112, 76, 116, 107, 114, 83, 90, 93, 104, 81, 111, 84, 120, 123, 106, 87, 117, 80, 92, 68, 75, 64, 115, 122, 82, 105, 100, 109, 78, 71, 124, 72, 74, 188, 137, 128, 130, 149, 159, 179, 181, 165, 138, 176, 180, 164, 158, 178, 148, 170, 133, 136, 183, 131, 160, 147, 171, 132, 151, 135, 154, 146, 163, 184, 142, 153, 182, 166, 134, 173, 143, 191, 185, 190, 140, 189, 177, 152, 156, 222, 216, 204, 215, 210, 248, 213, 253, 203, 205, 196, 239, 209, 201, 202, 198, 252, 254, 219, 242, 224, 246, 247, 208, 220, 225, 240, 223, 206, 228, 245, 193, 218, 194, 237, 255, 211, 251, 221, 199, 217, 250, 232, 235, 284, 318, 265, 272, 314, 261, 313, 258, 264, 308, 300, 319, 312, 302, 289, 288, 269, 310, 277, 292, 306, 274, 311, 279, 315, 283, 285, 276, 301, 266, 317, 294, 271, 280, 291, 290, 305, 373, 341, 358, 327, 377, 378, 350, 334, 348, 361, 339, 363, 362, 330, 347, 328, 345, 380, 336, 355, 382, 367, 379, 370, 326, 354, 346, 342, 321, 376, 335, 329, 369, 352, 356, 366, 349, 340, 338, 337, 360, 320, 381, 333, 332, 397, 436, 437, 417, 406, 432, 414, 411, 442, 401, 435, 393, 395, 445, 386, 426, 412, 439, 410, 394, 422, 429, 425, 418, 387, 446, 428, 402, 409, 420, 416, 400, 438, 404, 421, 447, 389, 433, 427, 385, 424, 443, 452, 511, 494, 505, 479, 451, 508, 507, 457, 449, 510, 485, 456, 495, 477, 506, 472, 490, 455, 481, 500, 466, 488, 486, 471, 474, 483, 484, 469, 465, 459, 498, 503, 475, 487, 470, 463, 467, 478, 496, 454, 450, 492, 557, 519, 553, 518, 520, 566, 569, 539, 521, 568, 517, 561, 514, 571, 552, 544, 524, 550, 554, 565, 534, 528, 541, 572, 526, 527, 536, 558, 512, 573, 575, 538, 530, 560, 535, 522, 542, 529, 559, 548, 547, 525, 515, 622, 633, 613, 635, 630, 619, 597, 634, 628, 615, 584, 596, 587, 588, 576, 582, 625, 592, 639, 602, 638, 594, 580, 617, 626, 611, 636, 607, 604, 590, 606, 610, 637, 585, 579, 688, 694, 648, 685, 655, 668, 683, 670, 697, 665, 678, 702, 657, 646, 666, 676, 703, 664, 641, 650, 687, 691, 689, 692, 651, 698, 652, 669, 695, 686, 647, 696, 653, 662, 659, 701, 663, 654, 673, 682, 675, 684, 690, 667, 680, 693, 642, 725, 708, 716, 767, 753, 744, 739, 715, 737, 752, 754, 706, 743, 721, 733, 763, 748, 745, 738, 749, 724, 759, 728, 750, 712, 732, 736, 757, 709, 727, 751, 720, 755, 729, 730, 766, 762, 711, 718, 707, 723, 760, 758, 764, 731, 714, 747, 742, 826, 808, 776, 795, 768, 774, 781, 788, 799, 785, 770, 805, 822, 802, 792, 818, 779, 811, 790, 821, 806, 812, 794, 796, 803, 816, 778, 819, 830, 789, 813, 815, 772, 804, 831, 798, 797, 783, 827, 824, 791, 825, 780, 842, 888, 893, 841, 895, 869, 894, 886, 866, 883, 854, 873, 870, 845, 865, 891, 848, 853, 862, 859, 884, 892, 856, 850, 871, 832, 838, 844, 868, 867, 855, 846, 851, 834, 880, 890, 836, 839, 837, 852, 923, 914, 922, 944, 951, 928, 898, 929, 930, 912, 955, 896, 957, 939, 913, 900, 938, 919, 903, 916, 937, 931, 943, 907, 935, 911, 950, 926, 918, 942, 910, 921, 924, 956, 940, 925, 952, 909, 904, 897, 906, 902, 959, 934, 915, 1005, 1002, 1007, 1001, 992, 985, 998, 1022, 996, 984, 976, 1008, 971, 995, 1016, 1020, 960, 1018, 963, 991, 972, 987, 994, 980, 961, 1004, 982, 974, 1021, 967, 973, 1009, 966, 988, 968, 997, 1035, 1028, 1058, 1071, 1043, 1040, 1077, 1033, 1041, 1066, 1047, 1042, 1045, 1044, 1080, 1072, 1087, 1063, 1069, 1048, 1050, 1049, 1037, 1046, 1061, 1081, 1053, 1074, 1025, 1054, 1039, 1056, 1078, 1065, 1084, 1064, 1036, 1034, 1083, 1038, 1070, 1085, 1032, 1067, 1031, 1030, 1124, 1139, 1094, 1092, 1141, 1127, 1106, 1103, 1090, 1089, 1126, 1135, 1144, 1093, 1088, 1107, 1102, 1125, 1098, 1099, 1134, 1132, 1121, 1150, 1104, 1138, 1112, 1091, 1108, 1105, 1151, 1118, 1128, 1145, 1111, 1113, 1114, 1136, 1148, 1131, 1095, 1179, 1182, 1155, 1184, 1165, 1209, 1196, 1152, 1192, 1174, 1188, 1168, 1181, 1194, 1186, 1183, 1197, 1167, 1163, 1177, 1211, 1190, 1213, 1180, 1204, 1212, 1191, 1166, 1172, 1153, 1185, 1164, 1199, 1170, 1208, 1193, 1198, 1215, 1189, 1159, 1201, 1244, 1235, 1243, 1242, 1246, 1252, 1227, 1253, 1234, 1236, 1272, 1263, 1265, 1229, 1220, 1240, 1225, 1262, 1270, 1249, 1264, 1267, 1217, 1232, 1276, 1250, 1233, 1219, 1230, 1245, 1268, 1254, 1274, 1255, 1273, 1258, 1257, 1241, 1271, 1224, 1260, 1256, 1261, 1279, 1226, 1216, 1266, 1277, 1326, 1303, 1325, 1334, 1338, 1309, 1311, 1331, 1307, 1283, 1314, 1337, 1298, 1289, 1343, 1333, 1320, 1284, 1290, 1297, 1317, 1339, 1327, 1296, 1293, 1332, 1280, 1318, 1324, 1291, 1282, 1342, 1301, 1305, 1288, 1323, 1294, 1319, 1382, 1403, 1347, 1390, 1405, 1377, 1367, 1348, 1391, 1357, 1379, 1380, 1400, 1355, 1386, 1365, 1375, 1385, 1368, 1394, 1399, 1397, 1389, 1359, 1378, 1372, 1407, 1401, 1345, 1344, 1349, 1384, 1398, 1387, 1370, 1364, 1383, 1352, 1406, 1358, 1396, 1354, 1402, 1350, 1362, 1371, 1395, 1404, 1356, 1421, 1413, 1439, 1424, 1429, 1428, 1463, 1409, 1469, 1452, 1470, 1457, 1467, 1434, 1423, 1459, 1419, 1454, 1412, 1440, 1417, 1431, 1436, 1453, 1442, 1468, 1448, 1458, 1456, 1465, 1411, 1449, 1461, 1427, 1422, 1455, 1464, 1450, 1478, 1506, 1522, 1502, 1490, 1504, 1485, 1480, 1528, 1515, 1501, 1499, 1481, 1518, 1520, 1477, 1498, 1484, 1483, 1517, 1496, 1487, 1486, 1507, 1497, 1519, 1492, 1526, 1494, 1511, 1493, 1482, 1525, 1473, 1491, 1523, 1513, 1476, 1514, 1479, 1505, 1524, 1534, 1475, 1474, 1529, 1500, 1516, 1472, 1508, 1521, 1489, 1568, 1545, 1549, 1554, 1552, 1582, 1536, 1541, 1566, 1556, 1593, 1537, 1557, 1539, 1595, 1562, 1571, 1560, 1579, 1573, 1550, 1543, 1575, 1588, 1574, 1598, 1599, 1590, 1576, 1567, 1538, 1546, 1564, 1548, 1561, 1542, 1589, 1584, 1553, 1577, 1558, 1569, 1572, 1580, 1586, 1587, 1540, 1603, 1642, 1654, 1661, 1660, 1614, 1607, 1608, 1609, 1644, 1651, 1628, 1621, 1659, 1617, 1657, 1647, 1622, 1639, 1656, 1604, 1648, 1611, 1662, 1626, 1646, 1653, 1630, 1636, 1605, 1619, 1645, 1606, 1618, 1655, 1650, 1633, 1631, 1658, 1640, 1623, 1612, 1678, 1699, 1673, 1697, 1689, 1685, 1720, 1693, 1681, 1665, 1715, 1664, 1683, 1686, 1701, 1727, 1666, 1713, 1707, 1698, 1704, 1671, 1711, 1675, 1702, 1716, 1708, 1672, 1703, 1670, 1692, 1722, 1690, 1712, 1667, 1687, 1668, 1669, 1700, 1695, 1718, 1710, 1677, 1674, 1679, 1694, 1721, 1714, 1705, 1782, 1758, 1743, 1791, 1757, 1753, 1734, 1764, 1763, 1760, 1786, 1731, 1756, 1735, 1766, 1759, 1784, 1778, 1768, 1789, 1777, 1730, 1740, 1746, 1762, 1781, 1741, 1779, 1765, 1761, 1742, 1751, 1733, 1785, 1755, 1737, 1736, 1783, 1790, 1776, 1831, 1842, 1837, 1839, 1826, 1815, 1843, 1818, 1800, 1822, 1823, 1849, 1817, 1852, 1834, 1794, 1796, 1798, 1809, 1813, 1811, 1799, 1848, 1814, 1801, 1821, 1835, 1836, 1819, 1829, 1792, 1851, 1797, 1841, 1808, 1820, 1795, 1840, 1803, 1853, 1845, 1828, 1846, 1806, 1844, 1825, 1903, 1878, 1872, 1906, 1875, 1871, 1890, 1902, 1870, 1885, 1896, 1865, 1912, 1880, 1887, 1866, 1868, 1904, 1861, 1864, 1876, 1897, 1888, 1856, 1899, 1893, 1883, 1898, 1877, 1886, 1873, 1913, 1874, 1911, 1918, 1859, 1857, 1901, 1881, 1889, 1891, 1882, 1916, 1862, 1945, 1950, 1958, 1973, 1922, 1964, 1932, 1981, 1956, 1947, 1967, 1957, 1969, 1935, 1974, 1965, 1920, 1925, 1983, 1939, 1940, 1927, 1961, 1930, 1941, 1959, 1960, 1948, 1954, 1955, 1924, 1976, 1966, 1963, 1926, 2023, 2038, 2004, 2018, 1994, 1985, 2042, 2041, 2015, 2009, 2008, 2030, 2013, 2000, 2032, 2040, 2037, 1993, 2020, 2034, 2016, 2043, 2044, 2031, 2025, 1986, 2022, 1987, 1991, 2026, 2006, 2029, 2028, 1999, 2017, 2010, 1996, 2046, 2036, 1995, 2024, 2014, 2052, 2107, 2097, 2074, 2069, 2073, 2091, 2056, 2093, 2099, 2060, 2100, 2080, 2089, 2086, 2075, 2057, 2087, 2103, 2065, 2062, 2078, 2095, 2051, 2084, 2077, 2072, 2058, 2082, 2105, 2096, 2048, 2081, 2068, 2101, 2054, 2071, 2066, 2059, 2070, 2053, 2094, 2079, 2109, 2049, 2104, 2175, 2119, 2137, 2123, 2162, 2163, 2124, 2117, 2132, 2156, 2157, 2170, 2148, 2141, 2131, 2147, 2126, 2121, 2140, 2133, 2112, 2151, 2173, 2120, 2116, 2144, 2159, 2160, 2169, 2118, 2174, 2114, 2172, 2113, 2145, 2158, 2161, 2139, 2155, 2166, 2125, 2135, 2180, 2189, 2227, 2231, 2183, 2209, 2185, 2236, 2232, 2221, 2194, 2192, 2199, 2179, 2176, 2190, 2238, 2186, 2182, 2213, 2202, 2225, 2203, 2187, 2222, 2210, 2181, 2217, 2201, 2230, 2212, 2198, 2191, 2214, 2177, 2211, 2208, 2233, 2235, 2224, 2204, 2297, 2303, 2248, 2296, 2243, 2282, 2299, 2240, 2255, 2262, 2301, 2281, 2272, 2289, 2270, 2251, 2288, 2293, 2252, 2269, 2279, 2257, 2286, 2283, 2292, 2285, 2273, 2295, 2256, 2278, 2263, 2280, 2258, 2287, 2274, 2260, 2284, 2254, 2290, 2241, 2300, 2276, 2267, 2294, 2268, 2245, 2271, 2320, 2304, 2338, 2365, 2305, 2340, 2316, 2348, 2324, 2343, 2309, 2336, 2334, 2355, 2311, 2329, 2356, 2330, 2318, 2339, 2362, 2351, 2332, 2353, 2314, 2354, 2361, 2342, 2346, 2310, 2313, 2323, 2357, 2337, 2350, 2307, 2341, 2315, 2358, 2317, 2367, 2322, 2363, 2326, 2386, 2401, 2375, 2414, 2380, 2368, 2369, 2397, 2376, 2400, 2409, 2431, 2404, 2417, 2412, 2429, 2415, 2372, 2371, 2405, 2422, 2411, 2408, 2387, 2374, 2403, 2424, 2392, 2416, 2419, 2407, 2428, 2373, 2395, 2425, 2384, 2396, 2445, 2461, 2464, 2482, 2485, 2481, 2458, 2468, 2441, 2495, 2454, 2467, 2457, 2491, 2475, 2448, 2465, 2470, 2447, 2477, 2455, 2452, 2480, 2487, 2494, 2435, 2474, 2432, 2459, 2476, 2433, 2440, 2462, 2439, 2460, 2437, 2490, 2434, 2443, 2471, 2557, 2552, 2549, 2556, 2515, 2534, 2507, 2539, 2540, 2531, 2555, 2554, 2553, 2548, 2551, 2500, 2527, 2522, 2506, 2546, 2517, 2523, 2505, 2496, 2516, 2544, 2559, 2541, 2511, 2526, 2519, 2533, 2524, 2497, 2514, 2542, 2521, 2499, 2547, 2532, 2510, 2536, 2529, 2545, 2560, 2588, 2605, 2565, 2569, 2585, 2577, 2595, 2598, 2574, 2597, 2590, 2621, 2570, 2599, 2609, 2589, 2600, 2594, 2613, 2606, 2615, 2622, 2608, 2620, 2592, 2602, 2612, 2572, 2591, 2593, 2566, 2582, 2573, 2576, 2578, 2561, 2617, 2604, 2623, 2596, 2564, 2619, 2580, 2607, 2657, 2660, 2627, 2676, 2636, 2653, 2685, 2687, 2682, 2629, 2672, 2658, 2665, 2659, 2668, 2651, 2670, 2666, 2625, 2684, 2628, 2635, 2630, 2624, 2678, 2663, 2677, 2626, 2642, 2673, 2669, 2650, 2652, 2656, 2662, 2640, 2645, 2686, 2680, 2664, 2696, 2722, 2697, 2710, 2736, 2730, 2688, 2726, 2724, 2728, 2734, 2742, 2727, 2739, 2747, 2717, 2738, 2701, 2691, 2706, 2709, 2694, 2700, 2719, 2723, 2704, 2735, 2725, 2745, 2743, 2714, 2703, 2718, 2729, 2720, 2708, 2689, 2698, 2705, 2721, 2746, 2773, 2782, 2799, 2802, 2806, 2752, 2771, 2790, 2784, 2815, 2808, 2770, 2813, 2774, 2793, 2801, 2760, 2757, 2810, 2788, 2764, 2783, 2796, 2805, 2766, 2765, 2780, 2795, 2778, 2776, 2798, 2755, 2792, 2762, 2787, 2754, 2763, 2791, 2852, 2825, 2865, 2821, 2845, 2837, 2872, 2818, 2866, 2828, 2858, 2879, 2846, 2878, 2851, 2873, 2843, 2876, 2827, 2839, 2826, 2848, 2874, 2829, 2850, 2841, 2857, 2875, 2819, 2835, 2831, 2832, 2842, 2824, 2863, 2855, 2854, 2867, 2817, 2869, 2920, 2892, 2896, 2943, 2907, 2937, 2882, 2881, 2902, 2942, 2883, 2935, 2888, 2889, 2904, 2906, 2880, 2925, 2928, 2926, 2930, 2891, 2929, 2901, 2893, 2915, 2898, 2887, 2886, 2922, 2903, 2897, 2895, 2941, 2909, 2931, 2918, 2919, 2917, 2890, 2884, 2934, 2948, 2959, 2997, 2944, 2956, 2995, 2945, 2952, 2975, 2996, 2993, 2992, 2981, 2974, 2986, 2949, 2980, 2969, 3003, 3007, 2971, 2976, 2961, 2953, 3005, 2989, 2957, 2979, 2977, 2988, 2947, 2983, 3004, 2999, 2962, 2954, 3000, 2985, 2950, 2964, 3002, 2984, 2972, 2968, 3065, 3071, 3052, 3045, 3067, 3025, 3061, 3019, 3070, 3043, 3021, 3018, 3035, 3034, 3016, 3048, 3022, 3063, 3020, 3033, 3028, 3058, 3064, 3060, 3015, 3053, 3031, 3014, 3059, 3062, 3040, 3069, 3012, 3050, 3030, 3009, 3032, 3068, 3008, 3051, 3055, 3039, 3121, 3103, 3130, 3128, 3075, 3093, 3080, 3094, 3133, 3123, 3072, 3124, 3118, 3111, 3097, 3126, 3116, 3090, 3073, 3131, 3102, 3087, 3117, 3106, 3129, 3105, 3101, 3083, 3104, 3132, 3076, 3099, 3114, 3091, 3079, 3084, 3127, 3074, 3098, 3135, 3134, 3107, 3089, 3122, 3095, 3139, 3158, 3153, 3136, 3190, 3156, 3192, 3175, 3191, 3145, 3170, 3197, 3194, 3159, 3198, 3183, 3196, 3180, 3167, 3184, 3179, 3140, 3186, 3195, 3152, 3166, 3147, 3199, 3187, 3181, 3171, 3189, 3148, 3177, 3163, 3141, 3185, 3164, 3176, 3143, 3193, 3138, 3165, 3213, 3243, 3251, 3203, 3233, 3248, 3239, 3217, 3238, 3245, 3257, 3232, 3253, 3206, 3215, 3235, 3259, 3262, 3202, 3225, 3241, 3228, 3244, 3261, 3219, 3201, 3214, 3226, 3207, 3252, 3263, 3205, 3218, 3208, 3223, 3212, 3240, 3247, 3231, 3227, 3304, 3273, 3315, 3301, 3297, 3289, 3322, 3314, 3288, 3306, 3317, 3283, 3303, 3318, 3325, 3308, 3291, 3310, 3272, 3292, 3265, 3295, 3268, 3305, 3309, 3279, 3284, 3275, 3320, 3313, 3290, 3280, 3307, 3278, 3287, 3293, 3299, 3326, 3269, 3267, 3285, 3282, 3311, 3341, 3354, 3375, 3368, 3342, 3384, 3334, 3363, 3329, 3390, 3370, 3344, 3376, 3336, 3351, 3335, 3378, 3330, 3356, 3339, 3380, 3346, 3332, 3348, 3386, 3359, 3366, 3379, 3372, 3343, 3374, 3361, 3350, 3360, 3364, 3373, 3338, 3353, 3340, 3377, 3371, 3391, 3382, 3394, 3439, 3432, 3393, 3411, 3435, 3427, 3438, 3398, 3422, 3397, 3444, 3414, 3447, 3423, 3443, 3409, 3405, 3419, 3418, 3448, 3416, 3425, 3415, 3437, 3453, 3431, 3417, 3446, 3396, 3403, 3429, 3402, 3413, 3412, 3404, 3399, 3441, 3395, 3407, 3428, 3434, 3454, 3400, 3440, 3470, 3506, 3476, 3513, 3483, 3501, 3457, 3517, 3461, 3458, 3477, 3481, 3464, 3468, 3489, 3463, 3462, 3498, 3512, 3499, 3471, 3503, 3469, 3474, 3460, 3466, 3502, 3478, 3504, 3515, 3511, 3492, 3507, 3488, 3485, 3505, 3456, 3496, 3486, 3487, 3465, 3467, 3490, 3509, 3491, 3580, 3537, 3536, 3538, 3575, 3521, 3571, 3522, 3554, 3570, 3574, 3577, 3529, 3532, 3545, 3549, 3546, 3564, 3552, 3558, 3525, 3576, 3555, 3528, 3557, 3561, 3535, 3563, 3581, 3526, 3520, 3565, 3556, 3539, 3591, 3588, 3613, 3592, 3606, 3597, 3639, 3599, 3642, 3596, 3640, 3644, 3603, 3587, 3636, 3619, 3589, 3620, 3645, 3611, 3628, 3595, 3605, 3590, 3641, 3612, 3593, 3614, 3598, 3634, 3623, 3608, 3646, 3631, 3604, 3616, 3617, 3615, 3651, 3679, 3698, 3656, 3672, 3697, 3652, 3680, 3674, 3692, 3708, 3705, 3691, 3694, 3711, 3704, 3664, 3657, 3685, 3658, 3671, 3662, 3709, 3702, 3707, 3669, 3663, 3706, 3648, 3700, 3681, 3670, 3686, 3710, 3654, 3668, 3693, 3666, 3689, 3696, 3701, 3738, 3765, 3774, 3725, 3775, 3771, 3714, 3744, 3762, 3773, 3724, 3717, 3755, 3730, 3757, 3715, 3723, 3736, 3742, 3764, 3729, 3751, 3713, 3763, 3712, 3739, 3721, 3733, 3726, 3743, 3735, 3749, 3722, 3740, 3737, 3719, 3741, 3761, 3750, 3768, 3752, 3732, 3772, 3720, 3794, 3796, 3795, 3817, 3797, 3780, 3815, 3822, 3830, 3821, 3839, 3826, 3816, 3805, 3782, 3804, 3809, 3790, 3776, 3793, 3788, 3811, 3837, 3823, 3812, 3820, 3779, 3802, 3813, 3829, 3803, 3827, 3831, 3818, 3834, 3838, 3832, 3798, 3835, 3781, 3828, 3799, 3833, 3784, 3876, 3902, 3861, 3872, 3852, 3894, 3841, 3858, 3886, 3859, 3854, 3846, 3893, 3851, 3864, 3863, 3878, 3840, 3873, 3860, 3843, 3867, 3882, 3847, 3887, 3848, 3856, 3884, 3901, 3895, 3862, 3892, 3849, 3874, 3866, 3877, 3842, 3850, 3881, 3857, 3889, 3853, 3880, 3879, 3953, 3936, 3962, 3912, 3922, 3935, 3956, 3913, 3951, 3921, 3924, 3917, 3941, 3908, 3911, 3964, 3910, 3934, 3914, 3930, 3939, 3940, 3942, 3959, 3906, 3954, 3907, 3937, 3960, 3949, 3929, 3918, 3945, 3915, 3950, 3931, 3933, 3932, 3943, 3923, 3967, 3965, 3948, 3938, 3944, 3997, 3972, 3976, 4021, 4005, 4025, 3994, 4009, 4004, 3970, 3987, 3982, 4027, 4001, 3975, 4017, 3971, 4022, 3968, 3989, 3986, 3974, 4026, 3984, 4031, 3991, 4008, 4011, 4019, 4010, 4015, 3996, 3988, 4016, 4030, 3978, 3980, 4023, 3973, 4018, 3979, 3990, 4020, 4053, 4067, 4037, 4034, 4066, 4063, 4040, 4062, 4075, 4071, 4065, 4072, 4032, 4039, 4057, 4047, 4060, 4064, 4058, 4055, 4051, 4061, 4068, 4033, 4054, 4035, 4069]
Confusion Matrix
tensor([[1358, 1358, 1360],
        [   0,    0,    0],
        [   0,    0,    0]])

Validation Loss: nan, Accuracy: 1358/4076 (33%)
Best Accuracy:  59.02845927379784 %
Time Elapsed: 2h 29m 29s
Iterations: [300, 600, 900, 1200, 1500, 1800, 2100, 2400, 2700, 3000, 300, 600, 900, 1200, 1500, 1800, 2100, 2400, 2700, 3000]
Val_Accuracies: [55.372914622178605, 59.02845927379784, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814, 33.316977428851814]
Val_Losses: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
Train_Losses: [0.7737836167216301, 0.7243610391020775, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
