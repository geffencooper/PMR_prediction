#!/bin/bash 

# ------------------ logging details ------------------
session_name="speech_pace_HIDDEN_SIZE" # prefix the logging directory with this name
#log_dest="" # name of directory with logging info (stats, train model, parameters, etc.)

# ------------------ dataset details ------------------
root_dir="/data/perception-working/Geffen/SpeechPaceData/" # full path to dataset directory
train_data_dir="training_data_aug" # specify if training data is in another directory within root_dir
val_data_dir="validation_data" # specify if validation data is in another directory within root_dir
#test_data_dir="test_data" # specify if test data is in another directory within root_dir
train_labels_csv="train_labels2.csv" # file name of csv with training labels and/or metadata
val_labels_csv="val_labels2.csv" # file name of csv with validation labels and/or metadata
#test_labels_csv="test_labels2.csv"

# ------------------ training details ------------------
gpu_i=2 # (int) GPU instance to use (0, 1, 2, etc.)
model_name="SpeechPaceNN" # name of the model class (torch.nn.Module)
optim="Adam" # name of PyTorch optimizer to use
loss_freq=50 # (int) print the loss every nth batch
val_freq=300 # (int) do a validation pass every nth batch

# ------------------ hyperparameters ------------------
batch_size=64 # what size batch to use (32, 64, 128, etc.)
lr=0.002 # (float) learning rate
hidden_size=32 # dimension of hidden state (RNN)
classification="y" # use the model for classification (y/n) 
num_classes=3 # number of classes for the task, IF REGRESSION SET TO (-1)
regression="n" # use the model for regression (y/n)
input_size=26 # dimension of input features (e.g. MFCC features = 26)
num_layers=1 # number of GRU layers
num_epochs=2 # number of times to go through entire training set
normalize="n" # normalize input features (y/n)
hidden_init_rand="n" # initialize the hidden state with random values, otherwise use zeros (y/n)